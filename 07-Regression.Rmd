# Linear regression {}

Chapter not ready for review

While the goal of correlation is to give you a metric of the strength of the relationship between two variables; the goal of regression is to give you a numerical representation of the tendency in your data. That numerical representation is called a model that describes a line that best represent the tendency in your two variables. At times, that line can also be called a **regression line** or a **trend** line.

Here we will be working on linear regression models...this means to find an straight line to best represent the relationship between two variables.

For instance, the blue line below appears to represent well the tendency that exist between variables Y and X. The goal of this chapter is to use a mathematical procedure (linear regression) that allows you to identify that line.

```{r, echo=FALSE, message=FALSE,warning=FALSE}
X=c(-2.16, -0.76, -0.26,  0.84,  2.34) 
Y=c(-1.98, -1.48,  0.12,  1.32,  2.02)       

MeanX=mean(X)
MeanY=mean(Y)


#now let's do the plot
plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5))
abline(lm(Y ~ X),  col="blue",lwd=1)
```


As indicated in the prior chapter, all relationships do not have to be linear, there are also those so-call non-linear relationships. For now, we will cover only linear relationships, or straight lines that best summarize the relationship between two variables.


There are a few cool benefits to having a single line describing the data:

1. It allows you to mathematically define how two variables are related. Like if I increase X by 2 then Y increases by how much?.

2. It will allow you to make predictions  over areas in which you have no data. Like if X is 52.3 then why is?

3. You do not need to carry/display the data every time you want to work with the database.




To define that best-fitting line, we will use what is called the **least squares** method; sounds scary, but it is actually very simple.

$$\\[.0005in]$$

**Expectation for this chapter**

At the end of this chapter, it is expected that you:

1. Can estimate a linear regression model that defines the linear relationship between two variables using the least-squares criterion.

2. Be able to interpret the results from a linear model.

3. Use the equation of the regression model to predict Y-values (interpolation and extrapolation).

4. Calculate the coefficient of determination and understand what it tells you about the relationship between two variables.


Alright, let's get to it.


## Parts of a line {-}
Before we get into the needy-greedy of linear regression models, we should start by knowing how a line can be constructed.

In practical terms, an specific straight line, in a XY space, can be drawn by knowing just two parameters: the **intercept** and the **slope**.




```{r, out.width = "50%", echo= FALSE, fig.align = 'center',fig.cap = 'The slope and the intercept'}
knitr::include_graphics("images/SlopeAndIntercept.png")
```


The intercept is basically the position in the Y-axis where the regression line crosses, or the value of Y when X=0.


The slope is the inclination of that line, or the change in Y divided by X. It also indicates the mount by which Y changes for unit of change in X; more on this later.

With the slope and the intercept you can draw any line you like.



```{block2, type='rmdimportant'}
At times, you will see the intercept being referred with the lowercase letter $a$ or $b$ or $b1$, whereas the slope is at times named with the lowercase $m$ or $b2$ (those are not tokens).
```

Let's check what the intercept and the slope do to a line.

For plotting a line in R, we use the *abline* function, which we used before for drawing horizontal or vertical lines. You can also use that R-function to draw lines with specific slopes and intercepts. Let's try.

Lets plot three lines with different slopes, but the same intercept. 

```{r, message=FALSE,warning=FALSE}
#play around with the lines of code below to see how they work

#First lets create and empty plot and make the x and y axis nicer.
plot(0, 0,xlab="X",ylab="Y",  xlim=c(-4,4),ylim=c(-4,4.5),col="grey")
  abline(h=0,lwd=2, col="grey",lty=1) 
  abline(v=0,lwd=2, col="grey",lty=1) 
  
# lets draw a first line with an intercept of 1, and a slope of 0.5. 
# In abline, the first number will be the intercept and the second will be the slope.
  
Intercept=1  #lets create a variable for the intercept, which i choose to call intercept and make that variable = 1
Slope=0.5    #lets create a variable for the slope, which i choose to call slope and make that variable = 0.5

abline(Intercept, Slope, col="orange") #lets plot the line, with my slope and intercept and make it orange to differentiate it

# now, draw another line keeping the intercept the same and increase the slope to 1.5
Slope=1.5
abline(Intercept, Slope, col="blue") #lets make it blue to differentiate it

# try keeping the intercept the same and increase the slope to 2.5
Slope=2.5
abline(Intercept, Slope, col="green") #lets make it green to differentiate it

#We can verify the intercept, by plotting the point in Y, where the line intercept.
points(0,Intercept, pch="*",cex=3, col="red") #remember the intercept is when x=0.
```


Lets now change the intercept, but keep the slope the same


```{r, message=FALSE,warning=FALSE}
#play around with the parameters in the code below to see how they work

#plot first
plot(0, 0,xlab="X",ylab="Y",  xlim=c(-4,4),ylim=c(-4,4.5),col="grey")
  abline(h=0,lwd=2, col="grey",lty=1) 
  abline(v=0,lwd=2, col="grey",lty=1) 
  
# now try a combination of difference intercepts but the same slope.
  
Intercept=1  
Slope=0.5   

abline(Intercept, Slope, col="orange") #lets plot the line, and make it orange to differentiate it

# keep the slope the same and increase the Intercept to 2
Intercept=2
abline(Intercept, Slope, col="blue") #lets make it blue to differentiate it

# keep the slope the same and increase the Intercept to 3
Intercept=3
abline(Intercept, Slope, col="green") #lets make it green to differentiate it

```



### How to interpret the slope {-}

The slope should be interpreted as the amount of change in Y for a single unit of change in X.

Let's say, you are told that a given relationship between total rainfall (in litters) and the amount of time (in hours) is 500. From that slope alone, we can tell now that for each extra hour, there will be 500 litters of water falling.

```{block2, type='rmdimportant'}
The units of the slope are the units of the variable in Y divided by the units of the varaibles in X. Say the units of Y are in litters and the units of X are in hours, so the slope will be in litters/hours.
```

To contextualize the slope graphically, you can take any point along the trendline, and move horizontally one unit; the difference between the Y-point at the first X and the Y-point at X+1, is your slope...Lets check.


```{r, message=FALSE,warning=FALSE}
#plot first
plot(0, 0,xlab="X",ylab="Y",  xlim=c(0,10),ylim=c(0,5000),col="grey")
  abline(h=0,lwd=2, col="grey",lty=1) 
  abline(v=0,lwd=2, col="grey",lty=1) 

#lets draw a trend line with a slope of 500, that intercepts the orgin.
Intercept=0  
Slope=500  

abline(Intercept, Slope, col="orange") #lets plot the trend line, and make it orange to differentiate it


#lets move one unit along X, starting at zero
segments (0,0,1,0, col="red",lwd=3) #lets make that segment red

#from that point, X+1, lets move up 500 units, if the calculation is correct, then that segment of 500 units in Y should finish at the interception with the trend line...lets see

segments (1,0,1,500, col="blue",lwd=3) #here I draw a segment starting a x=1, y=0...until x=1, y=500
```

```{block2, type='rmdimportant'}
The slope represents the inclination of the regression line and reflects the amount of change in Y for a single unit of change in X.
```

### How to interpret the intercept {-}

The intercept is the expected value of Y when X=0.

Say that among children there is a relationship between age (in years) and size (in centimeters), and the Y-intercept is 35cm. What does that tell you?. 

So since the Y-intercept is the value at which X=0, then a Y-intercept of 35cm means the size of a child when he is zero years of age. Basically, children on average are born at 35cm of size.


```{block2, type='rmdimportant'}
The intercept represents the value of Y, when X= 0. The units of the intercept are the same units of the varaible in Y.
```

## Purpuse of the regression line {-}
As indicated earlier, a common name for the intercept is the lowercase letter $b$ and a common name for the slope is the lowercase letter $m$, in the regression model they come together as:

\begin{equation}
Y = mX + b
\end{equation}

The beauty of the regression model above is that by knowing $m$, and $b$, you can predict any value of Y, if you know X.

Say there is a relationship between years of higher education and salary, which is well defined with a regression equation with an intercept of $25,000 and a slope of $10,000 dollars/year. Given the units given, you should predict that salary is in the Y-axis and years of education in the X-axis. I can turn that into the equation, like:

\begin{equation}
Salary = 10,000 * \text{(Years Of Education)} + 25,000
\end{equation}

I can also display, that line in an XY plot, using the abline function:

```{r, message=FALSE,warning=FALSE}
#plot first
plot(0, 0,xlab="Years of education",ylab="Salary (in US dollars)",  xlim=c(0,10),ylim=c(0,150000),col="grey")
  abline(h=0,lwd=2, col="grey",lty=1) 
  abline(v=0,lwd=2, col="grey",lty=1) 

#lets draw a trend line with the given parameters
Intercept=25000  
Slope=10000  

abline(Intercept, Slope, col="orange") #lets plot the trend line, and make it orange to differentiate it
```

You can also ask questions, like what will be an average salary for a person that studies 4 years of higher education?..You simply replace the $x$ variable in the equation with the number 4 and do the mathematical calculation to get the average expected salary.

\begin{equation}
Salary = 10,000 * 4 + 25,000
\end{equation}

So the expected salary of a person that studies four years of higher education is $65.000.

<iframe src="https://www.impactsofclimatechange.info/H5P/wp-admin/admin-ajax.php?action=h5p_embed&id=18" width="958" height="329" frameborder="0" allowfullscreen="allowfullscreen"></iframe><script src="https://www.impactsofclimatechange.info/H5P/wp-content/plugins/h5p/h5p-php-library/js/h5p-resizer.js" charset="UTF-8"></script>

Ok, at this moment you know the basics of a linear regression model. In a nutshell, a linear regression model is a mathematical equation that includes a slope and an intercept which allows you to draw a line, from which you can also predict any value of Y given values of X.

## The least-squares line {-}
By now, you know the formulation (i.e., $Y= mX + b$) and the general purpose (i.e., to predict Y, given values of X) of a linear regression model. The next task in this chapter is to figure out how to get the best line that can be drawn through two variables that we want to relate.

Obviously, you can draw an infinite number of straight lines trhough a set of datapoints; then a key question is which line best describe the data?. That is the goal of the linear regression model: to draw a **best-fit line** through the datapoints. 

The best fitting line will be a line that minimizes the distance from each point to that line. In mathematical terms, that line is called the **least-squares regression line**. In a moment, you will see how that name (least-squares regression line) speaks by itself; I hope in a moment you will see how the term is self-explanatory. 

### Understanding the least-squares line {-}
Let's use an analogy, and imagine the best fitting line as a knife that cut a cake.

Take the figure below as an example. That blue line would be the knife and the red dotted lines the pieces of cake for each person. Will this be a good cut? Will that be the best-fitting line?


```{r, echo=FALSE, message=FALSE,warning=FALSE}

Slope=-.4
intercept=-2.5
#now let's do the plot
plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)

abline(intercept, Slope ,col="blue",lwd=1)

Names=c("Laura", "Peter", "Tom", "Chip","John")
text(X,Y,labels=Names,pos=2, col="orange")


Predicted=intercept+Slope*X
segments(X,Y, X, Predicted, col="red", lty=2,lwd=1)


```

Not quite s. I can imagine Laura not being happy about getting a smaller piece of cake. She may complain but there are four guys that will not support her, as they are likely very happy getting bigger shares of that cake. So in this case, this line is not the best describing the data.

What about the line below?. Will that be a good-fitting line to the data?

```{r, echo=FALSE, message=FALSE,warning=FALSE}

Slope=0.1
intercept=0
#now let's do the plot
plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)

abline(intercept, Slope ,col="blue",lwd=1)

Names=c("Laura", "Peter", "Tom", "Chip","John")
text(X,Y,labels=Names,pos=2, col="orange")


Predicted=intercept+Slope*X
segments(X,Y, X, Predicted, col="red", lty=2,lwd=1)

```

Well, may be...you may think the complains by Laura and Peter getting smaller pieces could be balanced out by the extra happiness of Chip and John getting larger shares of the cake.

However, there are some justified complains that can be avoided if we better cut that cake; a cut in which we can reduce the complains by every body. That can only be done drawing a line that minimizes the distance from each point to the line, and that is the so-called "least-square regression line".


Like in the image below:
```{r, echo=FALSE, message=FALSE,warning=FALSE}
LM=lm(Y ~ X)

plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)

abline(LM$coefficients[1], LM$coefficients[2] ,col="blue",lwd=1)

Names=c("Laura", "Peter", "Tom", "Chip","John")
text(X,Y,labels=Names,pos=2, col="orange")


Predicted=LM$coefficients[1]  +  LM$coefficients[2]*X
segments(X,Y, X, Predicted, col="red", lty=2,lwd=1)


```

### Deciphering the least-squares line {-}

But of all lines that we can draw through a set of data points, how can we know what is the best-fit line?, the one in which everybody is happy using the cake analogy?. There are actually different ways to get to that line...let's start by using brutal force, which may actually help us understand the idea behind the "least-squares".

We know for a fact that the 'best-fitting line' has to pass by the XY coordinates defined by the mean of all values in X and the mean value of all values in Y. At that inflection point, all data in X and all data in Y are evenly separated, so the best fitting-line has to pass by that given point.

Let's plot the mean value of X and the mean value of Y (Dashed grey-lines in the image below) and at the interception of those lines let's we plot the mean of X and the mean of Y (red dot in image below).



```{r, echo=FALSE,message=FALSE,warning=FALSE}
#lets use the same data we have been using so far for time studying and grades
plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)


      abline(h=MeanY,lwd=2, col="grey",lty=2)  #Ymean
      legend(2.8,MeanY,"Y-mean",box.col = "lightblue", bg = "lightblue",xjust=0, adj = 0.2)

      abline(v=MeanX,lwd=2, col="grey",lty=2)  #Xmean
      legend(-1.5,4.8,"X-mean",box.col = "lightblue", bg = "lightblue",xjust=0, adj = 0.2)


text(X,Y,labels=Names,pos=2, col="orange")

points(MeanX,MeanY, col="red",pch=16, cex=2)

```

Next, we draw a very inclined line passing trough that inflection point defined by the coordinates Mean-X and Mean-Y. Like in the image below.


Next, we measure the distance from each point to that line (red-dashed lines), which I indicate with the red numbers in the image below.

(Remember, the best line will be one with the smallest distance between each point and the given line)

```{r, echo=FALSE,message=FALSE,warning=FALSE}
Slope=1.9

LM=lm(Y ~ X)

plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)

      abline(h=MeanY,lwd=2, col="grey",lty=2)  #Ymean
      abline(v=MeanX,lwd=2, col="grey",lty=2)  #Xmean
      
      
abline(LM$coefficients[1], Slope ,col="blue",lwd=1)

Names=c("Laura", "Peter", "Tom", "Chip","John")
text(X,Y,labels=Names,pos=2, col="orange")


Predicted=LM$coefficients[1]  +  Slope*X
segments(X,Y, X, Predicted, col="red", lty=2,lwd=1)


Residuals=round (Y-Predicted,3)

text(X,(Y-Residuals/2),labels=Residuals,pos=4, col="red",cex=.8)

```

That difference between each point and the best-fitting line (red-dashed lines and red-numbers) are called **residuals**. It may also be named **residual errors**. 

Why call the residual also an error?. well, in the case of the linear regression, we want a model that best describes the data. Unfortunately, the best-line does not pass by every single one of the points, so that difference between the each point and the line is the error in the model.

Next, we add up all residual errors. Remember, the best fitting line will be that one with the least residual error:

$(`r Residuals[1]`)$ +  $(`r Residuals[2]`)$ +  $(`r Residuals[3]`)$ +  $(`r Residuals[4]`)$ +  $(`r Residuals[5]`)$  = $`r round(sum(Residuals),4)`$

Hmm, that does no make sense, the sum is `r round(sum(Residuals),4)`; yet we know that for Laura alone, the residual error is $(`r Residuals[1]`)$.


So how can you add up positive and negative errors, so they do not cancel each other out?. You should know....

I hope you say by "squaring" pr elevating the given number to the power of 2. If you recall from the last chapter, the approach of squaring any value allows to convert all values positive or negative to non-negative values...lets try.


$(`r Residuals[1]`)^2$ +  $(`r Residuals[2]`)^2$ +  $(`r Residuals[3]`)^2$ +  $(`r Residuals[4]`)^2$ +  $(`r Residuals[5]`)^2$ =$`r round(sum(Residuals^2),4)`$

Ok, that is more like it. That value that we just calculated is call the **Sum of Square Errors** or SSE. The best fitting line, will be that one in which SSE is the smallest. There why the approach to finding out the best finding line is called the "least squares".

Lets finish the exercise by brutal force, drawing lines with different inclinations and estimating their SSE. Like the figure below.


```{r, out.width = "100%", echo= FALSE, fig.align = 'center',fig.cap = 'Finiding the line with the least-squares error'}
knitr::include_graphics("images/test.GIF")
```


We can compare the sum of squares errors, SSE, of each line to find out the one with the least, like this:


```{r, echo=FALSE,message=FALSE,warning=FALSE,results='hide'}
 library(RColorBrewer)
ColSca <- rev(brewer.pal(9, 'YlOrRd'))   #From discrete colors  brewer.pal(9,"YlOrRd")
ColSca <- colorRampPalette(ColSca)  #to continuous

#ColSca = colorRampPalette(heat.colors(10))

Data=read.csv("D:/GEO380/Datasets/RSS.csv")
colnames(Data)=c("X","Line", "SSE")
Data$colors= ColSca(100)[cut(Data$SSE, breaks = seq(0.9, 27.5, len = 100))] 


plot(Data$SSE~Data$Line, xlab="Line number", ylab="SSE",ylim=c(0,12))

Winner=Data[which.min(Data$SSE),]
points(Winner$Line,Winner$SSE, pch="*",cex=4, col="red")

```

And it seems we have a winner, our line number `r Winner$Line` was the one with the lowest sum of squares, which I separate below:





```{r, echo=FALSE, message=FALSE,warning=FALSE}
library(TeachingDemos)

LM=lm(Y ~ X)
predicted <- predict(LM) 
residuals <- round(residuals(LM),1)

#now let's do the plot
plot(X, Y,xlab="X",ylab="Y", col="blue",pch="*", cex=2, xlim=c(-4,4),ylim=c(-4,4.5),xaxt = 'n',yaxt = 'n', ann=FALSE)
abline(LM$coefficients[1], LM$coefficients[2] ,col="blue",lwd=1)
      abline(h=MeanY,lwd=2, col="grey",lty=2)  #Ymean
      abline(v=MeanX,lwd=2, col="grey",lty=2)  #Xmean
segments(X,Y, X, predicted, col="orange", lty=1,lwd=2)

shadowtext(X,(Y-residuals/2),labels=residuals,pos=4, col="red")
text(X,Y,labels=Names,pos=2, col="orange")

```

Ok, I hope is clear then what is it the least squares approach to find the best fitting line. Next, we will learn how to estimate the parameters for the intercept, $b$, and the slope, $m$, that define the line in the linear regression model, the smart and easy way.

## Estimating the least-squares line {-}
The ultimate goal of a linear regression model is to identify the parameters for the intercept, $b$, and the slope, $m$, of the line that minimize<span style="color: blue;">s</span> the sum of square errors also called least-square errors.

```{block2, type='rmdnote'}
The linear regression line is expresed as $Y = mX + b$
```

### Refreshing the Slope {-}
Before we get into the mathematical equation to describe the slope of the linear regression model, lets review your 4th grade geometry about how to calculate the slope between two points.

If you recall Ms. Smith, your Match teacher in 4th grade, she told you that the slope between two points can be calculated as:

\begin{equation}
Slope = m = \frac{\Delta y}{\Delta x}= \frac{y_{2}-y_{1}}{x_{2}-x_{1}}
\end{equation}

Basically, the change in Y divided by the change in X. Put another way, if I change X by 1 unit, how much will Y change?

Lets check the math, using a line for which we know the slope, using the R-function *abline*, as we did earlier.



```{r, message=FALSE,warning=FALSE}
Slope= 2  #lets set a line with a slope of 2
Intercept=1 # We do not need the intercept but lets use a value of 1 as an example

plot(0,0,xlab="X",ylab="Y", col="blue",pch=".", cex=2, xlim=c(-0,4),ylim=c(0,8), yaxs="i", xaxs="i") #lets create an empty plot

#Next we draw the line with the known slope:
abline(Intercept, Slope ,col="blue",lwd=1) 

#Now place two points along that line...say a point at the coordinates (1,3) and another point at the coordinates (3,8).
#If you recall, the coordinates of a point are x and y given between parenthesis.

# draw Point 1 in the plot and put a label to it.
points(1,3,pch=21, col="black",bg="yellow",cex=2,lwd=.1) #First point
text(1,3, labels="(1,3)",pos=2) #lets create a label

# do the same for Point 2
points(3,7,pch=21, col="black",bg="yellow",cex=2,lwd=.1) #second point
text(3,7, labels="(3,7)",pos=2) #lets create a label  

#lets draw a segment for the change X, which if x0=1, and x=3, then the difference is 2.
segments(x0=1,y0=3,x=3,y=3, col="red", lty=2,cex=2) #lets draw the segment for the difference in x between the two points, and make it red
text(2,3, labels="2",col="red",pos=1) #lets create a label for that segment

#lets draw the difference in Y, which basically y0=3, and y=7, so the difference, delta, is 4.
segments(x0=3,y0=3,x=3,y=7, col="blue", lty=2,cex=2) #lets draw the segment for the difference in y between the two points
text(3,5, labels="4",col="blue",pos=4) #lets create a label  
```

Continuing with the example above, the change in Y, also called $\Delta y$, was 4. The change in X, also called $\Delta x$, was 2. So, the slope can be calculated as:

\begin{equation}
m = \frac{\Delta y}{\Delta x}= \frac{4}{2}=2
\end{equation}


That is exactly, the slope we set in abline, which serve the purpose to illustrate that the slope of a line is simply the change in Y divided by the change in X.

With that in mind lets now calculate the slope of the regression line

## The slope {-}
Why not to use Ms. Smith way to calculate the slope for the regression model?. Well, we use the same principle, difference in Y divided difference in X, but we cannot use the same formula  because in the case of a regression model we have more than two points.


There are numerous ways to calculate the slope, $m$, of a linear regression model. However, the simplest is:

\begin{equation}
Slope = m = \frac{\sum(x-\bar{x})*(y-\bar{y})}{\sum(x-\bar{x})^2}
\end{equation}

We have seen those terms before. The numerator was included in the covariance (i.e., how two variables trend together) and the denominator was included in the variance (i.e., how disperse are the data in one variable).

If you think about it...that equation speaks by itself. 

As we mentioned earlier, the slope of any line can be described as the change in Y divided by the change in X. 

From Chapter 5, the Section on Dispersion, you may recall that the best indicator of the variability in a variable was the variance, which has as term $\sum(x-\bar{x})^2$. From chapter 6, you may recall that the best indicator of the tendency between two variables was the covariance $\sum(x-\bar{x})*(y-\bar{y})$.

As such, we if want the slope among a set of points that follows their central tendency, then the change in X will be $\sum(x-\bar{x})^2$, and the change in Y will be how Y varies with X, which mathematically is $\sum(x-\bar{x})*(y-\bar{y})$.

You can phrase the equation for the slope in the regression line in a different way, if X changes by the variance of X, then Y will change by the covariance of Y and X.


Ok, now that the equation for the slope is clear, lets calculate it.

Let's use the data we have been using on the time studying and grades,

```{r, echo= FALSE, message=FALSE,warning=FALSE}
Names=c("Peter","Laura", "John", "Chip", "Tom")
Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5)
Grade=c(55, 64, 75, 82,95)
XDif=Hours_Studying-mean(Hours_Studying)
YDif=Grade-mean(Grade) 

Data=data.frame(Names=Names,Hours_Studying=Hours_Studying,Grade=Grade,XDif=XDif,YDif=YDif)

colnames(Data) <- c("Names", "Hours Studying", "Grade","$$(x-\\bar{x})$$",   "$$(y-\\bar{y})$$")


knitr::kable(
  Data, longtable = TRUE, booktabs = TRUE,
  caption = 'Grades and time studying Stats', align = "rcccc"
)
```

<span style="color: white;">a</span>
<span style="color: white;">a</span>

So, all we have to do is to replace the differences in X and the difference in Y in the slope formula:



\begin{equation}
slope = m = \frac{(`r XDif[1]`*`r YDif[1]`) + (`r XDif[2]`*`r YDif[2]`) + (`r XDif[3]`*`r YDif[3]`) + (`r XDif[4]`*`r YDif[4]`) +(`r XDif[5]`*`r YDif[5]`)}   { (`r XDif[1]`)^2 + (`r XDif[2]`)^2 + (`r XDif[3]`)^2 + (`r XDif[4]`)^2 +(`r XDif[5]`)^2}
\end{equation}



\begin{equation}
slope = m = 9.6 
\end{equation}


So the slope is, m, is equal to `r round(sum(XDif*YDif)/sum(XDif^2),2)`. The units will be the units in Y (i.e., grade), divided by the units of x (i.e., hours studying). So if the unit of change in X in one hour then the unit of change in Y will be `r round(sum(XDif*YDif)/sum(XDif^2),2)` points higher in grade. 

Put another way, for each extra hour that you study a week, you can expect a `r round(sum(XDif*YDif)/sum(XDif^2),2)` points higher in your grade, neat or what?.

I should mention that there as several other ways to calculate the slope of the linear regression model, but I find them a bit more complicated and difficult understand. I prefer to use the simple formula above, but be aware there are a few other ways to get to the slope of the least-squares line.

## The intercept {-}

There are numerous combulated equations to calculate the intercept of the linear regression model. Here I want us to use a procedure that you probably learned in 6th grade. 

If you look at the linear regression model equation,

\begin{equation}
Y = mX + b
\end{equation}

You could isolate the intercept, $b$, as:

\begin{equation}
b = Y - mX
\end{equation}

If you recall, the line of the least-squares regression line has to pass by the coordinates defined by the mean value of X and the mean value of x. So to stimate the interecpt all you have to do is to replace the $X$ and $Y$ parameters of the linear regression equation by the mean of X, $\bar{x}$, and the mean of Y, $\bar{y}$. Like this:

\begin{equation}
b = \bar{y} - m\bar{x}
\end{equation}

Because you also know the slope from the prior section. Then you have all parameters of the equation above to stimate the intercept.

Let's estimate the intercept with the data on the time studying and grades. From that data, the mean of X, $\bar{x}$, was `r mean (Hours_Studying)`, and the mean of Y, $\bar{y}$, was `r mean (Grade)`. So, the intercept for that linear regression is:


\begin{equation}
Intercept = b =`r mean (Grade)` - `r mean (Hours_Studying)`*`r round(sum(XDif*YDif)/sum(XDif^2),3)`

\end{equation}

\begin{equation}
Intercept = b =`r  round(mean (Grade) - mean (Hours_Studying) * sum(XDif*YDif)/sum(XDif^2),3)`

\end{equation}

So the Y-Intercept of the least-square line for the relationship between grades and time studying is
`r  round(mean (Grade) - mean (Hours_Studying) * sum(XDif*YDif)/sum(XDif^2),2)`. You can interpret that value as the grade that you can expect in my class if you do not study a single hour of the week. Hmm, not good.


## Linear regression in R {-}
By now you know the equation of the linear regression model, the idea behind the least-squares approach, and how to calculate and interpret the different elements of the equation.

R can calculate all those equations automatically for you using the *lm* function, which stands for *linear model*. The cool think is that you can now interpret the results from that function. Lets try using eh data on grades and time studying.





```{r, echo= FALSE, message=FALSE,warning=FALSE}
#lets start by bringing back the data 
Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5)
Grade=c(55, 64, 75, 82,95)

# the function lm, requires you to simply set the model as y ~ x. In our case, grades are the dependent variable, so it goes on the Y-axis, and time studying is the independent variable, so it goes in the x-axis.
lm (Grade~Hours_Studying)

```

The results of the linear model, *lm*, in R are pretty straightforward, check the image below.
```{r, out.width = "90%", echo= FALSE, fig.align = 'center',fig.cap = 'Linear regression output in R'}
knitr::include_graphics("images/lm in R.png")
```

## The coeficient of determination {-}
There is one final cool thing about the linear regression model: it allows you to quantify a neat parameter called the coefficient of determination, $r^2$.

That parameter is useful for two main reasons:

1. It tells you how good is the relationship between the two variables, although you sort of know this from the correlation coefficient, $r$, that you estimated earlier.

2. It tells you the percent of variance of Y that is explained by X.

Say you found a relationship between plant size and nutrient input with an $r^2$ of 85%. This tells you that 85% of the variability you have among your plants can be explained by the input of nutrients. It will probably goo idea to add nutrient to the plants.

Say that in the opposite you found an $r^2$ of 5%. Then only 5% of the variability in plant size can be explained by nutrient input. In this case, it may be wastefull to add nutrients to the plants, since they affect so little how plants will grow. This may be the case if you have some good soil, so, no need for nutrients.

In this example about nutrients and plants, you can see how $r^2$ let you make inferences about the strength of the relationships. 

The only bad thing about this $r^2$, as oppose to the correlation coefficient, $r$, is that you cannot know the direction of the relation. So you may still have to relay on the correlation coefficient, $r$, to know in which direction are the two variables related.

Before we get into the mathematics of the $r^2$, check out this brief explanation:

<iframe width="560" height="315" src="https://www.youtube.com/embed/wHvxTZWV-sE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


The coefficient of determination, $r^2$, is mathematically calculated as:



\begin{equation}
\text{Coefficient of Determination} = r^2 = \frac{SSmean - SSfit}{SSmean}
\end{equation}


Let's break that equation into its pieces to see what is doing. 

$SSmean$, stands for Sum of Squares of the Mean. That is the same term we have used before $\sum(y - \bar y)^2$. Basically, how far from the mean is each point. If we were to divide that by the number of samples, you will get the variance that we studied earlier.

Just to refresh, you take the mean of all values in Y (horizontal line, in image below), for each point measure the distance to that mean (dotted lines), then you square each value and add them together. If you did not square them, when summing them, the result will be zero. 

```{r, out.width = "20%", echo= FALSE, fig.align = 'center',fig.cap = 'Sum of Squares from the Mean, SSmean'}
knitr::include_graphics("images/Sum of Squares.png")
```

You need to think of the Sum of Squares of the Mean,$SSmean$, as the variability in Y.


$SSfit$, stands for Sum of Squares around the Fit. Let's see what this means.

Take the data on grades and time studying, relate grades against times studying, and find the best line (Orange line in figure below). Then for each point measure the distance from the point to the line, or so-call *residuals* (red-dotted lines). Take each residual, square it, and then add them together. What you get is the Sum of Squares around the Fit, $SSfit$.


```{r, out.width = "50%", echo= FALSE, fig.align = 'center',fig.cap = 'Sum of Squares around the Fit, SSfit'}
knitr::include_graphics("images/Sum of Squares Fit.png")
```

You need to think of the Sum of Squares around the Fit, $SSfit$, as the variability in Y that was not explained by X.

Remember, the regression line is the mathematical formulation of how Y relates to X, whatever is not accounted for by that line are the residuals, or the variation in Y, that remains to be accounted for.

So, it you look at the formulation for $r^2$, basically, you are trying to quantify the fraction of variability of Y that was accounted for by the relationship of Y to X. Easy right?

Let's calculate $r^2$,

```{r, echo= FALSE, message=FALSE,warning=FALSE}
#take the data on grades and time studying
X=c(0.5, 1.8, 2.4, 3.8, 4.5) #hours studying
Y=c(55, 64, 75, 82,95)       #grades

# lets stimate the regression line using lm, and lets put that model in a variable
LM = lm (Y~X)   #this is the linear model between Grades~Hours_Studying

#you can find out the residuals of that model using the R-Fucniton residuals.
Residuals= residuals (LM)  #here we create a vector with the residuals from our model

#Estimate SSfit
SSFit= sum (Residuals^2)  # here you are squaring each residual, then adding them

#Lets now stimate SSMean
DeltaY=Y-mean(Y)  # here you take each value in Y and substract it to the mean of Y
SSMean= sum(DeltaY^2) #here you square each score in the line above, square it and sum them together

#we have all we need for the calculation of the R2.

R2=(SSMean-SSFit)/SSMean
R2
```



So, the $r^2$ of the relationship between grades and time studying is `r round(R2,2)`. That is the fraction of the variability in grades that is explained by the amount of time students study. You can also report the $r^2$ as a percentage by multiplying the fraction by 100.

In R, the $r^2$ is outputted as part of the *lm* function in combination with the function *summary*, like this:

```{r, echo= FALSE, message=FALSE,warning=FALSE}
summary (LM)   #Here LM is the linear model, lm, we create in the code just above.  
```

The image below indicates the outputs, we have studied so far:
```{r, out.width = "80%", echo= FALSE, fig.align = 'center',fig.cap = 'lm outputs'}
knitr::include_graphics("images/R2.png")
```

## The significance {-}

Several times in this book, we have mentioned how there is always a chance that any result can arise by chance alone. The linear regression model is not different. There may still be a chance that you find an $r^2$ similar to the one you found if you did not use any variable. 

any two random variables, with the same number of samples, will give you the same regression line and  than you found for two given variables.

To rule out that possibility, so guys 




