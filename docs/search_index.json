[["6-correlation.html", "6 Correlation", " 6 Correlation One of the core goals in statistics is to calculate the strength of the relationship between two variables. That goal can be applied to a broad diversity of research questions. Say you did an experiment to test the effect of temperature on plants and say you have plantss growing at different temperatures. In this case, you probably would like to relate temperature to a dependent variable like say plant size. Instead, you may be interested to see how rain affects the common cold, in which case you may wish to relate the amount of daily rainfall and the daily number of hospitalizations by cold. The possibilities are endlessbut at the end of the dayall of them come down to simply assessing the strength of the relationship between two variables. For this specific purpose, we use the correlation and/or the regression analysis. . Expectation for this chapter At the end of this chapter, it is expected that you: Can create a publication quality R scatterplot to visualize the relationship between two variables. Define the type of relationship between two variables (e.g., positive, negative, non-linear, non-existent). Calculate the coefficient of correlation between two variables and understand what it tells you about the relationship between the two variables. Alright, a lot to cover, so lets get started. "],["visualization-of-relationships.html", "Visualization of relationships", " Visualization of relationships When looking into correlations and relationships the main display tool is the scatterplot. You already know how to use the scatterplot from Chapter 4. From that chapter, you should also already know the criteria for a plot to meet the standards required for publication. For the purpose of displaying a correlation and/or regression between two variables, the key consideration is if you assume if one variables influences the other. From Chapter 1, the section on Experiments, you learned that the variable that influences another variable is called the independent variable. The one variable that is affected is called the dependent variable. If the purpose of your study does not involve assessing if one variable influences the other, then it does not matter what variable you use for the X- or Y-axis. However, if your study assumes that one variable influences another one, the independent variable is located in the X-axis, while the dependent variable should be located in the Y-axis. Lets get to work in R. Lets work on an interesting relationship I sow in the New York Times, between the people that voted for Donald Trump by State and the degree of higher education at those States. First, download the two databases from Here and Here. Next, load the data into R (remember Chapter 3, section about loading your own data). TrumpVoters_by_State &lt;- read.csv(&quot;D:/GEO380/Datasets/TrumpVoters.csv&quot;) #Fraction of people by State that voted for Trump HigherEducation_by_State &lt;- read.csv(&quot;D:/GEO380/Datasets/US-Pop-HigherEducation.csv&quot;) #Fraction of people by State that have higher education degrees Next check the data were loaded correctly and review the structure of the data: head (TrumpVoters_by_State) ## State TRUMPVoteAsFraction ## 1 Alabama 0.6208 ## 2 Alaska 0.5128 ## 3 Arizona 0.4867 ## 4 Arkansas 0.6057 ## 5 California 0.3162 ## 6 Colorado 0.4325 Review the same for the second database: head (HigherEducation_by_State) ## State BachelorDegreePerStateAsFraction ## 1 Alabama 0.087 ## 2 Alaska 0.101 ## 3 Arizona 0.102 ## 4 Arkansas 0.075 ## 5 California 0.116 ## 6 Colorado 0.140 The data appear to have loaded correctly. But I have the data I want in two different data.frames, so I have to merge them. In this case, I have one variable in common to the two data.frames that I can use to merge them by; that is the variable State. So, lets merge our two data.frames by State. p i Data=merge(TrumpVoters_by_State,HigherEducation_by_State,by=&quot;State&quot;) When merging databases, things can get tricky as you need to have at least one column in common to merge by. You need to ensure that in both databases, each field uses the same names for the data. For instance, say you have a common column call state, but in one database the data are shown by full names but in the other by abbreviate name. In this case, the merge function will return an empty database, because the two databases do not have variables that can be matched. In such cases, you need to modify the original databases to ensure the two data.frames share a common variable, with similary named data. Lets review the new merged database: u head (Data) ## State TRUMPVoteAsFraction BachelorDegreePerStateAsFraction ## 1 Alabama 0.6208 0.087 ## 2 Alaska 0.5128 0.101 ## 3 Arizona 0.4867 0.102 ## 4 Arkansas 0.6057 0.075 ## 5 California 0.3162 0.116 ## 6 Colorado 0.4325 0.140 Ok, now lets plot the data. In this case, I think the fraction of people that voted for Trump could be function of how educated they were, not the other way around; that would be like saying that Trump affected the degree of education of each State.hmmwhen I say like that is does not sound that unlikely, ah?. This distinction is important because you need to determine which variable goes in the Y axis, and which one on the X-axis. If I think, the votes for Trump were influenced by the level of their education, them level of education will be the independent variable and then it will be located in the X-Axis. In this reasoning, The percent of the States population that voted for trump will be the dependent variable, and so, it should be located on the Y-axis. Lets do that plot: plot(Data$TRUMPVoteAsFraction~Data$BachelorDegreePerStateAsFraction, ylab=&quot;Trump voters by State (Fraction)&quot;,xlab=&quot;High degree education (Fraction of population)&quot;) Hmm, from this visualization alone you can tell something is cooking herethe least educated states voted for Trump more so than States with more educated populations. Lets explore this relationship in more detail, as a case example. "],["linear-relationships.html", "Linear relationships", " Linear relationships Generally speaking two variables can related in three difference fashions: linear, non-linear, or non-related. There are different mathematical approaches to tackle each type of relationship. Here we will cover only linear relationships, but I want you to learn to at least identify the other types. Lets start with the linear relationships. Linear relationships, as the name sort of indicates are better described by a straight line. This type of relationships can be further separated between positive relationships, in which Y-increases as X-increases and negative relationships, in which Y-decreases as X-increases. Obviously, you also have the option that the two variables do not relate to each other (see figure below) Figure 6.1: Types of relationships "],["non-linear-relationships.html", "Non-linear relationships", " Non-linear relationships Relationships that cannot be well described with a linear model, are called non-linear relationships. Non-linear relationships can take all sorts of shapes, names and mathematical approaches to define them. They will not be covered as part of this class, but you should be aware they exist. Figure 6.2: Types of relationships "],["the-covariance.html", "The Covariance", " The Covariance The strength of the linear association between two variables is mathematically measured with the so-call Correlation Coefficient. The Correlation Coefficient is abbreviated with the lowercase letter \\(r\\) (that is not a token). However, to estimate the Correlation Coefficient, you need to first estimate the so-call Covariance. Check a brief explanation of the covariance in the following video: The covariance is an extension of the variance calculation we did earlier to measure the spread of the data in a variable, but in the covariance we analyze two variables. In fact, if you were to assess the relationship between a variable and itself, the covariance will be identical to the variance. In a nutshell, the covariance tells you if the differences in two variables are trending on the same direction. Mathematically, the covariance is calculated with the following equation: \\[\\begin{equation} Covariance = COV(XY) = \\frac{\\sum_{i=1}^n (x -\\bar{x})*(y -\\bar{y})}{n-1} \\end{equation}\\] Lets try a simple example to estimate the Covariance. Lets consider the relationship that exist between the time that you study for my class and the grade that you get. Figure 6.3: Time stuying relates to test scores Say, I asked five students how long they studied each week and the grade they got in my prior classes. These were the data: Table 6.1: Grades and time studying Stats Names Hours_Studying Grade Peter 0.5 55 Laura 1.8 64 John 2.4 75 Chip 3.8 82 Tom 4.5 95 As always, we start by plotting the data: StudyingTimes= data.frame( Names=c(&quot;Peter&quot;,&quot;Laura&quot;, &quot;John&quot;, &quot;Chip&quot;, &quot;Tom&quot;), #lets create a data.frame with three columns Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5), Score=c(55, 64, 75, 82,95)) #now let&#39;s do the plot plot(Score~Hours_Studying,data=StudyingTimes,xlab=&quot;Hours a week studying&quot;, ylab=&quot;Final class score (%)&quot;) Lets break the calculation of the covariance into its parts so we can better appreciate what it does. First, we calculate the mean of alll values in X and the difference from each value to that mean: Figure 6.4: Differences in X Lets do the same for the Y-axis: Figure 6.5: Difference in Y Following the equation of the covariance, for the first point in the data (i.e., Peter), we place his difference to the mean in X (i.e, -2.1) and its difference to the mean in Y (-19.2), in the numerator. Like this: Figure 6.6: Difference in X We can do that for all data points and obtain: \\[\\begin{equation} Covariance = COV(XY) =\\frac{\\sum_{} (-2.1)(-19.2) + (-0.8)(-10.2) + (-0.2)(0.8) + (1.2)(7.8) + (1.9)(20.8)}{5-1} \\end{equation}\\] . \\[\\begin{equation} Covariance = COV(XY) =24.3 Hours*Test Score \\end{equation}\\] Hmm???, Right?as mentioned earlier the score of the covariance by itself is hard to interpret, but it may still provide useful information about the trend of the datain this case the covariance is positive, indicating that the differences in X trend in a positive direction as the differences in Y. Basically, as students study more they get higher gradesPlease remember that!and here it goes a token j In R, the covariance is calculated with the cov function: cov(StudyingTimes$Score,StudyingTimes$Hours_Studying, use = &quot;everything&quot;, method = &quot;pearson&quot;) ## [1] 24.3 "],["the-correlation-coefficient-r.html", "The Correlation Coefficient, r", " The Correlation Coefficient, r As indicated earlier, the strength of the linear association between two variables is mathematically measured with the so-call Correlation Coefficient. At times, it is also called the Pearson product-moment correlation coefficient, after Karl Pearson, who is credited with formulating r. Mathematically, the correlation coefficient, \\(r\\), is calculated with the following equation: \\[\\begin{equation} r = \\frac{cov (XY)}{Sx * Sy} \\end{equation}\\] Basically, the correlation coefficient, \\(r\\),is the Covariance divided by the multiplication of the standard deviation of the data in X and the standard deviation of the data in Y. If you think about this equation, the covariance is the product of the differences in X and Y. While the standard deviations are independently the differences in X and the differences in R. So, in practical terms, the correlation of coefficient is an standardized metric. It will never be smaller than -1 or larger than 1. That is why the correlation coefficient is such a nice term to access the over tendency between two variables. if it is closer to -1 then you know the data probably follow a strong and negative trend. If it is close to 1, then the data follow a positive and strong trend. If it is closer to zero, then the data is all over the place (there is not correlation). Towards the end of the chapter, we will do over more details on how to interpret the correlation coefficient. Lets calculate it, COVXY= cov(StudyingTimes$Score,StudyingTimes$Hours_Studying, use = &quot;everything&quot;, method = &quot;pearson&quot;) #Covariance SDX= sd(StudyingTimes$Hours_Studying) #Standard deviation for X SDY= sd(StudyingTimes$Score) #Standard deviation for Y r=COVXY/(SDX*SDY) r ## [1] 0.9817004 So the coefficient of correlation between the time that you study and the score in my class is 0.98. That means the more time you study the higher your gradenice, ah??? In R, the coefficient of correlation can be calculated direvtly with the fucntion cor, cor(StudyingTimes$Score,StudyingTimes$Hours_Studying, method = &quot;pearson&quot;) ## [1] 0.9817004 Alternative formulation While looking into the correlation coefficient you will likely see alternative formulations of it that yield the same or very close approximations. For instance, you may find it formulated like this: \\[\\begin{equation} r = \\frac{1}{n-1} \\sum_{}\\frac{x-\\bar{x}}{Sx}\\frac{y-\\bar{y}}{Sy} \\end{equation}\\] This equation above, is pretty much the same we used earlier, but reorganizing the parts. At times, you can also defined as: \\[\\begin{equation} r = \\frac{n * \\sum_{} xy- (\\sum_{} x)*(\\sum_{} y)}{\\sqrt {n * \\sum_{} x^2- (\\sum_{} x)^2 } * \\sqrt {n * \\sum_{} y^2- (\\sum_{} y)^2 }} \\end{equation}\\] Which will yield a very close approximation to the equation we used earlier. Be careful! The notation \\(\\sum_{}x^2\\) means first square \\(x\\) and then calculate the sum, whereas \\((\\sum_{}x)^2\\) means first sum the x values and then square the result. For the equation above, all we have to compute \\(\\sum_{}x\\), \\(\\sum_{}y\\), \\(\\sum_{}x^2\\), \\(\\sum_{}y^2\\), and \\(\\sum_{}x*y\\), Let try, for the sake of being sure and for you to use some tools from R. Y=StudyingTimes$Score X=StudyingTimes$Hours_Studying SumX= sum(X) #sum all values of x SumY= sum(Y) #sum all values of y SumX2=sum (X^2) #sum all values of x^2.. SumY2=sum (Y^2) # sprintf(&quot;%.0f&quot;,sum (Y **2)) SumXY=sum (X*Y) #sum all value of x * y n=nrow(Data) # the number of observations is basically the number of rows in the database SumY2 ## [1] 28495 The results are: \\(\\sum_{}x\\) = 13 \\(\\sum_{}y\\) = 371 \\(\\sum_{}x^2\\) = 43.94 \\(\\sum_{}y^2\\) = 28495 \\(\\sum_{}x*y\\) = 1061.8 n = 49 Now we plug those values into the coefficient of correlation, r, equation: . \\[\\begin{equation} r = \\frac{n * \\sum_{} xy- (\\sum_{} x)*(\\sum_{} y)}{\\sqrt {n * \\sum_{} x^2- (\\sum_{} x)^2 } * \\sqrt {n * \\sum_{} y^2- (\\sum_{} y)^2 }} \\end{equation}\\] . \\[\\begin{equation} r = \\frac{49 * 1061.8- (13)*(371)}{\\sqrt {49 * 43.94- (13)^2 } * \\sqrt {49 * 28495- (371)^2 }} \\end{equation}\\] . In R, it is basically: r=(n*SumXY-(SumX*SumY)) / ( sqrt(n*SumX2 -SumX^2) * sqrt(n*SumY2 -SumY^2) ) r ## [1] 0.9446396 \\[\\begin{equation} r = 0.94 \\end{equation}\\] Hmm, what do you think is causing the difference to the original calculation? "],["interpreting-r.html", "Interpreting r", " Interpreting r The correlation coefficient, \\(r\\), is unitlessby dividing by the standard deviations the products of the differences in x and y, the result loose its units. This means the value can be compared among studies for any set of x and y variables but can also be interpreted between a set of standard values (i.e., -1 to 1). In short, the correlation coefficient, \\(r\\), can range from -1 (perfectly negative correlation) to 1 (perfectly positive correlation). Like in the images below. Figure 6.7: Examples of correlation coefficients "],["causation.html", "Causation", " Causation The correlation coefficient measures the strength of a linear relationship between two variables. Thus, it makes no implication about cause or effect. The fact that two variables tend to increase or decrease together does not mean that a change in one is causing a change in the other. At times, two variables may be strongly correlated because they are equally correlated to a third (either known or unknown) variable. Such variables are called lurking variables. Lets take the following example: Figure 6.8: Examples of lurking variables In the figure above, the dashed lines show an association. The solid red arrows show a cause-and-effect link. The variable x is explanatory, y is a response variable, and z is a lurking variable. Basically, in example B, you will find a strong correlation between x and y, not because they are causality related, but because they are both strongly affected by a variable you did not measure (i.e., Z in the case example above) or a so-call lurking variable. The effect is that you can make spurious conclusion if you interpret the coefficient correlation as a cause and effect. For instance, you know that in tropical countries there is a strong correlation between the consumption of ice cream and shark attacks?. One could conclude that sharks like sweet people?. Hmmthink about this correlation.is there a lurking variable here?. what could it be? Figure 6.9: Example of a lurking variable "],["p-value.html", "p-value", " p-value If you were to take any two random variables and correlate them together, you will still get a correlation value. You may think that if the two variables are random, then the correlation will be close to zerowell you are wrong. It turns out that even by random chance alone variables may still be correlated. The chances of getting a higher correlation increase the lower the sample size. Just think about it, if you were two correlate any two data points, almost certainly your correlation will be 1 or -1. To address this potential caveat, we use established tables that tell you if the correlation you found could be different from random. Basically, you need to know if the correlation is significant or not. That word significant is tricky because it implies a level or threshold of error you are willing to take. That threshold is call critical value, again it is the margin of error you are willing to accept as error. In biology, we normally give ourselves a 5% chance of being wrong (p&lt;0.05). But at times, it is preferable to be more certain and we take only a 1% chance (p&lt;0.01). Remember that the parameter \\(n\\) is call sample size. For the case of the correlation, the significance of the correlation can be assessed quickly with a probability table, as the one shown below: Figure 6.10: Significance of correlations Lets use the example we have been working on about the relationship between the time students study and their grades. In that case, \\(r\\)=0.94 and the sample size \\(n\\) was five students. HOW TO USE TABLE 4-6 TO TEST R 1. First compute r from a random sample of n data pairs (x, y). 2. Find the table entry in the row headed by n and the column headed by your choice of a. Your choice of a is the risk you are willing to take of mistakenly concluding that r ? 0 when, in fact, r 5 0. 3. Compare r to the table entry. (a) If r  table entry, then there is suffi cient evidence to conclude that r ? 0, and we say that r is signifi cant. In other words, we conclude that there is some population correlation between the two variables x and y. (b) If r table entry, then the evidence is insuffi cient to conclude that r ? 0, and we say that r is not signifi cant. We do not have enough evidence to conclude that there is any correlation between the two variables x and y. Usually a scatter diagram does not contain all possible data points that could be gathered. Most scatter diagrams represent only a random sample of data pairs taken from a very large population of all possible pairs. Because r is computed on the basis of a random sample of (x, y) pairs, we expect the values of r to vary from one sample to the next (much as the sample mean x varies from sample to sample). This brings up the question of the signifi cance of r. Or, put another way, what are the chances that our random sample of data pairs indicates a high correlation when, in fact, the populations x and y values are not so strongly correlated? Right now, lets just say that the signifi cance of r is a separate issue that will be treated in Section 11.4, where we test the population correlation coeffi cient r (Greek letter rho, pronounced row). r 5 sample correlation coeffi cient computed from a random sample of (x, y) data pairs. r 5 population correlation coeffi cient computed from all population data pairs (x, y). There is a less formal way to address the signifi cance of r using a table of critical values or cut-off values based on the r distribution and the number of data pairs. Problem 21 at the end of this section discusses this method. The value of the sample correlation coeffi cient r and the strength of the linear relationship between variables is computed based on the sample data. The situation may change for measurements larger than or smaller than the data values included in the sample. For instance, for infants, there may be a high positive correlation between age in months and weight. However, that correlation might not apply for people ages 20 to 30 years. The correlation coeffi cient is a mathematical tool for measuring the strength of a linear relationship between two variables. As such, it makes no implication about cause or effect. The fact that two variables tend to increase or decrease together does not mean that a change in one is causing a change in the other. A strong correlation between x and y is sometimes due to other (either known or unknown) variables. Such variables are called lurking variables. In ordered pairs (x, y), x is called the explanatory variable and y is called the response variable. When r indicates a linear correlation between x and y, changes in values of y tend to respond to changes in values of x according to a linear model. A lurking variable is a variable that is neither an explanatory nor a response variable. Yet a lurking variable may be responsible for changes in both x and y. Page 158 "]]
