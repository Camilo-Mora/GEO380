[["index.html", "Welcome", " By Camilo Mora, Ph.D. Welcome Welcome to GEOG 380 Statistical Methods. "],["introduction.html", "Introduction", " Introduction Most people gets turn off by statistics; but the reality is that Stats have not only allow human development, but we use stats, without even knowing, for almost every life choice that we make. In theory, Statistics requires some mathematical background, but in practice, stats are simply creating knowledge/information from observations/data. Definition Statistics is the approach by which knowledge is extracted from data. Let me provide an example that I find pretty cool. In the late 1879, just before going into holidays, Louis Pasteur told one of his assistants to inject chickens with a fresh culture of Cholera as part of an experiment to find a vaccine. By the way, thanks to this guy, Pasteur, you have milk for breakfast this morning; he was the one who discovered Pasteurization or the process to sterilize food without damaging its flavors, which allowed to store highly perishable foods for long time. Moving on with the story I presume Pasteurs assistant have a few other things to do that day, or may be he just wanted to go to his holidays; he not only forgot to inoculate the chickens, he left the bacterial culture outside. A month later, upon returning from holidays, the assistant injected the chickens with the old culture, but nothing happened to the chickens. For a moment, put yourself in the position of Pasteur and think what you would have done?. Sad to admit, but I probably would have been upset. I would have interpreted the action as careless, plus what a psychological torture for the poor chickens staring for a month to a lethal doses of bacteria. In addition, and more sadly, I probably would have interpreted the results of nothing happening to the chickens, as simply the bacterial culture got ruined from being outside for so long. I would have restarted the experiment with new chickens and new bacterial cultures. What would you have thought about the result of this failed experiment?. Take a moment to think. Figure 0.1: Pasteur Intriguingly, Pasteur told the assistant to inoculate the chickens again with fresh bacterial cultures and after a while they observed that the chickens got mildly sick but they did not succumb to the deadly bacteria. The chickens were immune to the bacteria. Essentially, Pasteour just made one of the major discoveries of humanity: a vaccine. This discovery came to be called Attenuated Vaccine, in which the virulence of bacteria is reduced by oxygen allowing the body to fight the bug. The method saved millions of chickens, and people as well. The method was used against many other deadly diseases, and put Pasteur in the history books. Nothing is known about what happened to the lab assistant. There are several things that made this case a very successful story. Obviously, Pasteur knew very well what he was doing to get even the smallest of hints from what most other people would have interpreted as failure, but he was also very methodic and knew his stats very well. He was able to gather knowledge from observations. He applied statistics very well. That is a game-changer example about the use of statistics. But as mentioned earlier, stats are used almost all the time, in almost every decision we made. Even simple things like the clothes that you are wearing right now was a choice you made most likely based on stats. If this morning was cold, you probably choose to wear a jacked. Feeling cold was you taking data that then you use analytically to make the decision to wear a jacket. The aim of this class is to give you basic i) methodological, ii) analytical, and iii) data visualization skills on stats. In a nutshell, I want you to learn how to identify a question, define a protocol to respond it, analyze the data collected, and display your results convincingly. "],["book-data-collection.html", "Book data collection", " Book data collection I have introduced numerous non-belonging letters throughout this book. I cannot tell you much about these letters other than they are where they do not belong. From now on those letters are called tokens. You need to collect those tokens as you move along and create your own database of tokens. Collect such data as meticulously as you can. I have a three motivations with this exercise. Firsts, make you realize the reality of science. At times, data could be clear, and you may find a letter that clearly does not belong where it is. Other times, however, you will wonder if the letter was an error or a token. In real life, nature will not clarify things for you. If the data you collect is not right, it will go to become part of the noise in your data. Second, at the end of this book, you will analyze your data collected in an statistical test. Finally, I want to ensure you read this book with much detail. Good luck with your token hunting. Here is goes your first token: a By the way, please do not stress out over the collection of these tokens, they will come as you move along. "],["what-is-r.html", "What is R?", " What is R? The second goal of this course is for you to become familiar with the use of R; we may use Microsoft Excel sporadically. R is like a Rolls-Royce, the king among softwares; a revolutionary free program that has come to become popular in many disciplines; and there are several reasons for this: Most importantly, it is free. By its nature of being open-source, thousands of people around the world have contributed their work in the form of packages, which has allowed R to increase in scope, being used for almost anything from doing statistical analysis, to figures, to movies, to animations, to videos, you name it. R Packages are like the phone apps that you download and install in your phone and that you use to do a diversity of things. Currently, there are more than 16,000 R-Packages, each providing different tools to do almost anything. There is also a very large group of users willing to help you if you face any struggle. Oh, and they will help you for free, as well. Stackoverflow is a web-page were you can find solutions to any problem you may encourter with R. Onlined platforms such as Stackoverflow have blogs of people helping each other with any issue about R. Every question is nicely cataloged, and at times there may even be alternative solutions to the same problem. The reality is that with so many millions of people using R, chances are that if you have a problem, someone else probably had the same problem before, and thus, their solution most likely is already ready for you in the web. Finally, R can handle the load. In R, you can run codes as small as those run in a calculator to as large as those ones run in super computers. No matter what professional path you take in life, you will have an use for R. "],["do-not-dispair-please.html", "Do not dispair, please", " Do not dispair, please R will be new to most of you. In addition, I have found over the years teaching this class that there is a large disparity among students in their prior exposure to mathematical concepts, let alone coding. So lets be realistic, learning R will be hard initially, but, you can do it. On my part, I have created this online resource of the course as detailed as possible so you can get there by simply following the steps. I also included a comments section at the bottom of each section, so you can communicate any problem to get feedback from everyone. You will not be penalized for asking a question on the forum, but you may get rewarded if you answer it correctly. You will not be penalized for answering wrong. So please lets use that discussion platform. But please be nice when asking and responding questions. If you get frustrated figuring out how to do something, you have two likely paths. One is getting upset, which uses about half of your brain power while the other half is trying to figure out the solution. The other path is to cool down, and have your entire brain power working on solving the problem. Which pathway makes the more sense? Finding solutions to your R problems is a key skill that could save you hours of labor, and we have a dedicated section on how to find answers in the sea of information that is the web. Using Nathaniel Phillips words from his R Bookdown YaRrr! The Pirates Guide to R, R is very much like a relationship. Like relationships, there are two major truths to R programming: There is nothing more frustrating than when your code does not work There is nothing more satisfying than when your code does work! Figure 0.2: R is like a love relationship, from Nathaniel Phillips There will be times, when You will run into error after error and pound your fists against the table screaming: WHY ISNT MY CODE WORKING?!?!? There must be something wrong with this stupid software!!! You will spend hours trying to find a bug in your code, only to find that - frustratingly enough, you had had an extra space or missed a comma somewhere. Taken from Nathaniel Phillips. If you go through a nightmare with R, you are not alone. We all went through that. I have to tell you, it will not go away. The more proficient you become, the more likely you will be to push the boundaries of what is possible and known, so certainly you will have many times in which your codes does not work. Think about Pasteur, at those moments. There will be a steep learning curve, but while you get there keep cool, do not desperate. If you face any roadblock; first, calm down. Then, have Google help you. Please think about this: in the next year or so, hopefully, you will be working at a company and/or doing grad school. At that moment, you really need to have mastered the capacity to solve problems by yourself. Gaining that independence will give you a huge advantage. So it is important for you to figure out what is the best method for you to find solutions to your R problems. Learning how to find solutions to problems in the web will be part of the learning of this class, and later on we will do exercises on this. If after you have done your best looking for a solution to a problem, but failed, then, reach out to me. I will be happy to help you. but I want to know that before hand you did your best to figure out the solution to your problem on your own. Learning R will become increasingly easier over time. As you become more proficient, you will start making more sophisticated things with R. When it comes to doing things, with R, the sky is the limit. R would become nicer to you over time, from Nathaniel Phillips Figure 0.3: R would become nicer to you over time, from Nathaniel Phillips I want to caution, however, that this course requires a good quality time commitment. Note my double intonation on good quality and on time commitment. Please study during times you cannot be distracted. I cannot emphasize enough that spending lots of time studying is not the same as studying well. As codes become longer and more complex, your mind would eventually need to focus to conceptualize the entire code. I can tell you with certainty that it will be much harder, border line impossible, to visualize large codes, if you are to be distracted. Learning R will not be done in one hour before the exam. So take as much time as you can to study. With practice will come perfection. Finally, it is worth mentioning that knowing how to use R is a skill that you want to learn. Of all the classes you will take at the University, this is one you want to put on your CV. Saying that you know R will be a very desirable skill for any job you end up doing. z "],["1-the-scientific-method.html", "1 The scientific method", " 1 The scientific method Figure 1.1: Knowledge and ignorance The scientific method is an approach by which robust knowledge is acquired. The approach can be applied to diverse situations, not only science. You probably learned about the scientific method in the third or fourth grade, but I like to refresh the general concepts as I commonly see students formulating experiments for which there is not clear question, doing analyzes whose results will not answer the question, etc. Knowing the exact protocol of how to go from question to answer will avoid you wasting time into what I call rabbit holes; basically, a lot of effort put into work that does not help the ultimate goal you proposedd. "],["the-problem.html", "The problem", " The problem The first step in the scientific method is when you observe a problem. However, defining a problem is not as trivial as you may think. There is some complexity to the process between the first time you observed a problem to the time you specifically define the problem you want to solve with the scientific method. Lets use a real case example. Lets assume that you just joined one of the many organizations in Hawaii interested in land restoration. Figure 1.2: Carbon Neutrality Project One of the first observations you will make is that the speed at which we cut down trees is much faster than the speed at which we plant trees. As a matter of fact, in Honolulu, we cut down over 15,000 trees a year, but only about 4,000 thousand trees are planted. Figure 1.3: Invasive Guava forrest in Hawaii A second observation you would likely make is that we are loosing land to many introduced species like guava, halekoa, albizia, several grazes, etc. Those species are very aggressive and are secluding/pushing to the brick native species. With those observations in mind, your first likely question is how do you make the approach of land restoration more efficient? Narrow the problem That simple question of how to make restoration more efficient can take you into tens of different pathways, which again, brings into attention the importance of keeping focus and methodic. Lets assume that you choose to focus on the production of seedlings, as many seedlings die after planting and they are not cheap. Figure 1.4: A few problems with seedling production Search prior knowledge The second step in the scientific method is to find out what has been already done about this problem. You need to collect information to better define what the problem is. Do not start working on the first idea that occurs to you. May be some else thought of your idea already, may be there are better ideas, may be other ideas give you an even better idea. Ultimately, you should not risk getting into a rabbit hole other people already checked for you, or worse off reinventing the wheal. Figure 1.5: Search information Searching for prior knowledge can be done in different ways. You can start with a general Google search. In the example about restoration, you can ask a simple question like why seedling mortality is so high. Ask questions like you will ask to a person. The more specific you get the better. Because most information in the web likely has been collected without using the scientific method, you should be cautious when using such information. The reason to consider the web as a general source of information is that it contains the cumulative experiences of millions of people, which bring me to another source of information, which is gaining from the expertise of other people. Find people working on your problem, and ask the same questions you asked in Google. Upon gathering prior knowledge in the form of experiences via webpages or interviews, you need to scale up the quality of information you search for using Google Scholar. Figure 1.6: Scholar Page Google Scholar is a search engine of scientific publications. There are many other databases you can use to look for papers like PubMed and Jstor, but Scholar alone will likely be enough. Several studies have shown that almost any search in Scholar returns the same papers as searches in more specific databases. Figure 1.7: PDFs in Scholar While those databases may give you the title and summary of the paper (i.e., so call citation), finding the full paper is another deal. When freely available in the web, Google Scholar will provide the PDF of papers in the hyperlink highlighted with the red square in the figure above. If you have an email student account, you can also try to get the PDF via your local library, which link is provided besides each citation in Google Scholar (green square in figure above). These links are only available when you do the search within your universitys network. You can also click on the All versions tab (Highlighted with the red arrow in the image above); commonly there are PDFs there as well. If you are not in a rush, you can click on the citation, which will take you to the web-page of the journal, get the email address of the author and send that person a message asking for the PDF. There is also a web page commonly shared among students called Sci-Hub, which contains PDFs of almost any paper. There is another one that provides books Z-Library. I am not familiar with the legality of these projects, but you should be aware they exists. You can read about Sci-Hub Here, and about Z-Library Here. Again, it is very important that you know your sources of information. And apply common sense, when you use such information. Please do the following exercise: Using the words How many species are there on Earth, do a search in Google and in Google Scholar. Check out the first ten citations on each search. What differences do you see? What are the social implications of these differences? Obviously, after you find the PDFs of papers, you have to read them. Some papers may provide new citations for you to look for. The step of information/literature search should be taken with much seriousness. You really want to be sure you know well what is already known out there; You need to become an expert on whatever is that you want to work on. Having a good understanding of the prior cumulative knowledge will allow you to quickly judge if something is worth working on or not. It will also give you ideas into how to do or not to do things. Pasteur famously said, chance only favours the prepared mind. Given what I told you about him, why do you think it is important, then, to know the literature well? Figure 1.8: An information search is demanding, if done well Back to defining the problem After the non-trivial exercise of looking for prior information, you should probable go back to re-defining what the problem is, being as specific as you can. If you were to actually replicate the example before about forest restoration, you will find that one, among many problems, with seedlings is that you probably want healthy seedlings grown at the nursery to maximize survival after planting. Having seedlings growing fast can also reduce the time they spend in the nursery reducing overhead costs. Tall and healthy seedlings can better cope with the stress after planting, and escape above and below ground competition with weeds. You would probably also found that water and food (in the form of nutrients) is critical to making seedlings grow faster and healthier. If you did a good information search, certainly, you also found out about the importance of soil microbes, specifically Mycorrhizae. Figure 1.9: Mycorrhizae in roots Mycorrhizae is a group of species of fungi that finds home on the roots of trees, creating mutualistic relationships with the tree. They break down organic matter and nitrogen compounds that can be adsorbed faster by the seedling. At this point, it will likely become clear that an obvious question to ask is if this Mycorrhizae has any effect on your seedlings. "],["the-hyphotesis.html", "The hyphotesis", " The hyphotesis Figure 1.10: The hyphotesis =o The third step in the scientific method for solving a problem is to define the hypothesis. Once you have a clear question, and you are relatively certain that no one has asked the question before, or answer it insufficiently, you should move to the next phase of the scientific method, which is to formulate your question more specifically. It could be as simple as: Does Mycorrhizae increase the body size of plants? For the purpose of the scientific method you need to be very specific in what you want to study. See for instance, how the question is explicit about fungi, affecting specifically the body size of plants. There are a few other nuances about formulating a question, that we will talk about when we deal with designing the experiment. You could also reformulate your very specific question in the form of an hypothesis, like: Mycorrhizae has a significant positive effect on the body size of plants You should now note in the hypothesis version of the question that you now include an expectation that the effect will be positive. The other addition in the hypothesis is the word significant, meaning that any effect you find is considerably larger than the effect you would find if you were not to use Mycorrhizae. Mathematically, the hypothesis has to be defined between two alternatives you can choose from: 1) the Null hypothesis and 2) the Alternative hypothesis. The null hypothesis H0 The null hypothesis is denoted by the symbol H0. It commonly represents a statement of no effect, no difference. In the restoration example, H0 is Mycorrhizae has no significant effect on the body size of plants The alternate hypothesis H1 The alternate hypothesis is denoted by the symbol H1. It commonly represents any hypothesis that differs from the null hypothesis. It should be defined in such a way to be accepted only when the null hypothesis is rejected. In the restoration example, H1 is Mycorrhizae has a positive significant effect on the body size of plants. "],["the-experiment.html", "The experiment", " The experiment The process of developing an experiment to answer a question or test a hypothesis is called Experimental Design, and it is full of nuances (commonly called demonic intrusions) that could render your entire work meaningless, so designing a good experiment is critical to obtain robust data. Before we design an experiment, it is important to know several terms. k Population, sample, subject Three important terms for you to know in experimental design are the subject/individual, the sample and the population. Population refers to every individual of interest. The sample refers to only some of the individuals of interest. When you quantify a given variable in an entire population that is called a census. Any statistic based on an entire population is called point stimate. Drag terms where you think they belong. You need to appreciate that statistics from the samples can vary from sample to sample, whereas statistics from the population are fixed for the given population. For instance, in the figure above, yellow birds represent 1 out of ten birds (i.e., 10% of the sample) in the sample outlined with a solid line. Yet in another sample (outlined with a dotted purple line), out of ten individuals there are two yellow birds (i.e., 20%). It is important that you appreciate the differences between the sample and the population, because at times those differences is exactly what you want to measure and test. Say you know that the average heart rate of people is 80 beats per minute but yours is 90 beats per minute. Is your rate significantly higher that it is expected? In this case, you can compare your hearths rate to that of the average population. Later on we will study how to test this mathematically. Variables In experimental design, the word Variables can mean several things, so it is important to be specific. The dependent variable The dependent variable, for instance, is a characteristic of the individual to be measured or observed. This is also called the response variable. In other words, the dependent variable is the attribute of the system you will expect to change, and which you plan to measure. In the case of the restoration hypothesis, it will be the body size of the plants. The independent variable The independent variable is what we will manipulate in the experiment. It can also be called a factor. In the case of the restoration hypothesis, it will be the presence or absence of Mycorrhizae. You can also use different concentrations of Mycorrhiza. Collectively, all the concentrations would be called the independent variable, and each concentration could be call a level. Each likely level in the independent variable can also be called a treatment. Quantitative variable Variables are also used when referring to the type of data you collect. If what you measure in the individuals is numerical (i.e., it can be measured with numbers, for instance, height or weight), it will be called a quantitative variable. Qualitative variable A qualitative variable describes an individual by placing the individual into a category or group (for instance, male or female). The control and treatment Figure 1.11: The population In experimental design, when testing the effects of a given independent variable, it is important to see what happens to individuals in the absence of the effect of such an independent variable. Individuals in the group that are not subject to the independent variable are called collectively the control group. The groups created by individuals that are exposed to the independent variable are collectively called a treatment group. Any difference in the response variable between the control and the treatment will be attributed to the independent variable. Say that individuals in the control group were on average 100g, while those on a given treatment were 120g. From this comparison, you can see that the treatment created a 20% increase in weight. Of course, not all individuals will have exactly the same weight (the so-call variability), and later on, we will see how to use that variability with statistical methods to determine if such a different is statistically significant or not. Replication In experimental design, replication refers to the number of independent individuals in a control or treatment upon which you test your hypothesis. Replication is a critical element in experimental design because it determines the robustness of your conclusion. Drag terms where you think they belong. h In almost any experiment there will be random variation in the response. Thus, an observed difference between the control and treatments could be mistakenly attributed to a cause-and-effect relationship when the source of the difference may just be random variation. In short, the difference may simply be due to the noise rather than the signal. This type of error is affected considerably by the amount of replication you have. Lets use an example. If nothing is wrong with a coin, you know that the probably of head or tail is the same at 50%. However, if you test this hypothesis with one replicate, lets say it landed in tails, then you will conclude that tails occur 100% of the times. If you try for a second time, and lands on tails again, your conclusion remains the same. If it is heads, now you have to change your conclusion as now either side has a 50% change. Lets say you try again, and regardless of where it lands, your probability for either side of the coin now changed to 75%. You can continue doing this over numerous trials, and eventually the probability will rest at about 50% for either side of the coin, again if nothing is wrong with the coin. From the example above, you can see how reduced replication can lead to variation in the conclusions. Lets review the effect of replication mathematically in an exercise, in which I measured the weight of all 1000 seedlings in my nursery. In this case, I have done a census as I measured every single individual. I plotted the number of seedlings at each weight, obtaining the following figure: The average weight of individuals in my population was 99.87 Now lets see what will happen when I take samples of different sizes from that population. In the figure above, each point is the average of a sample with the number of individuals shown in the x-axis. The red line is the true average of the population. From the figure above, you can see how samples with fewer number of replicated individuals have much larger variability in the mean weight than samples with more individuals. Basically, the variability in the samples reduces as I increase the size of replicated individuals. If my population was 1,000 individuals, the closer my sample is to that number, the more accurate the results from my sample. To better visualize the relationship between the number replicates and variability, lets do the same take 100 samples at each sample size: Again, you can see how samples with fewer replicated individuals are more variable than samples with many replicated individuals. There is not a set number of replicates you should use for an experiment as the optimum depends on the degree of variability in the variable you are measuring. Later on we will quantify the optimum sample size for a given population. Sample size and replication are confusing terms and could at times be interrelated. Say that you want to compare a group of individuals to population, in this case each individual will be replicated and all together they will be your sample size. However, if your experiment requires to take multiple samples, each sample with a given number of individuals, then each sample should be treated as independent measures, and in this case, each sample is a replicate. Remember, a replicate needs to be independent from each other. Pseudo-Replication One important assumption when you use statistics is that your replicates have to be independent (i.e., one replicate should not depend on another), and this can be tricky at times. Figure 1.12: Pseudo Replication Lets assume that you want to do an experiment with fishes to see if certain diet is better than other. You already identified that the optimum sample size was 1,000 fish. So, you put 1000 fishes in one aquarium and 1000 fishes in another aquarium, you feed the fish with the different diets, and later on, you measure how heavy the fishes are. The example above is a typical example of pseudo-replication. You may think you have 1,000 replicates, but in reality you only have one, which is each aquarium with fish. In this case, the response you find on each fish is not independent as all fishes in one aquarium are equally affected by whatever happens in the aquarium. Why do you only have one true replicate?. Why data from this experiment be reliable? There is famous paper by Hurlbert in Ecological Monographs in 1984 that outlines the many things that can go wrong due to pseudoreplication. Figure 1.13: likely confusions to emerge due to Pseudo Replication Randomness Another issue to be mindful of when doing experiments is to ensure that any other variable that you cannot test is applied randomly to the individuals in your experiment. The effect of variables that you cannot control or that you do not even know are called demonic intrusions. Basically, these variables can introduce evil effects to your response variable, but this can be controlled by ensuring you do everything randomly. Lets use an example to clarify this. Lets image that you are to run an experiment with two treatments, each with 100 replicated aquarium. Say that you put all the aquarium from one treatment on one side of the lab and the other 100 aquarium from the other treatment on the other side of the lab. What type of demonic intrusion could you think of in this case? I could image that the side of the lab facing the sun in the morning could be less hot than the aquariums on the other side. May be on one side they get more light than the other. May be there are windows, and thus different airflow, affecting oxygen in the water.this list can go on and on. All these variables can affect how your fish in the aquarium grow. As a consequence, in your results you may find a difference between fishes from different aquariums that are not due to the diet but to any of these demonic intrusions, resulting from you putting all aquariums of one treatment on one side of the lab, and the other treatment on the other side. Time to feed could be another non-control variable.. Could fish feed in the morning be less stress than fish feed in the afternoon?. The list of potential artifacts is long, but any likely effect can be avoided by ensuring that any aspect setting up the experiment is done randomly. For instance, locating the aquariums randomly in the laboratory. Design experiment With all considerations above, here are eight rules you need to enforce as you design your experiment (From PeerJ) 1.Begin by identifying a hypothesis for the topic you are interested in. Testable predictions generated will allow you to formulate a hypothesis. The hypothesis is an explanation of how you think a system works based on observation. The hypothesis will be either accepted or rejected based on the data collected. 2.Define parameters for the experiment by being clear and concise in your wording. By clearly defining terms, you can focus in on experimental methods and avoid ambiguity. This ensures that the results will be more accurate and there will be less flexibility in the experimental design, again increasing accuracy (Hurlbert, 1984). 3.Decide if you would like to perform a mensurative or manipulative experiment. A mensurative experiment involves making measurements at different times or in different areas. A manipulative experiment involves physically altering a treatment group, and thus always has two or more treatments (Hurlbert, 1984). 4.Choose an appropriate sample size that is fitting for the results you wish to obtain. Generally, a smaller sample size produces results that are inaccurate for generalization. A smaller sample size will also produce a smaller effect size measure, which is the efficacy of the treatment, and thus should be avoided (Ionnidis, 2005). 5.Introduce a control group. In biology, systems tend to exhibit temporal change, which could be an influencing third variable. In order to isolate any changes to the experimental treatment alone, a control is necessary. 6.Randomize assignment. By randomizing sample units to different treatment groups, experimenter bias is avoided. Randomization is a critical facet of the experimental design as it intersperses the samples being tested (Hurlbert, 1984). 7.Replicate! The number of replicates necessary will vary with design, however it ensures precision in experiments (Oksanen, 2001). 8.Ensure samples are dispersed in space or time to avoid pseudoreplication. This ensures replicates are statistically independent. Often, experimenters will make inferences based on the data collected and quantify samples from the same unit as independent, however in reality the samples come from the same unit, thus it is not a genuine replication (Oksanen, 2001). i "],["data-collection.html", "Data collection", " Data collection Once you have designed and started to run your experiment you need to start measuring your response variable (the so call data). It is important that you are systematic, methodic and organized as you collect data. Create a logbook to document any observation you may have, any change in the variables, responses on the individuals, etc. Document date and time for each observation. And do not lose the logbook. It is critical that you have several copies of your data on different places. Almost everyone has a terrible experience losing data. In my case, I lost my log-book in an airport. Hard drives fail, computers get stolen, etc. Account for any possibility; the last thing you want is to lose all the data from an experiment. "],["analyze-your-data.html", "Analyze your data", " Analyze your data Once you finish your experiment and you have all your data nicely organized. You will have to relay on mathematics to draw a conclusion. This step is called Hypothesis testing, and there are numerous methods that you can use. In the following chapters,we will introduce several statistical methods that you may use to test if your hypothesis is correct or not. "],["vizualize-the-data.html", "Vizualize the data", " Vizualize the data A great scientific story is that one that can be told with a simple figure. Latter on, we will introduce several types of figures that you can use to visualize your data. Figure 1.14: All impacts of climate change on diseases in one figure "],["write-report.html", "Write report", " Write report If you do not publish your results, it almost as if you did not do the research. Your paper needs to be clear, and detailed as to ensure any other person can replicate your study and find your same results. Due to issues related to poor experimental design, statistical test or lack of clarify on what was done, it has been found that a considerable amount of scientific studies are difficult to replicate. A 2016 poll of 1,500 scientists reported that 70% of them had failed to reproduce at least one other scientists experiment (50% had failed to reproduce one of their own experiments), leading to what is currently call a replicability crisis. "],["excercises.html", "Excercises", " Excercises Depending on your Internet connection, exercises may be slow to load. There are two exercises in this section. Exercise 1 The scientific methodd process. Drag boxes on the right into the boxes on the left in the order you think a question should be answered using the scientific method. Exercise 2 Key terms in statistics "],["homework.html", "Homework", " Homework Please do the following exercise: Based on the content of this chapter, think about a problem you have observed and create a flow chart outling the steps you will take to do a relibable experiment. Using a small figure or sketch to draw a likely experiment, and name its parts. Do this in a single sheet of paper. "],["2-installing-programs.html", "2 Installing programs", " 2 Installing programs We are going to take a break of stats for the following two chapters to install and learn some basics about R. "],["installing-r.html", "Installing R", " Installing R As mentioned earlier, R is free and it can be downloaded from Here. Once in that page click on CRAN (see red arrow in the figure below). Figure 2.1: RProject Webpage Next select the mirror. Mirrors are servers around the world that maintain copies of R and all the packages. You can click on the 0-Cloud, but any mirror will be fine (see image below) Figure 2.2: Installing R Next,select the system of your computer. I recommend you use Windows. There are only small differences between systems, and it should work the same in any system. But if you have Apple, you are on your own. There are several perks in Apple, not a big deal, but be aware that due to compatibility and settings, issues may emerge. Figure 2.3: Installing R Next, download the executable file of R, which is indicated in the read arrow in the figure below. Figure 2.4: Installing R Once downloaded, the file should be in the Downloads folder. Double click on it and accept all the default settings. The R icon should now appear at the bottom your tab or Windows menu (See arrows in image below). Figure 2.5: Installing R Click on the icon, and welcome to R. Crazy how that screen does not look like much but it is a portal to a world of opportunities. Figure 2.6: Installing R "],["tinn-r.html", "Tinn-R", " Tinn-R Once you run a line of code in R, it cannot be un-done. So very commonly, you have to re-run lines of code. For this reason, it is better if you write your code in a text editor program, and then running it R. You can save the text file, and next time, simply copy and paste. You can save your R code in any word processor. Notepad from Windows is just fine. You can also use Microsoft Word, but be mindful that that software commonly capitalizes the first letter in each new line. R is case sensitive, so Word is not worth the trouble. I personally use Tinn-R. It allows you to save your codes in a single file, gives you hints into your code, etc. Please install Tinn-R from here. R is case sensitive. A is different from a in R! This simple rule is at the core of most bugs in R! To create a file in Tinn-R, click on the Tinn-R icon (see arrow in image below). Once there, click on File, then on New. This will create a clean sheet for you to start writing your codes. You will use this type of sheets to save your codes, as you move along. Figure 2.7: Installing Tinn-R "],["r-studio.html", "R-Studio", " R-Studio There is also a very popular GUI (Graphical User Interface) called R-Studio, which allows you to run R while having the functionality of Tinn-R. Please know that R is not R studio. In R-Studio, R is almost hidden in the background, so it is important that you still know how to run R without R-Studio. With that said, R-studio facilitates considerably the learning of R, the downside is to think that R-studio is R. You can download R-studio from here; use the free version. Follow the instructions of the page to install the software. After installation R-Studio should appear as the icon in the image below. Figure 2.8: R-Studio The next video show you for how to use R-Studio. "],["installing-packages.html", "Installing packages", " Installing packages Part of the power of R is that tools developed by users around the world can be integrated into the Rs general functionality. Such tools are integrated into containers called packages, and deposited into the R repository for anyone to use. There are so many tools already developed that chances are there is a packages for anything you may need. The directory of available R-packages is available here. You can click on any available package to learn more about it, including instructions on how to use them. The primary way to install a package is using the command: # install.packages(&quot;raster&quot;) Between brackets should be the name of the package you want to install. You will be prompted to select a mirror. If this is the first time installing a package, you will also be asked to select a folder where to install the packages. I suggest you accept the default suggestions that are provided. Once installed in your local harddrive, you need to load the packages into your console every time you want to use it. To do so, you use the command: library(&quot;raster&quot;) ## Warning: package &#39;raster&#39; was built under R version 4.0.3 ## Loading required package: sp Between quotations is the name of the raster. Rlease remember that R is case sensitive. "],["asking-for-help.html", "Asking for help", " Asking for help You can access help on any specific R function using the question mark sign, followed by the name of the function. For instance, to ask for help on the mean, you type: # ?mean A window with available help on the function will pop-up (like the one in the image below). Figure 2.9: Asking for help in R One super nice thing about Rs help is that it provides examples on how the function should be used. See, for instance, the image above, an example (indicated with the red arrow) on how to use the mean function in R. You can simply copy those lines of commands and paste them in your console. This becomes very useful, as you can simply adjust that given code to your given data. "],["exercise.html", "Exercise", " Exercise Please install R and Tinn-R in your computer. Then install and load the package raster. Run the first example for the function raster. "],["3-basics-on-data-manipulation-in-r.html", "3 Basics on data manipulation in R", " 3 Basics on data manipulation in R Figure 3.1: R console You should think of R as an advance scientific calculator like the ones you used in high school. Basically, there is a set of basic functions already installed in R, which you can use with your data. You can also install packages, which gives you access to additional functions. Use Google to find what packages may have the functions you need. Say you want to do a genotypic analysis. Simply Google R packages to do genotypic analysis. If you want to do animations, search how to do an animation in R. "],["variables-1.html", "Variables", " Variables Variables in R are names used to storage objects that you can manipulate. For instance, let me create a variable A, whicha I want it to be the value 1. Basically, in the R console I type: A=1 then click Enter. That command, just created a variable A, which has a value of 1. To check, type A +Enter. A ## [1] 1 Lets now create a variable called B, which is equal to 2. B=2 I can now use those two variables to see what happens when I add them: A+B ## [1] 3 You cal also create a new variable based on other variables. For instance: C=A-B C ## [1] -1 You can check all the variables you have created using the command ls() ls() ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;i&quot; ## [5] &quot;MeanPopulation&quot; &quot;MeanSample&quot; &quot;Merge&quot; &quot;Population&quot; ## [9] &quot;Results&quot; &quot;Sample&quot; &quot;SampleSize&quot; &quot;trial&quot; As you can see, up to this moment I have created three variables, named A, B and C. With rare exceptions, each line of command in R is independent. You should try now. Open R in your computer or use the console below. The console below is exactly what you have in R in your computer. We will use this type of window in a few instances in this book, but I recommend you do your practices in your own computer. Create a variable called D=10 and other called z=5, and calculate thhe difference between D and z. "],["comments.html", "Comments", " Comments Comments are text you use to describe your code. They have to be preeceded with the number sign (#). Any line command after # will not be run by R As you start coding, it is always a good practice that you explain with your own words what each line of code is intended for. I cannot tell you the number of times, I have gone back to my old codes and ask myself, what was the purpose of this line. What was I thinking at that time? To ensure codes can be easy understood later on, we use comments. R comments are text added to your code, which R does not process. Any comment in R has to be preceded with the number sign (#). For instance: A=10 # Here I used R to show how to create a variable B=20 # In this case, i wanted my variable B to be 2 A+B ## [1] 30 w "],["operators.html", "Operators", " Operators Assignment operators As you noted in my previous page, I used the equal sign (=) to assign a value to a letter variable; that is called an assignment operator. Another more broadly recommended assignment operator is &gt;-, which is used like this: a&lt;-10 # in this case I used &lt;- as the assigment operators Note that in the code above I use &gt;- instead of = I can verify this, by typing a in the R console and then hitting enter. a ## [1] 10 Arithmetic operators R is preloaded with a lot of built-in mathematical operators. Here are some basic ones: Function Description + Addition - Subtraction * Multiplication / Division ^ Exponent log(x) Natural log log10(x) Common log abs(x) Absolute value sqrt(x) Square root ceiling(x) Round to largest integer floor(x) Round to smallest integer round(x, digits = 0) Round number to given number of digits rep(x, times = 1) Repeat a number a given number of times seq(from, to, by= ) Create a sequence of numbers sample(x,n,replace=T) Select n random numbers from vector x Lets say I have a variable a=145.677, but I one to stimate the largest integer of that number, then a=145.677 b=ceiling(a) b ## [1] 146 Give a try to some of these operators, yourself. Any time you need an operator, but you do not know what is it in R, simply ask your friend Google. Say you want to calculate the cosine of a number?. Simply search in Google How to calculate a cosine in R. Character operators At times, you do not want to handle numbers but words. R also has functions for that. Function Description substr(x, start=n1, stop=n2) Extract or replace substrings in a character vector grep(pattern, x , ignore.case=FALSE, fixed=FALSE) Search for pattern in x sub(pattern, replacement, x, ignore.case =FALSE, fixed=FALSE) Find pattern in x and replace with replacement text strsplit(x, split) Split the elements of character vector x at split paste(, sep=) Concatenate strings Lets try a few examples: a=&quot;camilo1989&quot; #here I have a character variable, but want to remove that number, so b=gsub(&quot;1989&quot;, &quot;&quot;, a) #here I replace the value indicated in between the first quotations, for what is in between the second quotations, in this case nothing. b ## [1] &quot;camilo&quot; Characters are always between quotations and this is done to differentiate from variable names. Say I have a variable I called data but I also have a word I want to use called data as well. In this case data by ilfset will be the database, but data will be recongnized as chracaters. Later on we will get to this. The function paste is another function that you will use a lot. a=&quot;camilo&quot; #here I have a character variable, that I want to concatenate with another one. b=&quot;1989&quot; #second variable. c=paste(a,b,sep=&quot;_&quot;) #here I merge a and b, and separate them with an underscore c ## [1] &quot;camilo_1989&quot; If you want nothing to separate the words, put nothing between the brackets in sep, like this c=paste(a,b,sep=&quot;&quot;) c ## [1] &quot;camilo1989&quot; "],["custom-made-functions.html", "Custom made functions", " Custom made functions As you become better at coding, you probably want to start creating your own functions. You can even create functions, that use other functions. Think of it like a lego game, in which you have thousands of functions (legos) that you can use to create whatever you want. Just as the Legos game, creating functions is very simple. The structure of an R function is as follow: You have the name you want for your function What arguments are needed for your function Between {} you place what you want to do with your arguments Outline what you want to output from your function Lets have an example. Say, I have two numbers (12.78454 and 1.34893439), which I want to first multiple, then divide the result by 2, then estimate the cosine, then multiple by pi, then estimate the ceiling; and I want to do this four times with different starting numbers. So lets start coding this, a=12.78454 #I place first number in a variable, so I do not have to retype all numbers b=1.34893439 #same for second number Multiple_AB=a*b #first command I have to do. Create new varaible multiplying a and b Divide= Multiple_AB/2 #second command is to divide the output by 2 Cosine= cos(Divide) #third command is to estimate the cosine in the results PI=Cosine*pi #four command is to multiply result by pi Cealing=ceiling(PI) #finally, estimate the largest integer Cealing #here is the result of that operation above ## [1] -2 If you see the code above, I wrote 8 lines of code to get the result for the first two set of numbers. If I want to do the same for say three other set of numbers, I will have to write 24 lines of code, not to mention the chance for error. Since the general operation is same for all cases, you can simply create a function, which will reduce the lenght of the code, while ensuring you use exactly the same procedure for all numbers. Lets do it, CamilosFunction &lt;- function(argument1, argument2) { #Argument 1 and 2, are the two numbers I have to run in my function Multiple_AB=argument1*argument2 #first command I have to do Divide= Multiple_AB/2 #second command is to divide the output by 2 Cosine= cos(Divide) #third command is to estimate the cosine in the results PI=Cosine*pi #four command is to multiply result by pi Cealing=ceiling(PI) #finally, estimate the largest integer return (Cealing) #here is the result of that operation above } If you see, all I did was to copy my entire process from the first set of values, and replace a and b, for the arguments in my function, add the {} and name my function. I just created a function, called CamilosFunction, that requires two values, to do the process indicated above, and return the result. Lets see, a=12.78454 b=1.34893439 CamilosFunction (a, b) #here is my function, which run many commands and give me a single result. ## [1] -2 You can now reuse your function with different numbers, each time requiring only one line of code. CamilosFunction (12.78454,10002.78454) ## [1] -2 CamilosFunction (120.78454,102.78454) ## [1] 3 CamilosFunction (912.74,10002.78454) ## [1] -1 Create your own function now. In which you take three numbers (4,2,3), calculate their average, then multiple the result by 4, then estimate the sin, and finally calculate the square. "],["data-structure-types.html", "Data structure types", " Data structure types R provides numerous ways to hold data. Lets check some of them. Vectors A vector is a collection of values. We call the individual values elements of the vector. We can make vectors with c( ). c means combine. Say you have three numbers 1,4,5 and you just do not want to type them every time. you can then put the numbers in a vector. c(1,4,5) #here I have a vector with three numbers ## [1] 1 4 5 You can also run operations in a vector. Lets use the example above, in which I want to multiply each value in that vector by 2. a=c(1,4,5) #here create a variable that contains my vector of three numbers a*2 #here I multiply my variable (vector with three numbers) by 2. ## [1] 2 8 10 Matrix You can merge multiple vectors to create a matrix. You can merge the vectors by columns using the function cbind or by rows using the function rbind. Lets do an example to clarify. Vector1=c(1,4,5) #here I have vector 1 with three numbers Vector2=c(56,42,93) #here I have vector 2 with three numbers ColumnMergedVector=cbind(Vector1,Vector2) #I use cbind to merge the vector by column ColumnMergedVector ## Vector1 Vector2 ## [1,] 1 56 ## [2,] 4 42 ## [3,] 5 93 See in the result above, how each vector became a column. May be you prefer to merge them by rows RowMergedVector=rbind(Vector1,Vector2) #I use rbind to merge the vector by rows RowMergedVector ## [,1] [,2] [,3] ## Vector1 1 4 5 ## Vector2 56 42 93 You can now see how the two vectors were merged by rows. DataFrames Dataframes are the most common type of data structure in R. It is has similar topology to a matrix, but it is more diverse in the types of data it can use. Dataframes, are like the Sheet version in Excel. To create a dataframe you use the data.frame function. Lets do an example. s DataFrame=data.frame(Vector1,Vector2) #I use data.frame to merge two vectros of data DataFrame ## Vector1 Vector2 ## 1 1 56 ## 2 4 42 ## 3 5 93 Dataframes also allow you to rename the column and row names, and there different ways to do that. Here is one, colnames(DataFrame)=c(&quot;Height&quot;,&quot;Width&quot;) #Here I rename the two columsn in the dataframe, with the names used in a vector. DataFrame ## Height Width ## 1 1 56 ## 2 4 42 ## 3 5 93 You are probably asking yourself what is the difference between a matrix and a data.frame?. Here are some differences: Matrix Dataframe Collection of data sets arranged in a two dimensional rectangular organisation Stores data tables that contains multiple data types in multiple column called fields. Its m*n array with similar data type. It is a list of vector of equal length. It is a generalized form of matrix. It has fixed number of rows and columns. It has variable number of rows and columns. The data stored in columns can be only of same data type. The data stored must be numeric, character or factor type. Data types are homogeneous. Data types are heterogeneous. "],["loading-your-own-data-into-r.html", "Loading your own data into R", " Loading your own data into R R also allows you to import your own data from a diversity of formats. Here we will use the most basic one, which is to import .csv files (the so-called comma delimited data). In this type of file, data are listed by rows, with the number between commas in a row indicating the columns. To load a .csv file in R, you use the command read.csv(). Between brackets you put the path to where the file is located. You write that path between quotation. It sounds complicated, but it is simple. Lets just try an example. Create a folder in your local desktop called Data. It is good practice to keep your data and codes in separated folders that you can then easily identify by the name alone. Download a csv file from here. Right click on the page and click on Save page as in the pop-up window. Name the file Countries_GDP.csv, and navigate to the Data folder you just created to save the file in there. Click Save. That file should now appear in your Data folder. Now that you have the csv file in your local hard-drive, you can load it into R. For this you need to do a couple things. First, get the path to where the file is at. For this, click on your windows explorer (that is the button that looks like a folder in your desktop, see image below) Figure 3.2: Windows explorer Navigate to the folder Data where you saved the file, and put the mouse cursor in the selectable window (See arrow in image below). That will reveal the path where the file is saved in your computer. Figure 3.3: Getting path to file Next, right click on that same window, and click on Copy. That will copy the path to your file (See image below). Figure 3.4: Getting path to file Open R, right click anywhere inside the main window of R and click on Paste. It should look something like this: Figure 3.5: Getting path to file Now you need to replace each back-slash for a forward-slash in your path. The position of those keys in my keyboard are shown in the image below. Figure 3.6: Slash keys in keyboard So the path to the file should be change from this: # &quot;D:/GEO380/Datasets/&quot; to this: # &quot;D:/GEO380/Datasets/&quot; Now, to the path of the folder in which you have the file, you need to add the file name, like this: # &quot;D:/GEO380/Datasets/Countries_GDP.csv&quot; I have to tell you that when I got started in R. A colleague sent me a file and told me to open his file in R. It sounded simple at the time, but it took me two days to figure out all these tiny things about setting the file correctly. Back in those days the internet was not so full of useful tutorials about how to do this. Ok, lets keep going. Now add to your path the R function that is used read csv files, which is read.csv(), like this: # read.csv(&quot;D:/GEO380/Datasets/Countries_GDP.csv&quot;) I suggest you attach this file to a variable name, so you can call it later. GDPData=read.csv(&quot;D:/GEO380/Datasets/Countries_GDP.csv&quot;) That is it, now you have loaded your data into R. Check it out by looking at the top part of the database head (GDPData) ## country continent year gdpPercap ## 1 Afghanistan Asia 1952 779.4453 ## 2 Afghanistan Asia 1957 820.8530 ## 3 Afghanistan Asia 1962 853.1007 ## 4 Afghanistan Asia 1967 836.1971 ## 5 Afghanistan Asia 1972 739.9811 ## 6 Afghanistan Asia 1977 786.1134 By the way, if you know that the csv file in the web will not be deleted or moved, you can read the data from the web directly using the URL to the file, instead of the path. Like this, GDPData=read.csv(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/Countries_GDP.csv&quot;) "],["calling-elements-in-a-data-frame.html", "Calling elements in a data frame", " Calling elements in a data frame Ok, now that you know how to store data in R. We need to learn how to see the data. To see a given data container, you can simply typeb its name and click enter. For instance, DataFrame ## Height Width ## 1 1 56 ## 2 4 42 ## 3 5 93 This is an ok way to see your data if the data container is not too big. However, if you have lots of data this command will fill up your screen. Instead, you can use the function head or tail, which will let you see the top five or the bottom five rows in a dataframe. Lets see. Let me first create a medium size dataframe: DataFrame&lt;- data.frame( x1 = c(rep(1,250)), # in Column 1 I repeat the number 1 for 25 times x2 = seq(1:250), #Column 2 I create a sequence of numbers from 1 to 25 x3 = sample(seq(1:1000),250)) # select 25 random numbers between 1 and 10000 In the code above, I just create a dummy dataframe with three columns and 250 rows using functions we already described in the chapter on arithmetic operators. If I try to see the full database, I just type the name of the dataframe and click enter. However, as you will notice calling the full dataframe will use a lot of your screen space because it will attempt to display all the data. DataFrame ## x1 x2 x3 ## 1 1 1 471 ## 2 1 2 9 ## 3 1 3 721 ## 4 1 4 775 ## 5 1 5 624 ## 6 1 6 173 ## 7 1 7 964 ## 8 1 8 567 ## 9 1 9 91 ## 10 1 10 958 ## 11 1 11 151 ## 12 1 12 401 ## 13 1 13 846 ## 14 1 14 577 ## 15 1 15 559 ## 16 1 16 686 ## 17 1 17 713 ## 18 1 18 857 ## 19 1 19 899 ## 20 1 20 161 ## 21 1 21 937 ## 22 1 22 100 ## 23 1 23 463 ## 24 1 24 460 ## 25 1 25 140 ## 26 1 26 254 ## 27 1 27 121 ## 28 1 28 562 ## 29 1 29 328 ## 30 1 30 98 ## 31 1 31 890 ## 32 1 32 520 ## 33 1 33 406 ## 34 1 34 40 ## 35 1 35 288 ## 36 1 36 845 ## 37 1 37 277 ## 38 1 38 639 ## 39 1 39 951 ## 40 1 40 849 ## 41 1 41 972 ## 42 1 42 670 ## 43 1 43 733 ## 44 1 44 608 ## 45 1 45 268 ## 46 1 46 20 ## 47 1 47 569 ## 48 1 48 39 ## 49 1 49 780 ## 50 1 50 10 ## 51 1 51 155 ## 52 1 52 668 ## 53 1 53 186 ## 54 1 54 649 ## 55 1 55 694 ## 56 1 56 863 ## 57 1 57 561 ## 58 1 58 712 ## 59 1 59 977 ## 60 1 60 801 ## 61 1 61 440 ## 62 1 62 212 ## 63 1 63 521 ## 64 1 64 866 ## 65 1 65 706 ## 66 1 66 35 ## 67 1 67 855 ## 68 1 68 127 ## 69 1 69 138 ## 70 1 70 282 ## 71 1 71 664 ## 72 1 72 224 ## 73 1 73 335 ## 74 1 74 702 ## 75 1 75 616 ## 76 1 76 215 ## 77 1 77 294 ## 78 1 78 542 ## 79 1 79 139 ## 80 1 80 75 ## 81 1 81 90 ## 82 1 82 146 ## 83 1 83 854 ## 84 1 84 357 ## 85 1 85 803 ## 86 1 86 338 ## 87 1 87 716 ## 88 1 88 527 ## 89 1 89 374 ## 90 1 90 197 ## 91 1 91 487 ## 92 1 92 817 ## 93 1 93 245 ## 94 1 94 216 ## 95 1 95 23 ## 96 1 96 202 ## 97 1 97 104 ## 98 1 98 997 ## 99 1 99 778 ## 100 1 100 840 ## 101 1 101 948 ## 102 1 102 797 ## 103 1 103 404 ## 104 1 104 906 ## 105 1 105 234 ## 106 1 106 684 ## 107 1 107 169 ## 108 1 108 305 ## 109 1 109 942 ## 110 1 110 468 ## 111 1 111 278 ## 112 1 112 556 ## 113 1 113 858 ## 114 1 114 388 ## 115 1 115 230 ## 116 1 116 475 ## 117 1 117 645 ## 118 1 118 462 ## 119 1 119 793 ## 120 1 120 666 ## 121 1 121 368 ## 122 1 122 808 ## 123 1 123 158 ## 124 1 124 578 ## 125 1 125 717 ## 126 1 126 419 ## 127 1 127 877 ## 128 1 128 144 ## 129 1 129 603 ## 130 1 130 924 ## 131 1 131 839 ## 132 1 132 480 ## 133 1 133 481 ## 134 1 134 37 ## 135 1 135 323 ## 136 1 136 378 ## 137 1 137 336 ## 138 1 138 53 ## 139 1 139 711 ## 140 1 140 741 ## 141 1 141 881 ## 142 1 142 715 ## 143 1 143 307 ## 144 1 144 74 ## 145 1 145 659 ## 146 1 146 589 ## 147 1 147 17 ## 148 1 148 464 ## 149 1 149 315 ## 150 1 150 22 ## 151 1 151 782 ## 152 1 152 923 ## 153 1 153 932 ## 154 1 154 705 ## 155 1 155 181 ## 156 1 156 660 ## 157 1 157 637 ## 158 1 158 390 ## 159 1 159 683 ## 160 1 160 65 ## 161 1 161 994 ## 162 1 162 501 ## 163 1 163 834 ## 164 1 164 48 ## 165 1 165 165 ## 166 1 166 675 ## 167 1 167 63 ## 168 1 168 747 ## 169 1 169 176 ## 170 1 170 289 ## 171 1 171 842 ## 172 1 172 314 ## 173 1 173 441 ## 174 1 174 81 ## 175 1 175 628 ## 176 1 176 507 ## 177 1 177 18 ## 178 1 178 538 ## 179 1 179 182 ## 180 1 180 688 ## 181 1 181 99 ## 182 1 182 602 ## 183 1 183 185 ## 184 1 184 383 ## 185 1 185 85 ## 186 1 186 96 ## 187 1 187 429 ## 188 1 188 902 ## 189 1 189 891 ## 190 1 190 332 ## 191 1 191 651 ## 192 1 192 453 ## 193 1 193 993 ## 194 1 194 284 ## 195 1 195 239 ## 196 1 196 823 ## 197 1 197 615 ## 198 1 198 52 ## 199 1 199 533 ## 200 1 200 941 ## 201 1 201 981 ## 202 1 202 408 ## 203 1 203 772 ## 204 1 204 310 ## 205 1 205 693 ## 206 1 206 975 ## 207 1 207 150 ## 208 1 208 344 ## 209 1 209 692 ## 210 1 210 725 ## 211 1 211 113 ## 212 1 212 685 ## 213 1 213 534 ## 214 1 214 565 ## 215 1 215 247 ## 216 1 216 68 ## 217 1 217 46 ## 218 1 218 485 ## 219 1 219 638 ## 220 1 220 405 ## 221 1 221 672 ## 222 1 222 726 ## 223 1 223 295 ## 224 1 224 758 ## 225 1 225 503 ## 226 1 226 373 ## 227 1 227 550 ## 228 1 228 279 ## 229 1 229 195 ## 230 1 230 800 ## 231 1 231 469 ## 232 1 232 732 ## 233 1 233 541 ## 234 1 234 399 ## 235 1 235 991 ## 236 1 236 496 ## 237 1 237 880 ## 238 1 238 345 ## 239 1 239 313 ## 240 1 240 42 ## 241 1 241 299 ## 242 1 242 413 ## 243 1 243 943 ## 244 1 244 653 ## 245 1 245 916 ## 246 1 246 163 ## 247 1 247 612 ## 248 1 248 54 ## 249 1 249 934 ## 250 1 250 671 Head Alternatively, you can just check the top rows using the head function. Like this: head(DataFrame) ## x1 x2 x3 ## 1 1 1 471 ## 2 1 2 9 ## 3 1 3 721 ## 4 1 4 775 ## 5 1 5 624 ## 6 1 6 173 Tail Or the bottom rows using the tail function. Like this: tail(DataFrame) ## x1 x2 x3 ## 245 1 245 916 ## 246 1 246 163 ## 247 1 247 612 ## 248 1 248 54 ## 249 1 249 934 ## 250 1 250 671 Index You can also check specific elements of the dataframe using the index function, which in R is indicated with the square brackets [row,column]. The number to the left of the comma will be the row number, and the number to the right the column number. If you do not add a number, it will display all columns or all rows. For instance, check what number is in column 3 in the 2th row? DataFrame[2,3] ## [1] 9 Calling columns by name To call a column by name in a dataframe, you use the dollar sign $ to merge the name of the dataframe with the name of the column, like this, head(DataFrame$x3) #here I only display the top values of the column Country_Name ## [1] 471 9 721 775 624 173 "],["filter-data.html", "Filter data", " Filter data A recurring task in data analytics is to filter data, this means to select specific subsets/chucks of your data. Filters can be used to create new variables, to apply new functions, to see specific data, etc. There are numerous ways to filter data in R, but here we wall use the function filter in the package dplyr. # install.packages(&quot;dplyr&quot;) #First, I install the package dplyr and tidyverse, since I have not installed it previously # install.packages(&quot;tidyverse&quot;) library(tidyverse) #next I load the packages into the current section of R. library(dplyr) Now, lets check the first few rows in the GDP database we loaded earlier, tail(GDPData) #lets look at the column head of the GDP data we loaded earlier. ## country continent year gdpPercap ## 1699 Zimbabwe Africa 1982 788.8550 ## 1700 Zimbabwe Africa 1987 706.1573 ## 1701 Zimbabwe Africa 1992 693.4208 ## 1702 Zimbabwe Africa 1997 792.4500 ## 1703 Zimbabwe Africa 2002 672.0386 ## 1704 Zimbabwe Africa 2007 469.7093 From the subset of the data above, you can see that the GDPData database has four columns, and from the year column, you can tell that the data of GDP (gdpPercap) is repeated for each year. Hmm, but what if I just want to see the data for countries in Asia?. For that we use the function filter. The syntax of the filter function reads like this: Figure 3.7: Filter function Pipes, %&gt;%, are a powerful tool for doing sequential operations. Commonly it requires tthe tidyverse package. In the next section you will better see the use of pipes. Now that you know the syntax of the filter function from the dplr package, lets test it. GDPData %&gt;% filter(continent==&quot;Asia&quot;) ## country continent year gdpPercap ## 1 Afghanistan Asia 1952 779.4453 ## 2 Afghanistan Asia 1957 820.8530 ## 3 Afghanistan Asia 1962 853.1007 ## 4 Afghanistan Asia 1967 836.1971 ## 5 Afghanistan Asia 1972 739.9811 ## 6 Afghanistan Asia 1977 786.1134 ## 7 Afghanistan Asia 1982 978.0114 ## 8 Afghanistan Asia 1987 852.3959 ## 9 Afghanistan Asia 1992 649.3414 ## 10 Afghanistan Asia 1997 635.3414 ## 11 Afghanistan Asia 2002 726.7341 ## 12 Afghanistan Asia 2007 974.5803 ## 13 Bahrain Asia 1952 9867.0848 ## 14 Bahrain Asia 1957 11635.7995 ## 15 Bahrain Asia 1962 12753.2751 ## 16 Bahrain Asia 1967 14804.6727 ## 17 Bahrain Asia 1972 18268.6584 ## 18 Bahrain Asia 1977 19340.1020 ## 19 Bahrain Asia 1982 19211.1473 ## 20 Bahrain Asia 1987 18524.0241 ## 21 Bahrain Asia 1992 19035.5792 ## 22 Bahrain Asia 1997 20292.0168 ## 23 Bahrain Asia 2002 23403.5593 ## 24 Bahrain Asia 2007 29796.0483 ## 25 Bangladesh Asia 1952 684.2442 ## 26 Bangladesh Asia 1957 661.6375 ## 27 Bangladesh Asia 1962 686.3416 ## 28 Bangladesh Asia 1967 721.1861 ## 29 Bangladesh Asia 1972 630.2336 ## 30 Bangladesh Asia 1977 659.8772 ## 31 Bangladesh Asia 1982 676.9819 ## 32 Bangladesh Asia 1987 751.9794 ## 33 Bangladesh Asia 1992 837.8102 ## 34 Bangladesh Asia 1997 972.7700 ## 35 Bangladesh Asia 2002 1136.3904 ## 36 Bangladesh Asia 2007 1391.2538 ## 37 Cambodia Asia 1952 368.4693 ## 38 Cambodia Asia 1957 434.0383 ## 39 Cambodia Asia 1962 496.9136 ## 40 Cambodia Asia 1967 523.4323 ## 41 Cambodia Asia 1972 421.6240 ## 42 Cambodia Asia 1977 524.9722 ## 43 Cambodia Asia 1982 624.4755 ## 44 Cambodia Asia 1987 683.8956 ## 45 Cambodia Asia 1992 682.3032 ## 46 Cambodia Asia 1997 734.2852 ## 47 Cambodia Asia 2002 896.2260 ## 48 Cambodia Asia 2007 1713.7787 ## 49 China Asia 1952 400.4486 ## 50 China Asia 1957 575.9870 ## 51 China Asia 1962 487.6740 ## 52 China Asia 1967 612.7057 ## 53 China Asia 1972 676.9001 ## 54 China Asia 1977 741.2375 ## 55 China Asia 1982 962.4214 ## 56 China Asia 1987 1378.9040 ## 57 China Asia 1992 1655.7842 ## 58 China Asia 1997 2289.2341 ## 59 China Asia 2002 3119.2809 ## 60 China Asia 2007 4959.1149 ## 61 Hong Kong, China Asia 1952 3054.4212 ## 62 Hong Kong, China Asia 1957 3629.0765 ## 63 Hong Kong, China Asia 1962 4692.6483 ## 64 Hong Kong, China Asia 1967 6197.9628 ## 65 Hong Kong, China Asia 1972 8315.9281 ## 66 Hong Kong, China Asia 1977 11186.1413 ## 67 Hong Kong, China Asia 1982 14560.5305 ## 68 Hong Kong, China Asia 1987 20038.4727 ## 69 Hong Kong, China Asia 1992 24757.6030 ## 70 Hong Kong, China Asia 1997 28377.6322 ## 71 Hong Kong, China Asia 2002 30209.0152 ## 72 Hong Kong, China Asia 2007 39724.9787 ## 73 India Asia 1952 546.5657 ## 74 India Asia 1957 590.0620 ## 75 India Asia 1962 658.3472 ## 76 India Asia 1967 700.7706 ## 77 India Asia 1972 724.0325 ## 78 India Asia 1977 813.3373 ## 79 India Asia 1982 855.7235 ## 80 India Asia 1987 976.5127 ## 81 India Asia 1992 1164.4068 ## 82 India Asia 1997 1458.8174 ## 83 India Asia 2002 1746.7695 ## 84 India Asia 2007 2452.2104 ## 85 Indonesia Asia 1952 749.6817 ## 86 Indonesia Asia 1957 858.9003 ## 87 Indonesia Asia 1962 849.2898 ## 88 Indonesia Asia 1967 762.4318 ## 89 Indonesia Asia 1972 1111.1079 ## 90 Indonesia Asia 1977 1382.7021 ## 91 Indonesia Asia 1982 1516.8730 ## 92 Indonesia Asia 1987 1748.3570 ## 93 Indonesia Asia 1992 2383.1409 ## 94 Indonesia Asia 1997 3119.3356 ## 95 Indonesia Asia 2002 2873.9129 ## 96 Indonesia Asia 2007 3540.6516 ## 97 Iran Asia 1952 3035.3260 ## 98 Iran Asia 1957 3290.2576 ## 99 Iran Asia 1962 4187.3298 ## 100 Iran Asia 1967 5906.7318 ## 101 Iran Asia 1972 9613.8186 ## 102 Iran Asia 1977 11888.5951 ## 103 Iran Asia 1982 7608.3346 ## 104 Iran Asia 1987 6642.8814 ## 105 Iran Asia 1992 7235.6532 ## 106 Iran Asia 1997 8263.5903 ## 107 Iran Asia 2002 9240.7620 ## 108 Iran Asia 2007 11605.7145 ## 109 Iraq Asia 1952 4129.7661 ## 110 Iraq Asia 1957 6229.3336 ## 111 Iraq Asia 1962 8341.7378 ## 112 Iraq Asia 1967 8931.4598 ## 113 Iraq Asia 1972 9576.0376 ## 114 Iraq Asia 1977 14688.2351 ## 115 Iraq Asia 1982 14517.9071 ## 116 Iraq Asia 1987 11643.5727 ## 117 Iraq Asia 1992 3745.6407 ## 118 Iraq Asia 1997 3076.2398 ## 119 Iraq Asia 2002 4390.7173 ## 120 Iraq Asia 2007 4471.0619 ## 121 Israel Asia 1952 4086.5221 ## 122 Israel Asia 1957 5385.2785 ## 123 Israel Asia 1962 7105.6307 ## 124 Israel Asia 1967 8393.7414 ## 125 Israel Asia 1972 12786.9322 ## 126 Israel Asia 1977 13306.6192 ## 127 Israel Asia 1982 15367.0292 ## 128 Israel Asia 1987 17122.4799 ## 129 Israel Asia 1992 18051.5225 ## 130 Israel Asia 1997 20896.6092 ## 131 Israel Asia 2002 21905.5951 ## 132 Israel Asia 2007 25523.2771 ## 133 Japan Asia 1952 3216.9563 ## 134 Japan Asia 1957 4317.6944 ## 135 Japan Asia 1962 6576.6495 ## 136 Japan Asia 1967 9847.7886 ## 137 Japan Asia 1972 14778.7864 ## 138 Japan Asia 1977 16610.3770 ## 139 Japan Asia 1982 19384.1057 ## 140 Japan Asia 1987 22375.9419 ## 141 Japan Asia 1992 26824.8951 ## 142 Japan Asia 1997 28816.5850 ## 143 Japan Asia 2002 28604.5919 ## 144 Japan Asia 2007 31656.0681 ## 145 Jordan Asia 1952 1546.9078 ## 146 Jordan Asia 1957 1886.0806 ## 147 Jordan Asia 1962 2348.0092 ## 148 Jordan Asia 1967 2741.7963 ## 149 Jordan Asia 1972 2110.8563 ## 150 Jordan Asia 1977 2852.3516 ## 151 Jordan Asia 1982 4161.4160 ## 152 Jordan Asia 1987 4448.6799 ## 153 Jordan Asia 1992 3431.5936 ## 154 Jordan Asia 1997 3645.3796 ## 155 Jordan Asia 2002 3844.9172 ## 156 Jordan Asia 2007 4519.4612 ## 157 Korea, Dem. Rep. Asia 1952 1088.2778 ## 158 Korea, Dem. Rep. Asia 1957 1571.1347 ## 159 Korea, Dem. Rep. Asia 1962 1621.6936 ## 160 Korea, Dem. Rep. Asia 1967 2143.5406 ## 161 Korea, Dem. Rep. Asia 1972 3701.6215 ## 162 Korea, Dem. Rep. Asia 1977 4106.3012 ## 163 Korea, Dem. Rep. Asia 1982 4106.5253 ## 164 Korea, Dem. Rep. Asia 1987 4106.4923 ## 165 Korea, Dem. Rep. Asia 1992 3726.0635 ## 166 Korea, Dem. Rep. Asia 1997 1690.7568 ## 167 Korea, Dem. Rep. Asia 2002 1646.7582 ## 168 Korea, Dem. Rep. Asia 2007 1593.0655 ## 169 Korea, Rep. Asia 1952 1030.5922 ## 170 Korea, Rep. Asia 1957 1487.5935 ## 171 Korea, Rep. Asia 1962 1536.3444 ## 172 Korea, Rep. Asia 1967 2029.2281 ## 173 Korea, Rep. Asia 1972 3030.8767 ## 174 Korea, Rep. Asia 1977 4657.2210 ## 175 Korea, Rep. Asia 1982 5622.9425 ## 176 Korea, Rep. Asia 1987 8533.0888 ## 177 Korea, Rep. Asia 1992 12104.2787 ## 178 Korea, Rep. Asia 1997 15993.5280 ## 179 Korea, Rep. Asia 2002 19233.9882 ## 180 Korea, Rep. Asia 2007 23348.1397 ## 181 Kuwait Asia 1952 108382.3529 ## 182 Kuwait Asia 1957 113523.1329 ## 183 Kuwait Asia 1962 95458.1118 ## 184 Kuwait Asia 1967 80894.8833 ## 185 Kuwait Asia 1972 109347.8670 ## 186 Kuwait Asia 1977 59265.4771 ## 187 Kuwait Asia 1982 31354.0357 ## 188 Kuwait Asia 1987 28118.4300 ## 189 Kuwait Asia 1992 34932.9196 ## 190 Kuwait Asia 1997 40300.6200 ## 191 Kuwait Asia 2002 35110.1057 ## 192 Kuwait Asia 2007 47306.9898 ## 193 Lebanon Asia 1952 4834.8041 ## 194 Lebanon Asia 1957 6089.7869 ## 195 Lebanon Asia 1962 5714.5606 ## 196 Lebanon Asia 1967 6006.9830 ## 197 Lebanon Asia 1972 7486.3843 ## 198 Lebanon Asia 1977 8659.6968 ## 199 Lebanon Asia 1982 7640.5195 ## 200 Lebanon Asia 1987 5377.0913 ## 201 Lebanon Asia 1992 6890.8069 ## 202 Lebanon Asia 1997 8754.9639 ## 203 Lebanon Asia 2002 9313.9388 ## 204 Lebanon Asia 2007 10461.0587 ## 205 Malaysia Asia 1952 1831.1329 ## 206 Malaysia Asia 1957 1810.0670 ## 207 Malaysia Asia 1962 2036.8849 ## 208 Malaysia Asia 1967 2277.7424 ## 209 Malaysia Asia 1972 2849.0948 ## 210 Malaysia Asia 1977 3827.9216 ## 211 Malaysia Asia 1982 4920.3560 ## 212 Malaysia Asia 1987 5249.8027 ## 213 Malaysia Asia 1992 7277.9128 ## 214 Malaysia Asia 1997 10132.9096 ## 215 Malaysia Asia 2002 10206.9779 ## 216 Malaysia Asia 2007 12451.6558 ## 217 Mongolia Asia 1952 786.5669 ## 218 Mongolia Asia 1957 912.6626 ## 219 Mongolia Asia 1962 1056.3540 ## 220 Mongolia Asia 1967 1226.0411 ## 221 Mongolia Asia 1972 1421.7420 ## 222 Mongolia Asia 1977 1647.5117 ## 223 Mongolia Asia 1982 2000.6031 ## 224 Mongolia Asia 1987 2338.0083 ## 225 Mongolia Asia 1992 1785.4020 ## 226 Mongolia Asia 1997 1902.2521 ## 227 Mongolia Asia 2002 2140.7393 ## 228 Mongolia Asia 2007 3095.7723 ## 229 Myanmar Asia 1952 331.0000 ## 230 Myanmar Asia 1957 350.0000 ## 231 Myanmar Asia 1962 388.0000 ## 232 Myanmar Asia 1967 349.0000 ## 233 Myanmar Asia 1972 357.0000 ## 234 Myanmar Asia 1977 371.0000 ## 235 Myanmar Asia 1982 424.0000 ## 236 Myanmar Asia 1987 385.0000 ## 237 Myanmar Asia 1992 347.0000 ## 238 Myanmar Asia 1997 415.0000 ## 239 Myanmar Asia 2002 611.0000 ## 240 Myanmar Asia 2007 944.0000 ## 241 Nepal Asia 1952 545.8657 ## 242 Nepal Asia 1957 597.9364 ## 243 Nepal Asia 1962 652.3969 ## 244 Nepal Asia 1967 676.4422 ## 245 Nepal Asia 1972 674.7881 ## 246 Nepal Asia 1977 694.1124 ## 247 Nepal Asia 1982 718.3731 ## 248 Nepal Asia 1987 775.6325 ## 249 Nepal Asia 1992 897.7404 ## 250 Nepal Asia 1997 1010.8921 ## 251 Nepal Asia 2002 1057.2063 ## 252 Nepal Asia 2007 1091.3598 ## 253 Oman Asia 1952 1828.2303 ## 254 Oman Asia 1957 2242.7466 ## 255 Oman Asia 1962 2924.6381 ## 256 Oman Asia 1967 4720.9427 ## 257 Oman Asia 1972 10618.0385 ## 258 Oman Asia 1977 11848.3439 ## 259 Oman Asia 1982 12954.7910 ## 260 Oman Asia 1987 18115.2231 ## 261 Oman Asia 1992 18616.7069 ## 262 Oman Asia 1997 19702.0558 ## 263 Oman Asia 2002 19774.8369 ## 264 Oman Asia 2007 22316.1929 ## 265 Pakistan Asia 1952 684.5971 ## 266 Pakistan Asia 1957 747.0835 ## 267 Pakistan Asia 1962 803.3427 ## 268 Pakistan Asia 1967 942.4083 ## 269 Pakistan Asia 1972 1049.9390 ## 270 Pakistan Asia 1977 1175.9212 ## 271 Pakistan Asia 1982 1443.4298 ## 272 Pakistan Asia 1987 1704.6866 ## 273 Pakistan Asia 1992 1971.8295 ## 274 Pakistan Asia 1997 2049.3505 ## 275 Pakistan Asia 2002 2092.7124 ## 276 Pakistan Asia 2007 2605.9476 ## 277 Philippines Asia 1952 1272.8810 ## 278 Philippines Asia 1957 1547.9448 ## 279 Philippines Asia 1962 1649.5522 ## 280 Philippines Asia 1967 1814.1274 ## 281 Philippines Asia 1972 1989.3741 ## 282 Philippines Asia 1977 2373.2043 ## 283 Philippines Asia 1982 2603.2738 ## 284 Philippines Asia 1987 2189.6350 ## 285 Philippines Asia 1992 2279.3240 ## 286 Philippines Asia 1997 2536.5349 ## 287 Philippines Asia 2002 2650.9211 ## 288 Philippines Asia 2007 3190.4810 ## 289 Saudi Arabia Asia 1952 6459.5548 ## 290 Saudi Arabia Asia 1957 8157.5912 ## 291 Saudi Arabia Asia 1962 11626.4197 ## 292 Saudi Arabia Asia 1967 16903.0489 ## 293 Saudi Arabia Asia 1972 24837.4287 ## 294 Saudi Arabia Asia 1977 34167.7626 ## 295 Saudi Arabia Asia 1982 33693.1753 ## 296 Saudi Arabia Asia 1987 21198.2614 ## 297 Saudi Arabia Asia 1992 24841.6178 ## 298 Saudi Arabia Asia 1997 20586.6902 ## 299 Saudi Arabia Asia 2002 19014.5412 ## 300 Saudi Arabia Asia 2007 21654.8319 ## 301 Singapore Asia 1952 2315.1382 ## 302 Singapore Asia 1957 2843.1044 ## 303 Singapore Asia 1962 3674.7356 ## 304 Singapore Asia 1967 4977.4185 ## 305 Singapore Asia 1972 8597.7562 ## 306 Singapore Asia 1977 11210.0895 ## 307 Singapore Asia 1982 15169.1611 ## 308 Singapore Asia 1987 18861.5308 ## 309 Singapore Asia 1992 24769.8912 ## 310 Singapore Asia 1997 33519.4766 ## 311 Singapore Asia 2002 36023.1054 ## 312 Singapore Asia 2007 47143.1796 ## 313 Sri Lanka Asia 1952 1083.5320 ## 314 Sri Lanka Asia 1957 1072.5466 ## 315 Sri Lanka Asia 1962 1074.4720 ## 316 Sri Lanka Asia 1967 1135.5143 ## 317 Sri Lanka Asia 1972 1213.3955 ## 318 Sri Lanka Asia 1977 1348.7757 ## 319 Sri Lanka Asia 1982 1648.0798 ## 320 Sri Lanka Asia 1987 1876.7668 ## 321 Sri Lanka Asia 1992 2153.7392 ## 322 Sri Lanka Asia 1997 2664.4773 ## 323 Sri Lanka Asia 2002 3015.3788 ## 324 Sri Lanka Asia 2007 3970.0954 ## 325 Syria Asia 1952 1643.4854 ## 326 Syria Asia 1957 2117.2349 ## 327 Syria Asia 1962 2193.0371 ## 328 Syria Asia 1967 1881.9236 ## 329 Syria Asia 1972 2571.4230 ## 330 Syria Asia 1977 3195.4846 ## 331 Syria Asia 1982 3761.8377 ## 332 Syria Asia 1987 3116.7743 ## 333 Syria Asia 1992 3340.5428 ## 334 Syria Asia 1997 4014.2390 ## 335 Syria Asia 2002 4090.9253 ## 336 Syria Asia 2007 4184.5481 ## 337 Taiwan Asia 1952 1206.9479 ## 338 Taiwan Asia 1957 1507.8613 ## 339 Taiwan Asia 1962 1822.8790 ## 340 Taiwan Asia 1967 2643.8587 ## 341 Taiwan Asia 1972 4062.5239 ## 342 Taiwan Asia 1977 5596.5198 ## 343 Taiwan Asia 1982 7426.3548 ## 344 Taiwan Asia 1987 11054.5618 ## 345 Taiwan Asia 1992 15215.6579 ## 346 Taiwan Asia 1997 20206.8210 ## 347 Taiwan Asia 2002 23235.4233 ## 348 Taiwan Asia 2007 28718.2768 ## 349 Thailand Asia 1952 757.7974 ## 350 Thailand Asia 1957 793.5774 ## 351 Thailand Asia 1962 1002.1992 ## 352 Thailand Asia 1967 1295.4607 ## 353 Thailand Asia 1972 1524.3589 ## 354 Thailand Asia 1977 1961.2246 ## 355 Thailand Asia 1982 2393.2198 ## 356 Thailand Asia 1987 2982.6538 ## 357 Thailand Asia 1992 4616.8965 ## 358 Thailand Asia 1997 5852.6255 ## 359 Thailand Asia 2002 5913.1875 ## 360 Thailand Asia 2007 7458.3963 ## 361 Vietnam Asia 1952 605.0665 ## 362 Vietnam Asia 1957 676.2854 ## 363 Vietnam Asia 1962 772.0492 ## 364 Vietnam Asia 1967 637.1233 ## 365 Vietnam Asia 1972 699.5016 ## 366 Vietnam Asia 1977 713.5371 ## 367 Vietnam Asia 1982 707.2358 ## 368 Vietnam Asia 1987 820.7994 ## 369 Vietnam Asia 1992 989.0231 ## 370 Vietnam Asia 1997 1385.8968 ## 371 Vietnam Asia 2002 1764.4567 ## 372 Vietnam Asia 2007 2441.5764 ## 373 West Bank and Gaza Asia 1952 1515.5923 ## 374 West Bank and Gaza Asia 1957 1827.0677 ## 375 West Bank and Gaza Asia 1962 2198.9563 ## 376 West Bank and Gaza Asia 1967 2649.7150 ## 377 West Bank and Gaza Asia 1972 3133.4093 ## 378 West Bank and Gaza Asia 1977 3682.8315 ## 379 West Bank and Gaza Asia 1982 4336.0321 ## 380 West Bank and Gaza Asia 1987 5107.1974 ## 381 West Bank and Gaza Asia 1992 6017.6548 ## 382 West Bank and Gaza Asia 1997 7110.6676 ## 383 West Bank and Gaza Asia 2002 4515.4876 ## 384 West Bank and Gaza Asia 2007 3025.3498 ## 385 Yemen, Rep. Asia 1952 781.7176 ## 386 Yemen, Rep. Asia 1957 804.8305 ## 387 Yemen, Rep. Asia 1962 825.6232 ## 388 Yemen, Rep. Asia 1967 862.4421 ## 389 Yemen, Rep. Asia 1972 1265.0470 ## 390 Yemen, Rep. Asia 1977 1829.7652 ## 391 Yemen, Rep. Asia 1982 1977.5570 ## 392 Yemen, Rep. Asia 1987 1971.7415 ## 393 Yemen, Rep. Asia 1992 1879.4967 ## 394 Yemen, Rep. Asia 1997 2117.4845 ## 395 Yemen, Rep. Asia 2002 2234.8208 ## 396 Yemen, Rep. Asia 2007 2280.7699 From the results above, you can see how only data from Asia were selected. "],["pivot-table.html", "Pivot table", " Pivot table Pivottables are a functionality originally developed in Excel, which allows you to summarize data by attributes. Lets do an example to see how this work. tail(GDPData) #lets look at the column head of the GDP data we loaded earlier. ## country continent year gdpPercap ## 1699 Zimbabwe Africa 1982 788.8550 ## 1700 Zimbabwe Africa 1987 706.1573 ## 1701 Zimbabwe Africa 1992 693.4208 ## 1702 Zimbabwe Africa 1997 792.4500 ## 1703 Zimbabwe Africa 2002 672.0386 ## 1704 Zimbabwe Africa 2007 469.7093 If you look at the GDP database above, you will see that the data also includes the years of the GDPs. What if I want to summarize the data by country? Say for each country, I just want the average of all GDPs of the different years?. For that we use the function summarize from the dplr package. Like this: GDPData %&gt;% group_by(country) %&gt;% summarize(mean_GDP = mean(gdpPercap)) Ok, now let that code talk to you. Code is like reading a book; you read from the first word forward to start making a visual representation of that the sentence is saying. Coding is not different. Lets take the code above and read it from left to right, like in the image below. In the image below you can see what each part of the code is saying. Figure 3.8: Filter function It is a good practice for you to read the code. Lets tray to translate the code above (look at the image above as we do the translation). Here I go.first I take the dataframe called GDPData, then I group that dataframe by country, then I summarize the data on GDP by the mean values, oh, and I want the resulting mean to be called mean_GDP. Easy right!. "],["merge-data.html", "Merge data", " Merge data One other very common operation in R coding is to merge dataframes by common attributes (i.e., common column). In the section about Data structure types, we sow how to merge matrices or dataframes using the rbind or cbind functions, which work when the different databases can be paired by simply appending one database to the other. For instance, when the columns are in exact order in the two databases. Here we will use the merge function, or so call full_join from the dplr package, which works for cases when the data cannot be paired by simply appending one database to another. For instance, when the columns and/or rows are not in the exact order. Figure 3.9: Merging function Lets use an example. As you know we have already loaded in this current section of R a database of the GPDs of the countries in the world. What if I now collect another database about the average life expectancy per country, and want to put those two databases together? Since the data comes from difference sources, chances are that they differ in the number of columns, in the order of the columns, in the number of rows, etc. Basically, you cannot use rbind or cbind in this case. You could do that pairing of the two database by brutal force, copying the data from each country from one database, and pasting it in the same row for the given country in the other database. Say that operation takes you ten seconds for each country, and there are 250 countries, then that job will take you about 40 minutes, not to mention the chance for errors. Oh, but remember that each country has 40 years of data. So multiplyy the 40 minutes times 40 years, and you end up copying and pasting for more than a day. Alternatively you have R do the merging for you. Lets try it. First, lets load the data on the life expectancy, which I have placed in my Github folder ExpectancyData=read.csv(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/Countries_LifeExpectancy.csv&quot;) It is good practice to always check at least a portion of the database to check it has been loaded correctly, tail(ExpectancyData) #lets look at the column head of the Expectancy data we just loaded. ## X country continent year lifeExp ## 1699 1699 Zimbabwe Africa 1982 60.363 ## 1700 1700 Zimbabwe Africa 1987 62.351 ## 1701 1701 Zimbabwe Africa 1992 60.377 ## 1702 1702 Zimbabwe Africa 1997 46.809 ## 1703 1703 Zimbabwe Africa 2002 39.989 ## 1704 1704 Zimbabwe Africa 2007 43.487 Lets check the GDP database as well. tail(GDPData) #lets look at the column head of the GDP data we loaded earlier. ## country continent year gdpPercap ## 1699 Zimbabwe Africa 1982 788.8550 ## 1700 Zimbabwe Africa 1987 706.1573 ## 1701 Zimbabwe Africa 1992 693.4208 ## 1702 Zimbabwe Africa 1997 792.4500 ## 1703 Zimbabwe Africa 2002 672.0386 ## 1704 Zimbabwe Africa 2007 469.7093 Ok, we now have two databases loaded in R, one for the data on GDP and the other for the data on life expectancy. Lets merge them together, like this: MergeData &lt;- full_join(GDPData, ExpectancyData, by = c(&quot;country&quot;,&quot;year&quot;)) Now lets try to read, that line of code, like this: Figure 3.10: Merging function If I translate thisbasically, create a new dataframe called MergeData, in which I want to merge in full the database on GDPData and ExpectancyData by their common attribute country and year. tail(MergeData) #Check the results ## country continent.x year gdpPercap X continent.y lifeExp ## 1699 Zimbabwe Africa 1982 788.8550 1699 Africa 60.363 ## 1700 Zimbabwe Africa 1987 706.1573 1700 Africa 62.351 ## 1701 Zimbabwe Africa 1992 693.4208 1701 Africa 60.377 ## 1702 Zimbabwe Africa 1997 792.4500 1702 Africa 46.809 ## 1703 Zimbabwe Africa 2002 672.0386 1703 Africa 39.989 ## 1704 Zimbabwe Africa 2007 469.7093 1704 Africa 43.487 "],["subsetting-columns.html", "Subsetting columns", " Subsetting columns From the new database I just created, you can see how there are some columns that came with the original databases, which I do not need and do not I want to use. For this, you can create a new database with only the columns you want or delete the columns you do not like. Lets try. Deleting columns This is the simplest approach. Basically, I set the column I want to delete to NULL. Like this, MergeData$X= NULL #with this code I delete the column called X, which is probably an index number used by one of the data sources Now, you try to delete the column continent.x. Selecting columns Obviously, if you have many columns that you do not need, deleting columns may take a while, as you have to type each column name you want to delete. Alternatively, you can simply select the columns you need. Like this, SelectedColumns=MergeData[, c(&quot;country&quot;,&quot;year&quot;, &quot;gdpPercap&quot;, &quot;lifeExp&quot; ) ] This syntax should be familiar to you already. As it is the indexing function we used earlier [rows, columns]. If you read the code above, basically.I created a new database called SelectedColumns, which takes the database MergeData and select all rows (because there is nothing to the left of the comma) and the four columns listed in the vector after the comma. head(SelectedColumns) ## country year gdpPercap lifeExp ## 1 Afghanistan 1952 779.4453 28.801 ## 2 Afghanistan 1957 820.8530 30.332 ## 3 Afghanistan 1962 853.1007 31.997 ## 4 Afghanistan 1967 836.1971 34.020 ## 5 Afghanistan 1972 739.9811 36.088 ## 6 Afghanistan 1977 786.1134 38.438 f "],["adding-results-to-databases.html", "Adding results to databases", " Adding results to databases Up to this point, we have simply manipulated the data other people collected. What if we now want to do our own analysis with such data? Piece of cake lets tray, for instance, a new index, in which I want to see how much does it cost a year of life in each country. Basically, all I have to do is to divide how much each person does in each country (i.e., GDP per capita) by the life expectancy of people in each country. Like this, SelectedColumns$ValueofLife = SelectedColumns$gdpPercap / SelectedColumns$lifeExp WE cant translate the code above as, a a column called ValueofLife to the dataframe SelectedColumns, in which I divide gdpPercap by lifeExp. Lets check the results, head(SelectedColumns) ## country year gdpPercap lifeExp ValueofLife ## 1 Afghanistan 1952 779.4453 28.801 27.06313 ## 2 Afghanistan 1957 820.8530 30.332 27.06228 ## 3 Afghanistan 1962 853.1007 31.997 26.66190 ## 4 Afghanistan 1967 836.1971 34.020 24.57957 ## 5 Afghanistan 1972 739.9811 36.088 20.50491 ## 6 Afghanistan 1977 786.1134 38.438 20.45146 Lets do a quiick summary to see the average per country, SelectedColumns %&gt;% group_by(country) %&gt;% summarize(mean_valueOfLife = mean(ValueofLife)) ## # A tibble: 142 x 2 ## country mean_valueOfLife ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 21.8 ## 2 Albania 46.7 ## 3 Algeria 73.7 ## 4 Angola 97.2 ## 5 Argentina 129. ## 6 Australia 263. ## 7 Austria 273. ## 8 Bahrain 271. ## 9 Bangladesh 16.3 ## 10 Belgium 266. ## # ... with 132 more rows Have a look at the result above. Isnt it fascinating how in Afghanistan we could increase the life expectancy by one year by simply having people making $21.8 dollars more! "],["exercise-3.html", "Exercise", " Exercise "],["homework-1.html", "Homework", " Homework Do the following exercise: Open a new file in Tinn-R. and write the R code to do the following: Load the GDP and life expectancy databases, which we already loaded. Also add a database called, Countries_Population.csv, which is in the same path you used for the other two databases thata re online in my Github folder. Merge the three databases, by country and year. Remove all unnecessary columns (you only want country, year, GDP per capita, life expectancy and population). Calculate the total GDP of each nation (basically, multiple per capita GDP by the number of people). Calculate the average Total GDP of each country. Save the necessary code in Tinn and have it ready for next class. "],["4-basic-plots.html", "4 Basic plots", " 4 Basic plots Making good quality plots is fundamental to the successful delivery of a scientific finding. The best story is the one that can be told with figures o. Not surprising, making good figures is a demanding endeavor of the scientific process, and there are at least two steps to it. First, you need to decide on which type of figure best illustrates the point that you what to make. It is not about deceiving the reader, but rather use the most convincing available figure. Of course, each problem will have different needs, so you just need to be familiar about the different types of figures available, to better know which is the best figure for your own case problem. Figure 4.1: R plots R has several built-in functions to do all sorts of plots. You can even create your own types of plots. Some packages even allow you to create plots online, where people can interact with the data. Others let you animate the data. You can check a gallery of R plots here. Figure 4.2: Animated R plot Second, it is the issue of the standards of the figure. This relates to the editorial guidelines you need to follow to deliver your figure. Each scientific journal has different guidelines for figures. Some journals allow you to place multiple figures together, others like the letters to be of a given size and font type, other journals only like black and white, others want the figures on certain dimensions, etc. So you need to know the guidelines that are specific to the journal you want to publish on to ensure your figures are up to standard. If you are not publishing a paper, it will still be good for you to follow the guidelines of figure formatting for any journal, so you know your figures are up to the most professional standards required in the field. Regardless of the guidelines for the specific journal, it is important that your figure is clear enough to be understood. Here are some key conditions for any figure: Ensure that the axes have names. e.g. Human population. Ensure that between brackets the axes have units. e.g. Human population (Number of people) Ensure that your axes have tickmarks. Ensure all data are displayed. If you constraint the values shown in your axes, be sure you do not remove data by accident. Ensure the lines of the axes are black color. At times the axes colors are grey. The problem with this setting is that when printed, such axes may not appear, depending on how good your printer is. Ensure the thickness of the axes is at least 1 point. At times the lines are thin, and this creates problems with printing as well. When possible use black and white for your figures. While color is very common, many people take photocopies of papers or reports and they are very common in black and white. So in those cases any color figure will have troubles. p Lets check how to do some of the most common types of plots in R. "],["scatterplots.html", "Scatterplots", " Scatterplots Scatterplots allow you to see patterns of variation between two variables. To create a plot in R, simply type plot(y~x). Lets say I want to plot the second and third column in a dataframe. #Lets create some dummy data DataFrame&lt;- data.frame( x1 = c(rep(1,250)), # in Column 1 I repeat the number 1 for 250 times x2 = seq(1:250), #Column 2 I create a sequence of numbers from 1 to 250 x3 = sample(seq(1:1000),250)) # select 250 random numbers between 1 and 10000 #Now lets plot columns 2 and 3 of that data plot(DataFrame[,2]~DataFrame[,3]) #You can also plot by column name using the $ sign as indicated earlier to call a column plot(DataFrame$x2~DataFrame$x3) Now it comes the different settings needed to format the figure. Almost any attribute of a figure in R can be formatted. Next, I will show you how to format a few parameters. Once you know how to format a given parameter of a figure, you should be able to format any other parameter. Obviously a figure has tens of parameters you can modify, so you should not expect to know them all by memory, but you should be able to know how to find the given parameter, which by now you know it is by asking your friend Google. Lets modify a few parameters from the figure above. For instance, what about the axis names? You can add axis names using the command xlab or ylab inside the plot command, like this: plot(DataFrame[,2]~DataFrame[,3],xlab=&quot;Years&quot;, ylab=&quot;Precipitation&quot;) What about the different type of symbols? Figure 4.3: R plot symbols R offers 25 different symbol types (Figure above), which you can call using the parameter pch, like this: plot(DataFrame[,2]~DataFrame[,3],xlab=&quot;Years&quot;, ylab=&quot;Precipitation&quot;, pch=22) Other characters can be used to specify pch including +, *,-,.,#, %, o. Or simply any character you put between the quotations () in the pch parameter. What about colors for those points?. That is controlled using the col parameter. Like this, plot(DataFrame[,2]~DataFrame[,3],xlab=&quot;Years&quot;, ylab=&quot;Precipitation&quot;, pch=22, col=&quot;red&quot;) For some symbols you can also control the filling color, using the bg parameter. Like this, plot(DataFrame[,2]~DataFrame[,3],xlab=&quot;Years&quot;, ylab=&quot;Precipitation&quot;, pch=22, col=&quot;red&quot;, bg=&quot;blue&quot;) You can also control the size of the symbols using the cex parameter. Like this, plot(DataFrame[,2]~DataFrame[,3],xlab=&quot;Years&quot;, ylab=&quot;Precipitation&quot;, pch=22, col=&quot;red&quot;, cex=2) Remember, there are tens of parameters you can modify in a plot. So you need to know exactly the standards required in the journal you want to publish to ensure you deliver the best figures required. "],["histograms.html", "Histograms", " Histograms Histograms are a very important type of plot as it lets you see the frequency in which certain values appear in your data. This type of plot is also call a frequency distribution. To create a histogram in R, you use the command hist(x), where x, is the vector of data you want to plot. Lets create a frequency distribution of the GDP (Gross Domestic Producto) of countries in the world, using the csv file we loaded earlier, #lets reload the data, just in case you have not loaded it GDPData=read.csv(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/Countries_GDP.csv&quot;) head(GDPData) #now lets check the data ## country continent year gdpPercap ## 1 Afghanistan Asia 1952 779.4453 ## 2 Afghanistan Asia 1957 820.8530 ## 3 Afghanistan Asia 1962 853.1007 ## 4 Afghanistan Asia 1967 836.1971 ## 5 Afghanistan Asia 1972 739.9811 ## 6 Afghanistan Asia 1977 786.1134 hist(GDPData$gdpPercap) #now lets create a frequency of number of countries by GDP Just as with the scatterplot, you can improve the appearance of the figure. Lets start with the axis name. hist(GDPData$gdpPercap, xlab=&quot;GDP countries in USDollars&quot;) What about that title?. Not pretty, ah?. That is a default in R. To remove it, we set the main parameter to NULL, like this: hist(GDPData$gdpPercap, xlab=&quot;GDP countries in USDollars&quot;,main=NULL) Most journals do not allow you to put tittles in your plots. If you want to rather keep the tittle, but have a different tittle, you replace NULL with the title you want, like this: hist(GDPData$gdpPercap, xlab=&quot;GDP countries in USDollars&quot;,main=&quot;Countries of the World&quot;) "],["density-plots.html", "Density plots", " Density plots At times, when you use scatterplots with many data points, chances are that some points will overlap, and then create a misleading visual representation of the data as any overlapping data points will appear as a single point. A better representation of the data in this type of case is the use of density plots, in which the space of the entire plot is gridded into equal size cells, and the number of point overlapping on each cell counted and that is what is displayed. Lets do an example. #lets create a dummy dataset of many points # Create data x &lt;- rnorm(mean=1.5, 5000) y &lt;- rnorm(mean=1.6, 5000) #lets plot that data plot(y~x) From the figure above you can tell that it is hard to make sense of any pattern because many points overlap. One solution to this is to use a density plot. And there are different packages to do so. Here we will use the hexbin package. # Packages library(hexbin) library(RColorBrewer) #This library allows you to create color scales, we will see this later. ## Warning: package &#39;RColorBrewer&#39; was built under R version 4.0.3 # Make the plot bin&lt;-hexbin(x, y, xbins=40) #hexbin is the function to grid the points in the plot. You can use different number of grids. my_colors=colorRampPalette(rev(brewer.pal(11,&#39;Spectral&#39;))) #this is the color scale plot(bin, main=&quot;&quot; , colramp=my_colors , legend=F ) #now lets plot the hexbin/grid Now you can see the same data, but plotting the hexbin/grid. You can play with different settings of the hexbin package, by typing ?hexbin in the R console. You can also display both plots side by side using the par function. par(mar = c(4, 4, .1, .1)) plot(bin,colramp=my_colors) #hexbin plot plot(y~x) #Scatter plot "],["plotting-maps.html", "Plotting maps", " Plotting maps R also provides powerful tools to analyze geographical data. Pretty much anything you can do in ARCgis, you can do in R; the key difference is that in R is free!. Lets see how to plot a map in R. Lets start by getting some spatial data. I collected the global data on human population from here. That page is full of other types of global scale data you may find interesting. Here we will use the package raster for the purpose of loading and plotting the map. library (raster) GlobalPopulation=raster(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/GlobalHumanPopulation2020.tif&quot;) #load the raster. In the code above, I create a variable called GlobalPopulation that has the global data on human population. Note that to load the data of the raster, you use the same method you used before to load the .csv file. That is, using the path to the file, preceded by the command that reads the file. In this case, you use the command raster to read the file rather than read.csv. as the data you want to load is a raster and not a csv file. To plot the raster, all you have to do is to plot that variable, like this: plot (GlobalPopulation) You can use a different color scale using the RColorBrewer library. Lets try, library(RColorBrewer) #this library allows you to create your own scales #Lets create a color scale between yellow and red colors ColorScale &lt;- colorRampPalette(c(&quot;yellow&quot;,&quot;red&quot;)) plot(GlobalPopulation,col =ColorScale(100)) #plot human population with 100 color in my scale There are also premade color blind friendly scales. The most commonly used is called viridis. Here are the different options of scales it has to offer: Figure 4.4: Viridis color scales Lets now plot our map, using the magma scale library(viridis) ## Loading required package: viridisLite plot(GlobalPopulation,col =magma(1000)) #plot human population with 100 color in my scale Hmm, not to pretty ah?. why do you think the map does not look that good? Well, the reason is that there are some places in the world with soo many people that makes the rest of the world look by comparison like if the world was empty. Here the problem is the scale of comparison. In cases where the extremes of the data are to far a part, it is recommended to log transform your data, which brings some of those extremes closer. Lets see what log-transforming does, plot(log(GlobalPopulation),col =magma(1000)) #log transform human population Now you can better appreciate the patterns of variation, but you need to be mindful that the data are logarithmic. R also allows to reproject the data into different projection types. You should be aware that there are many issues with displaying maps in different projections, that is topic for another course. Raster &lt;- projectRaster(GlobalPopulation, crs=&#39;+proj=moll&#39;, over=T) #reproject raster to mollweide projection ## Warning in rgdal::rawTransform(projto_int, projfrom, nrow(xy), xy[, 1], : 55946 ## projected point(s) not finite plot(Raster) Hmm that raster of the worlds population is missing an outline of the world. For that, there is a package called maps. Lets try. library(maps) # this library offers several choices for background maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map worldmap &lt;- map(&quot;world&quot;, plot=F,interior = F) #create map of the world plot(worldmap) #check the map Hmm that packages provides the outline of the world as points. But really we want lines. Ok, we can use the maptools to convert points to lines library(maptools) #library to convert points to lines ## Checking rgeos availability: TRUE worldmapLines &lt;- map2SpatialLines(worldmap, proj4string=CRS(&quot;+proj=longlat +datum=WGS84&quot;)) plot( worldmapLines) #check the map Great, now you have a raster of the worlds human population and an outline of the world. Lets put them together plot(GlobalPopulation) #first plot your raster plot(worldmapLines,add=T) #then add the outline of the world on top. Find out the command to remove the color scale shown in the map above. Tip: type How to remove color scale of plot raster in r "],["animated-plots.html", "Animated plots", " Animated plots What about animated plots?. If you recall the database on socio-economic data from countries in the world, you will notice that each variable was collected at different years. What year to display?. Well you can display them all and see how any pattern will change over time using an animated plot that displays each year sequentially. For animated plots, we will use the ggplot2 and the gganimate libraries/packages. i #first lets bring the data. ExpectancyData=read.csv(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/Countries_LifeExpectancy.csv&quot;) GDPData=read.csv(&quot;https://raw.githubusercontent.com/Camilo-Mora/GEO380/main/Datasets/Countries_GDP.csv&quot;) #lets merge those two databases, using the dplyr packaged that we used earlier. library(dplyr) #load package to merge database #merge the databases MergeData &lt;- full_join(GDPData, ExpectancyData, by = c(&quot;country&quot;,&quot;year&quot;)) As it is customary, lets check the data were loaded and display correctly. tail (MergeData) ## country continent.x year gdpPercap X continent.y lifeExp ## 1699 Zimbabwe Africa 1982 788.8550 1699 Africa 60.363 ## 1700 Zimbabwe Africa 1987 706.1573 1700 Africa 62.351 ## 1701 Zimbabwe Africa 1992 693.4208 1701 Africa 60.377 ## 1702 Zimbabwe Africa 1997 792.4500 1702 Africa 46.809 ## 1703 Zimbabwe Africa 2002 672.0386 1703 Africa 39.989 ## 1704 Zimbabwe Africa 2007 469.7093 1704 Africa 43.487 It seems data were loaded fine. The three variables of interest here are gdpPercap (GDP per capita), lifeExp (life expectancy), year and continent. (note that continent is double, because that column was on each of the dataset we merged, and we did not specific to do the merging by continent as well). Lets create an animated plot of that data. Since the code needed will require several lines, it is recommended that you create the code in Tinn-R firts. Once it is done, copy and paste in the R console. # for the figures we will need library(ggplot2) #this package is to do all sorts of plots library(gganimate) #this package does the animation # Make a ggplot, but add frame=year: one image per year ggplot(MergeData, aes(gdpPercap, lifeExp, color = continent.x)) + geom_point() + scale_x_log10() + theme_bw() + # gganimate specific bits: labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;life expectancy&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) # Save at gif: anim_save(&quot;AnimatedPlot.gif&quot;) The code above generate the plot below, which should now appear in your working folder. You can find out your working directory by typing getwd() in your R console. You can also set the working folder to a different path using the setwd(); in between brakets you put the path to the folder you want R to deposit any result from your codes. For instance, setwd(D:/GEOG380), sets my working directory to a folder called GEOG380 in my D harddrive. Figure 4.5: Animated R plot "],["saving-plots.html", "Saving plots", " Saving plots Of course, if you do a plot in R is because you want to use it somewhere else. The quick and very dirty way to get a figure from R is to use the simple PrintScreen in your keyboard, which will create a figure of whatever you have in your screen at the moment. Figure 4.6: Pixelation effect of figure format After clicking on Print screen, then open any software, like Paint, Word, PowerPoint, etc., then right click on a new document and click Paste to display the figure. Of course, R offers many formats to save plots. Here we will save a figure as a PDF using the function pdf. Basically, the function pdf requires you to write the name you want for your file (e.g. RPlot.pdf), how big you want the plot, the size of the letters, and few other options. Type ?pdf to see additional settings. The way to save any figure in R is like this: Firsts, you write the line of code indicating the type of file you want to save as. That creates a clean sheet, where you can place your plot or plots. Once done with your figure or figures, you need a line of code called dev.off(); that closes the sheet and save the file. Lets try, pdf(&quot;RPlot.pdf&quot;, width = 3, height = 3,pointsize=8,paper=&quot;letter&quot;) plot(bin,colramp=my_colors) #hexbin plot dev.off() If you now check your working folder, there should be a file called RPlot.pdf, which has the figure you just created. Of course, you can edit the figure as you like, as we showed in the Scatterplot section. The benefit of l saving files as PDF is that the resolution is never affected. You can zoom in into a PDF file and the resolution is always the same. In turn, other types of figures like jpeg, gif, png, etc. you need to define the resolution in advance, and if you zoom in into those type of figure formats you will notice the pixelation. Figure 4.7: Pixelation effect of figure format "],["excersise.html", "Excersise", " Excersise Now that you know how to manipulate data and do some basic figures. For this chapters exercise, please find the guidelines for figures in the journal Nature and do a plot according to the standards required for that journal. Plot the relationship between GDP and population size, using the databases that have been provided to you already, and include of the basic criteria for figures mentioned at the start of this chapter. Place that plot in a Word document and submitting as the homework for this chapter. "],["5-descriptive-statistics.html", "5 Descriptive statistics", " 5 Descriptive statistics Ok, guys, by now you know how: to set a question, reformulate the question based on your literature review, set up an error proof experiment, enter and manipulate data in R and do some basic plots in R. Nice, ah?. Now we move on to the analysis of any data you collect. That process is called inference; basically, drowning a conclusion based on the numbers and your interpretation of those numbers. iii Inference is defined as the process of drawing conclusions based on evidence and reasoning. Once you finish an experiment and collect your data, there are a few things to do: First, you need visualize the data, using plots you learned how to do in the prior chapter. Second, you need to use a set of available metrics to describe your data in general. You need to get the big picture first. Those BIG picture metrics are called DESCRIPTIVE STATISTICS. Say you get hired to analyze the visiting times of costumers in a store. The manager wants to ensure his costumers are always being attended to so he wants to hire more people at pick hours. To tackled this problem, you probably want to record the time people walk into the store. Lets say 100 people came in a day. If you want to report your findings to the manager, you cannot just go and tell him, the first costumer came at 7 am, the second at 7:10, the third at 8:20 am.(one hour later)and the 100th costumer came in at 7:00pm. Very likely there is a pattern in that data, but the manager cannot really perceived it when the data are presented in raw form. So the manager cannot really make any decision to solve his problem based on what you did, so he may not have a reason to keep you. Really what you want to do is to describe that data for him, as to facilitate his decision. The metrics used to describe data overall are called descriptive statistics. Such metrics are commonly divided into those dealing with what is at the middle of your data (the so-call metrics of central tendency) and those dealing with how variable your data are (the so-call metrics of dispersion). "],["data-distributions.html", "Data distributions", " Data distributions As mentioned several times in this class, the first thing to always do when analyzing data is to create a visual representation. You need to see what are the general tendencies in your data, and for that we commonly start with the frequency distribution. Lets image that the plot below is the number of seedlings (points) at each height in my nursery. Interesting, ahh?. That type of distribution is called bimodal (there are two peaks in the data). Clearly, there is something going on in my nursery, some seedlings are doing particularly better than others. p The distribution below is called a uniform distribution, any number is just as likely. o The distribution below is called a left distribution. I know, it is weird how the data tend towards the right but it is called a left distribution. It is also called negative or asymmetrical distribution. Because it has only one peak, it can also be call unimodal distribution. In the figure below, you can explore different distributions, "],["central-tendency.html", "Central tendency", " Central tendency Central tendency refers to metrics that attempt to describe what is in the middle of your data. The main metrics of central tendency are the mean (also called average), the median, and the mode. Figure 5.1: Measures of central tendency l Lets create some dummy data to see what these metrics are intended to show, how to interpret them and when to use them. set.seed (10) y &lt;- rnorm(1000) # the function rnorm allows you to create a random set of numbers from a normal distribution #lets now plot that data using the hist function, you already know how to use: hist(y, main = &quot;Normal Distribution&quot;,breaks = 30) The arithmetic mean The arithmetic mean is the average of the numbers. It is easy to calculate: add up all the numbers, then divide by how many numbers there are. Mathematically, it is expressed as: \\[\\begin{equation} \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} \\end{equation}\\] The mean of a sample is denoted wit the letter \\(\\bar{X}\\); at times it does not include the horizontal bar above the letter. The mean of a population is at times denoted with the lowercase letter \\(u\\), at times it is not cursive. They are both calculated the same way. The symbol that looks like a pacman, \\(\\sum_{}\\), is call summation. The lower letter \\(i\\) indicates that the summation starts from the first number (i.e., i=1), and the higher letter n, means the last number. \\(Xi\\) means each value. In short, the numerator of the equation above says: sum all values starting from the first one until the last one. \\(n\\) indicates the sample size or total number of values in your database (if it is was the average of a population, you will likely see capital letter \\(N\\) instead of the lowercase \\(n\\) ). Figure 5.2: The stupid mean y In R, the mean is calculated with the function mean. MEAN=mean (y, na.rm = FALSE) #here I calculate the mean, and assign it to a variable. MEAN #to display the variable, simply type it, and click enter ## [1] 0.01137474 You may have noted that I added na.rm = TRUE in the mean function. This command is used as a caution to calculate the mean even if you have empty values. Lets check, VectorMissingAValue &lt;- c(12,7,3,4.2,18,2,54,-21,8,-5,NA)#here I created a vector of values, incluidng a missing value NA (NA in R stands for Not Available) mean (VectorMissingAValue) #If I calculate the mean of a vector with missing values I get ## [1] NA Now, if you include the command na.rm = TRUE, that will re-do the calculation excluding any missing value. mean (VectorMissingAValue,na.rm = TRUE) ## [1] 8.22 In most R functions, it is important to include the command na.rm = TRUE, which means to exclude any missing value. If for any reason your data has empty cells or missing values, falling to include the na.rm = TRUE command will result in an error.again this will be a problem only when you are missing data. Alright, lets keep going. You can visualize the mean in the histogram using the function abline, and put a name using the text function, like this: hist(y, main = NULL,breaks = 30, ylim=c(0,100),) abline(v=MEAN, col=&quot;red&quot;,lwd=5) #abline put a line in a figure...you use v for vertical and h for horizontal, the value that proceeds is where the line will be drawn. text(&quot;Mean&quot;,x=MEAN,y=100, col=&quot;blue&quot;) #the text function places a label on your figure at the given x and y position you like...there are several other parameters you can control. You can also use the function *legend*, which allows you more controls, including background colors. The trimmed mean There are several variations to calculate the mean, for instance the trimmed mean. One of the problems with the arithmetic mean is that it can be strongly impacted by extreme values. In those cases, it is recommend to trim the extremes and calculate the mean on the resulting values. Commonly, a %5 trimmed mean should satisfy. As its name indicates, you remove the top and bottom 5% of the data, and calculate the mean on those values. Lets try a simple example. Say you measured the time it took 20 students to run 100m. The individual values in seconds were 10, 20, 23, 23, 22, 20, 25, 22, 20, 25, 22, 20, 25, 20, 21, 21, 23, 21, 24, and 120. To calculate this in R, I put all values in a vector and calculate the mean, vals=c(10,20,23,23,22,20,25,22,20,25,22,20,25,20,21,21,23,21,24,120) mean(vals) ## [1] 26.35 But from the data, you can tell that one student was very fast (only took 10 secs) and another one was very slow (he took 120secs). Lets try a 5% trimmed mean, First, you sort the data from smallest to largest, 10,20,20,20,20,20,21,21,21,22,22,22,23,23,23,24,25,25,25,120 Now you remove the first and last (i.e, 5% of 20 is 1, so you delete the bottom and top one records, red numbers in the array). Now calculate the mean of the resulting data or the so-call 5% trimmed mean: Trimmedvals=c(20,23,23,22,20,25,22,20,25,22,20,25,20,21,21,23,21,24) mean(Trimmedvals) ## [1] 22.05556 The 5% trimmed mean is 22.06 compared to the arithmetic mean 26.35. The weighted average Sometimes we wish to average numbers, but we want to assign more importance, or weight, to some of the numbers. For that we used the weighted average, which is calculated as: \\[\\begin{equation} Weighted \\ average= \\frac{\\sum_{i=1}^n (X_i * w)}{\\sum_{i=1}^n w} \\end{equation}\\] where \\(Xi\\) is a data value and w is the weight assigned to that data value. Lets try an example. Suppose your professor tells you that your grade will be based on a midterm and a final exam, each of which is based on 100 possible points. However, the final exam will be worth 60% of the grade and the midterm only 40%. How could you determine an average score that would reflect these different weights? The average you need is the weighted average. If you scored 83 in your midterm and 95 in your final, what will be your final grade?. \\[\\begin{equation} Weighted \\ average= \\frac{(83*40)+(95*60)}{(40+60)} \\end{equation}\\] So your final grade will be 90.2. Your average is high enough to earn an A, which I hope you all get in my class!. And if you have been paying attention, here are two tokens: jj The median The median (also referred to as the 50th percentile) is the middle value in a sample of ordered values. Half the values are above the median and half are below the median. To calculate the median, you start by sorting all values from lowest to highest, the median is the value in the middle. When your set of numbers is odd, the median is the single value in the middle of the sorted list. Below is a vector of values indicating the price of different candy. Figure 5.3: Median calculation for odd number of values When your set of numbers is even, you have a problem because there will be two numbers right in the middle of the sorted vector. In this case you report the arithmetic mean of the two values in the middle. Figure 5.4: Median calculation for even number of values y In R, the median is calculated with the function median. MEDIAN=median (y) #here I calculate the mean, and assign it to a variable. MEDIAN #to display the variable, simply type it, and click enter ## [1] -0.003001333 p Lets plot the median with the mean value to the histogram we were doing before. hist(y, main = NULL,breaks = 30, ylim=c(0,110),) #lets plot the mean MEAN=mean(y) abline(v=MEAN, col=&quot;red&quot;,lwd=5) text(&quot;Mean&quot;,x=MEAN,y=110, col=&quot;red&quot;,pos=4) #pos=4 means, to place the text to the right of the coordinates xy. # now the median abline(v=MEDIAN, col=&quot;blue&quot;,lwd=3, lty=2) #I plot the median in blue to distinguish from the mean text(&quot;Median&quot;,x=MEDIAN,y=100, col=&quot;blue&quot;, pos=2) The mode The mode is simply the most common value in all of your data. R does not have a standard in-built function to calculate mode. So we create a user function to calculate the mode of a dataset. This function takes a vector as input and gives the mode value as output. getmode &lt;- function(v) { # the function will take a vector of values uniqv &lt;- unique(v) #select all the unique values in the vector uniqv[which.max(tabulate(match(v, uniqv)))] #stimate the number of times each value appears and return the largest } Now lets use our new function MODE=getmode(y) #here I calculate the mode, and assign it to a variable. MODE #to display the variable, simply type it, and click enter ## [1] 0.01874617 Unlike the mean and the median, the mode can also be used with character data (i.e., words). For instance, you want to know what is the most common name in the USA?. For that you can use the mode. # Create the vector with characters. NAMES &lt;- c(&quot;Peter&quot;, &quot;Carl&quot;, &quot;Darrell&quot;, &quot;John&quot;, &quot;Peter&quot;) getmode(NAMES) #here I calculate the mode, and assign it to a variable. ## [1] &quot;Peter&quot; Lets plot our mode in the histogram with the mean and the median we were working on before. hist(y, main = NULL,breaks = 30, ylim=c(0,110),) #lets plot the mean abline(v=MEAN, col=&quot;red&quot;,lwd=6) text(&quot;Mean&quot;,x=MEAN,y=110, col=&quot;red&quot;,pos=4) # now the median abline(v=MEDIAN, col=&quot;blue&quot;,lwd=4, lty=2) #I plot the median in blue to distinguish from the mean text(&quot;Median&quot;,x=MEDIAN,y=100, col=&quot;blue&quot;, pos=2) #now let&#39;s plot the mode MODE=getmode(y) abline(v=MODE, col=&quot;orange&quot;,lwd=4, lty=3) #I plot the median in blue to distinguish from the mean text(&quot;Mode&quot;,x=MODE,y=90, col=&quot;orange&quot;, pos=4) You should have noted that the mean, median and mode are almost the same in this case. That is because the data are normally distributed (follow a normal distribution). Lets see what happens when the data are skew. k The reverse happens with a right or positive distribution: If you see the two skewed distributions above, you will notice how the mean is pulled by the extreme values of the distribution. That is a critical consideration of the arithmetic mean, it is affected by extreme values or so-call outliers. So a basic rule of thumb is to look at the mean and the median. If theyre the same you can just use the mean, thats more easy for the average reader to understand. If they differ significantly report them both, or just report the median With skew distributions is when it becomes important the distinction between the mean and the median. Lets check an example in which you can deceive a conclusion based on reporting only the mean. Figure 5.5: Income distribution in the USA The figure above shows the household income of people in the USA. Clearly, the data follows a right distribution. In this distribution, the mean is larger than the median or the mode. So if you report the mean only, it may create an illusion that most people get paid well in the USA, when in reality most get paid much less. In this case, the mean increases primarily as a result of the wealthy becoming wealthier. If we are concerned about how the average American is doing, median is actually a better measure to understand their status. The figure below summarizes the central tendency metrics by the type of data distribution. Figure 5.6: Central tendendy metrics in skew distributions "],["dispersion.html", "Dispersion", " Dispersion Dispersion refers to describing the spread of your data. And there are different metrics that may help you describe the variability in your data. The minimum and the maximum The most basic metrics of the data dispersion are the minimum and the maximum. As their names indicate they are the lowest and the highest values in your data. In R, the minimum and maximum are calculated with the function min and max, respectively. Lets plot them: hist(y, main = NULL,breaks = 30, ylim=c(0,110),) #lets plot the mean abline(v=MEAN, col=&quot;red&quot;,lwd=6) text(&quot;Mean&quot;,x=MEAN,y=110, col=&quot;red&quot;,pos=4) # now the median abline(v=MEDIAN, col=&quot;blue&quot;,lwd=4, lty=2) #I plot the median in blue to distinguish from the mean text(&quot;Median&quot;,x=MEDIAN,y=100, col=&quot;blue&quot;, pos=2) #now let&#39;s plot the mode abline(v=MODE, col=&quot;orange&quot;,lwd=4, lty=3) #I plot the median in blue to distinguish from the mean text(&quot;Mode&quot;,x=MODE,y=90, col=&quot;orange&quot;, pos=4) #now let&#39;s plot the minimum MINIMUM=min(y) abline(v=MINIMUM, col=&quot;dark grey&quot;,lwd=4, lty=3) #I plot the median in blue to distinguish from the mean text(&quot;Minimum&quot;,x=MINIMUM,y=90, col=&quot;dark grey&quot;, pos=4) #now let&#39;s plot the maximum MAXIMUM=max(y) abline(v=MAXIMUM, col=&quot;dark grey&quot;,lwd=4, lty=3) #I plot the median in blue to distinguish from the mean text(&quot;Maximum&quot;,x=MAXIMUM,y=90, col=&quot;dark grey&quot;, pos=2) The range Another simple metric to describe the dispersion in your data is called the range.u The range is the difference between the smallest and the largest number in your data. Of course, within the range everything is possible, so while descriptive and useful in many situations, it is not always the best indicator of inference. Figure 5.7: The effect of the range The percentile A percentile indicates the percentage of values that are smaller than the given value. For example, if you score 75 points on a test, and are ranked in the 85th percentile, it means that 85% of students that took that exam did worse than you. The way to report it is like this: the 85th percentile was 75 points. How is the percentile calculated?. Simple, Rank the values in your database from smallest to largest, we called this the sorted vector. Multiply the number of values in your database by the given percentile you want to find as a fraction (If you want the 90th percentile, the fraction is 0.9), we call this resulting number the index or \\(Xth\\) point Now go back to your sorted vector, starting from the first number in that vector move to the right until the value located at the Xth point. The value located at the Xth point is your given percentile. Lets do an example, Take my population of ten rabbits below, and say I want to know the 80th percentile height of my rabbits? Figure 5.8: My rabbit population Because I am interested in height, I measure their heights. Say they were 37, 15, 35, 36, 5, 40, 41, 68, 45, and 56cm (I am making these numbers up, so do not overthink the size of my rabbits). I sort my rabbits by height. Say their sorted heights are 5, 15, 35, 36, 37, 40, 41, 45, 56 and 68cm. Figure 5.9: My rabbits sorted by height Next, I multiply the percentile of interest (80th) as a fraction (i.e., 0.8) by the number of rabbits (10). So the 80th percentile height is the height of whatever rabbit is sitting in 8th position. In this case the number is 45. So the 80th percentile of my rabbit population is 45cm. Figure 5.10: The 80th percentile In R, the percentile value in an vector of values is calculated with the function quantile. Basically, you need to enter the array of values, and the quantile you need (as a fraction). Lets take the height of my rabbits above. MyRabbits=c(37, 15, 35, 36, 5, 40, 41, 68, 45, 56) #vector of rabbit height, it does no quantile(MyRabbits, .8,type = 1) # the type parameter allows you to use different ways to approximate the value. check ?quantile for more details ## 80% ## 45 The quantile The quantiles are specific percentiles that divide the data in equal amounts. Quantiles can take different names. For instance, quartiles break the data in four groups, deciles in ten groups, and percentiles in 100 groups. Lets take the quartile example. In this case, the data needs to be broken in four groups, so the breaking points will be 0.25, 0.5, and 0.75. Those break points will create four different categories in which I can group my data (0 to 0.25], (0.25 to 0.5], (0.5 to 0.75] and (0.75, 1]. They can also be called the 1st quartile, 2nd quartile, 3rd quartile and 4th quartile. You can see those indexes at times being called Q1, Q2, Q3, and Q4. Lets display the quartiles for a normal distribution. library (RColorBrewer) library(dplyr) library(scales) set.seed(3) dt &lt;- data.frame(x=rbeta(20000,100,100)*20000) #Lets create some dummy normal data dt$x &lt;- rescale(dt$x,to = c(1, 100), from = range(dt$x)) #I rescale the data from 1 to 100 to better visualize the percentage results dt=dt %&gt;% arrange(x) #Sort values from smallest to largest dt$Position=1:nrow(dt) #Rank each value from smallest to largest dt$y=(dt$Position/nrow(dt)) *100 #Assign the percentile position of each value #Create colors to make the plot look nicer ColSca &lt;- brewer.pal(9, &#39;YlOrRd&#39;) #Create a vector of nice colors in the YlOrRd color scale ColSca &lt;- colorRampPalette(ColSca) #create a color ramp for the colors above Colors= ColSca(110) #create list of 100 colors in the ramp above #create histogram hist(dt$x, col=Colors,main = NULL,breaks = 100, ylim=c(0,800),xlab=&quot;Percent&quot;, ylab=&quot;Frequency&quot;) #calculate and add the quartiles and their names to the histogram. Prob=c(0.25,0.5,0.75,1) # this is the list of values that define my four quartiles Quartiles=quantile(dt$x,Prob ,type = 1) #calculate the quartiles of my data abline(v=Quartiles, col=&quot;grey&quot;,lwd=4, lty=1) #I plot the quartiles # now lets put names Names=c(&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;) #vector of names for my quartiles text(Names,x=Quartiles,y=800, col=&quot;black&quot;, pos=2) #place quartile names at y=800 and x= quartile values The variance The variance measures how far a set of numbers is spread out from their average value. Put in another way, the variance measures the average degree to which each number is different from the mean. For a sample, the variance is calculated as: \\[\\begin{equation} Variance = s^2=\\frac{\\sum_{i=1}^n (X_i-\\bar{X})^2}{n-1} \\end{equation}\\] For a population, the variance is calculated as: \\[\\begin{equation} Variance = ^2=\\frac{\\sum_{i=1}^n (X_i-u)^2}{N} \\end{equation}\\] The parameter \\((X_i-\\bar{X})^2\\) is called sum of squares. Becuase each value is compared to the mean, if you do not square it, the result will be zero because the negative values will cancel the positive values. Squaring the difference between \\((X_i-\\bar{X})\\) will make the quantity nonnegative. A few things you may have notice about these two equationss. The equations are almost identical to the arithmetic mean. Indeed, they are. The variance can be thought of as the average of the differences from each value to the mean. When calculating the variance of a population we use the capital letter N but lowercase letter n when it is a sample. The variance of a population is defined with the letter \\(^2\\); for a sample it is the lowercase \\(s^2\\). When calculating the variance of a sample, the denominator is n-1 but for a population is N. Since a random sample usually will not contain extreme data values (large or small), we divide by \\(n-1\\) in the formula for s to make s a little larger than it would have been had we divided by n. It is called the unbiased estimate for s. If we have the population of all data values, then extreme data values are, of course, present, so we divide by N instead of \\(n-1\\). Lets calculate the variance using a neat example, I took from Here. Here a group of children measured the height of their dogs in mm. Figure 5.11: Sample of dogs heights The heights (at the shoulders) are: 600mm, 470mm, 170mm, 430mm and 300mm. Next, you calculate the sample mean height, which is 394 mm (green line in figure below). HeightOfDog=c(600, 470, 170, 430 , 300) mean(HeightOfDog) ## [1] 394 j Figure 5.12: Sample of dogs heights Next, for each dog, you calculate the difference of its height to the mean (Basically, the height between the red and the green line below), Figure 5.13: Sample of dogs heights Lets isolate the individual differences: Figure 5.14: Sample of dogs heights Following the equation of the variance for a sample, you square each difference, then add them and divide them by the sample size minus one. \\[\\begin{equation} Variance = s^2=\\frac{206^2 + 76^2 + (224)^2 + 36^2 + (94)^2}{5-1} \\end{equation}\\] \\[\\begin{equation} Variance = s^2=27130mm^2 \\end{equation}\\] In R, the variance is calculate with the function var, HeightOfDog=c(600, 470, 170, 430 , 300) var(HeightOfDog) ## [1] 27130 Hmm, right?. The simplest variance to understand is when variance = 0, which means all values are the same. In reality, however, variance results are normally large numbers that are hard to interpret intuitively. But the variance allows you to calculate the standard deviation, which is actually much easy to comprehend. The standard deviation The standard deviation is calculated as the square root of the variance, and has the same units as the mean. \\[\\begin{equation} Standard \\ deviation= S = \\sqrt {s^2} \\end{equation}\\] In R, the standard deviation is calculated with the function sd, HeightOfDog=c(600, 470, 170, 430 , 300) sd(HeightOfDog) ## [1] 164.7119 You can also try the square root of the variance, sqrt(var(HeightOfDog)) ## [1] 164.7119 Basically, SD= 164mm. Now lets interpret that number. In a normal distribution, 95% of the observations are within 2SD from the mean. So 95% of the dogs in the sample should between 394-(164mm x 2) and 394+(164mm x 2) or between 66mm and 722mm. In the example with the dogs, we have the problem of low sample size. If you recall an earlier chapter, the problem with low samples is that the variability increases as the sample size decreases and that is reflected in the standard deviation (SD). Lets try the standard deviation in a large sample size. y &lt;- rnorm(1000) # the function rnorm allows you to create a random set of numbers from a normal distribution #lets now plot that data using the hist function, you already know how to use: hist(y, main = NULL,breaks = 30,ylim=c(0,120)) SD=sd(y) Mean=mean(y) #plot the mean abline(v=(Mean), col=&quot;blue&quot;,lwd=4) text(&quot;Mean&quot;,x=Mean,y=100, col=&quot;Blue&quot;, pos=4) #add two standard deviations abline(v=(Mean+(SD*2)), col=&quot;red&quot;) text(&quot;+2 SDs&quot;,x=(Mean+(SD*2)),y=100, col=&quot;red&quot;, pos=4) #minus two standard deviations abline(v=(Mean-(SD*2)), col=&quot;red&quot;) text(&quot;-2 SDs&quot;,x=(Mean-(SD*2)),y=100, col=&quot;red&quot;, pos=4) In the figure above, ~95% of the data should be between those two red lines. The coefience of variance The coefficient of variation (CV) is another metric of dispersion. It is the ratio of the standard deviation to the mean. It is commonly given as a percentage. Basically, \\[\\begin{equation} CV = \\frac{Standard \\ deviation}{\\bar{X}} *100 \\end{equation}\\] Higher values indicate that the standard deviation is relatively large compared to the mean. Lets try an example, a pizza restaurant measures its delivery time in minutes. The mean delivery time is 20 minutes and the standard deviation is 5 minutes. Thus, the coefficient of variation is 25% (some people may give it as a fraction, 0.25). This value tells you the relative size of the standard deviation compared to the mean. In this example, the standard deviation is 25% the size of the mean. If the value equals one or 100%, the standard deviation equals the mean. Values less than one indicate that the standard deviation is smaller than the mean (typical), while values greater than one occur when the S.D. is greater than the mean. "],["exercises.html", "Exercises", " Exercises Some of these questions may take a while to load depending on your internet connection. Assign the names to each distribution. o Take the following numbers (8 2 7 2 6), using the R console below estimate the mean, median, and mode, the standard deviation and 80th percentile. Now lets check some equations: "],["homework-2.html", "Homework", " Homework Using Tinn-R or R-Studio, write down a code that: Creates a sample of 1000 points from a normal distribution, and place them in a data.frame, with one column named X. Estimate the mean, median and mode of variable X; have these individual values stored in their own variable names. Estimate the standard deviation and place it in its own variable name. Make a histogram of the data you created in numeral one. Adjust parameters of the figure to the guidelines for good figures. Place the mean, median, mode and 2SDs from the mean as vertical lines. Make the color lines distinctive. Name the vertical lines above. Work with the XY position of the text so the labels look nice. Save the code and email it as a homework. "],["6-correlation.html", "6 Correlation", " 6 Correlation One of the core goals in statistics is to calculate the strength of the relationship between two variables. That goal can be applied to a broad diversity of research questions. Say you did an experiment to test the effect of temperature on plants and say you have plantss growing at different temperatures. In this case, you probably would like to relate temperature to a dependent variable like say plant size. Instead, you may be interested to see how rain affects the common cold, in which case you may wish to relate the amount of daily rainfall and the daily number of hospitalizations by cold. The possibilities are endlessbut at the end of the dayall of them come down to simply assessing the strength of the relationship between two variables. For this specific purpose, we use the correlation and/or the regression analysis. . Expectation for this chapter At the end of this chapter, it is expected that you: Can create a publication quality R scatterplot to visualize the relationship between two variables. Define the type of relationship between two variables (e.g., positive, negative, non-linear, non-existent). Calculate the coefficient of correlation between two variables and understand what it tells you about the relationship between the two variables. Alright, a lot to cover, so lets get started. "],["visualization-of-relationships.html", "Visualization of relationships", " Visualization of relationships When looking into correlations and relationships the main display tool is the scatterplot. You already know how to use the scatterplot from Chapter 4. From that chapter, you should also already know the criteria for a plot to meet the standards required for publication. For the purpose of displaying a correlation and/or regression between two variables, the key consideration is if you assume one variable influences the other. From Chapter 1, the section on Experiments, you learned that the variable that influences another variable is called the independent variable. The one variable that is affected is called the dependent variable. If the purpose of your study does not involve assessing if one variable influences the other, then it does not matter what variable you use for the X- or Y-axis. However, if your study assumes that one variable influences another one, the independent variable is located in the X-axis, while the dependent variable should be located in the Y-axis. Lets get to work in R. Lets work on an interesting relationship I sow in the New York Times, between the people that voted for Donald Trump by State and the degree of higher education at those States. First, download the two databases from Here and Here. Next, load the data into R (remember Chapter 3, section about loading your own data). TrumpVoters_by_State &lt;- read.csv(&quot;D:/GEO380/Datasets/TrumpVoters.csv&quot;) #Fraction of people by State that voted for Trump HigherEducation_by_State &lt;- read.csv(&quot;D:/GEO380/Datasets/US-Pop-HigherEducation.csv&quot;) #Fraction of people by State that have higher education degrees Next check the data were loaded correctly and review the structure of the data: head (TrumpVoters_by_State) ## State TRUMPVoteAsFraction ## 1 Alabama 0.6208 ## 2 Alaska 0.5128 ## 3 Arizona 0.4867 ## 4 Arkansas 0.6057 ## 5 California 0.3162 ## 6 Colorado 0.4325 Review the same for the second database: head (HigherEducation_by_State) ## State BachelorDegreePerStateAsFraction ## 1 Alabama 0.087 ## 2 Alaska 0.101 ## 3 Arizona 0.102 ## 4 Arkansas 0.075 ## 5 California 0.116 ## 6 Colorado 0.140 The data appear to have loaded correctly. But I have the data I want in two different data.frames, so I have to merge them. In this case, I have one variable in common to the two data.frames that I can use to merge them by; that is the variable State. So, lets merge our two data.frames by State. p i Data=merge(TrumpVoters_by_State,HigherEducation_by_State,by=&quot;State&quot;) When merging databases, things can get tricky as you need to have at least one column in common to merge by. You need to ensure that in both databases, each field uses the same names for the data. For instance, say you have a common column call state, but in one database the data are shown by full names but in the other by abbreviate name. In this case, the merge function will return an empty database, because the two databases do not have variables that can be matched. In such cases, you need to modify the original databases to ensure the two data.frames share a common variable, with similary named data. Lets review the new merged database: u head (Data) ## State TRUMPVoteAsFraction BachelorDegreePerStateAsFraction ## 1 Alabama 0.6208 0.087 ## 2 Alaska 0.5128 0.101 ## 3 Arizona 0.4867 0.102 ## 4 Arkansas 0.6057 0.075 ## 5 California 0.3162 0.116 ## 6 Colorado 0.4325 0.140 Ok, now lets plot the data. In this case, I think the fraction of people that voted for Trump could be function of how educated they were, not the other way around; that would be like saying that Trump affected the degree of education of each State.hmmwhen I say like that is does not sound that unlikely, ah?. This distinction is important because you need to determine which variable goes in the Y axis, and which one on the X-axis. If I think, the votes for Trump were influenced by the level of their education, them level of education will be the independent variable and then it will be located in the X-Axis. In this reasoning, The percent of the States population that voted for trump will be the dependent variable, and so, it should be located on the Y-axis. Lets do that plot: plot(Data$TRUMPVoteAsFraction~Data$BachelorDegreePerStateAsFraction, ylab=&quot;Trump voters by State (Fraction)&quot;,xlab=&quot;High degree education (Fraction of population)&quot;) Hmm, from this visualization alone you can tell something is cooking herethe least educated states voted for Trump more so than States with more educated populations. Lets explore this relationship in more detail, as a case example. "],["linear-relationships.html", "Linear relationships", " Linear relationships Generally speaking two variables can related in three difference fashions: linear, non-linear, or non-related. There are different mathematical approaches to tackle each type of relationship. Here we will cover only linear relationships, but I want you to learn to at least identify the other types. Lets start with the linear relationships. Linear relationships, as the name sort of indicates are better described by a straight line. This type of relationships can be further separated between positive relationships, in which Y-increases as X-increases and negative relationships, in which Y-decreases as X-increases. Obviously, you also have the option that the two variables do not relate to each other (see figure below) Figure 6.1: Types of relationships "],["non-linear-relationships.html", "Non-linear relationships", " Non-linear relationships Relationships that cannot be well described with a linear model, are called non-linear relationships. Non-linear relationships can take all sorts of shapes, names and mathematical approaches to define them. They will not be covered as part of this class, but you should be aware they exist. Figure 6.2: Types of relationships "],["the-covariance.html", "The Covariance", " The Covariance The strength of the linear association between two variables is mathematically measured with the so-call Correlation Coefficient. The Correlation Coefficient is abbreviated with the lowercase letter \\(r\\) (that is not a token). However, to estimate the Correlation Coefficient, you need to first estimate the so-call Covariance. Check a brief explanation of the covariance in the following video: The covariance is an extension of the variance calculation we did earlier to measure the spread of the data in a variable, but in the covariance we analyze two variables. In fact, if you were to assess the relationship between a variable and itself, the covariance will be identical to the variance. In a nutshell, the covariance tells you if the differences in two variables are trending on the same direction. Mathematically, the covariance is calculated with the following equation: \\[\\begin{equation} Covariance = COV(XY) = \\frac{\\sum_{i=1}^n (x -\\bar{x})*(y -\\bar{y})}{n-1} \\end{equation}\\] Lets try a simple example to estimate the Covariance. Lets consider the relationship that exist between the time that you study for my class and the grade that you get. Figure 6.3: Time stuying relates to test scores Say, I asked five students how long they studied each week and the grade they got in my prior classes. These were the data: Table 6.1: Grades and time studying Stats Names Hours_Studying Grade Peter 0.5 55 Laura 1.8 64 John 2.4 75 Chip 3.8 82 Tom 4.5 95 As always, we start by plotting the data: StudyingTimes= data.frame( Names=c(&quot;Peter&quot;,&quot;Laura&quot;, &quot;John&quot;, &quot;Chip&quot;, &quot;Tom&quot;), #lets create a data.frame with three columns Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5), Score=c(55, 64, 75, 82,95)) #now let&#39;s do the plot plot(Score~Hours_Studying,data=StudyingTimes,xlab=&quot;Hours a week studying&quot;, ylab=&quot;Final class score (%)&quot;) Lets break the calculation of the covariance into its parts so we can better appreciate what it does. First, we calculate the mean of alll values in X and the difference from each value to that mean: Figure 6.4: Differences in X Lets do the same for the Y-axis: Figure 6.5: Difference in Y Following the equation of the covariance, for the first point in the data (i.e., Peter), we place his difference to the mean in X (i.e, -2.1) and its difference to the mean in Y (-19.2), in the numerator. Like this: Figure 6.6: Difference in X We can do that for all data points and obtain: \\[\\begin{equation} Covariance = COV(XY) =\\frac{\\sum_{} (-2.1)(-19.2) + (-0.8)(-10.2) + (-0.2)(0.8) + (1.2)(7.8) + (1.9)(20.8)}{5-1} \\end{equation}\\] . \\[\\begin{equation} Covariance = COV(XY) =24.3 Hours*Test Score \\end{equation}\\] Hmm???, Right?as mentioned earlier the score of the covariance by itself is hard to interpret, but it may still provide useful information about the trend of the datain this case the covariance is positive, indicating that the differences in X trend in a positive direction as the differences in Y. Basically, as students study more they get higher gradesPlease remember that!and here it goes a token j In R, the covariance is calculated with the cov function: cov(StudyingTimes$Score,StudyingTimes$Hours_Studying, use = &quot;everything&quot;, method = &quot;pearson&quot;) ## [1] 24.3 "],["the-correlation-coefficient-r.html", "The Correlation Coefficient, r", " The Correlation Coefficient, r As indicated earlier, the strength of the linear association between two variables is mathematically measured with the so-call Correlation Coefficient. At times, it is also called the Pearson product-moment correlation coefficient, after Karl Pearson, who is credited with formulating r. Mathematically, the correlation coefficient, \\(r\\), is calculated with the following equation: \\[\\begin{equation} r = \\frac{cov (XY)}{Sx * Sy} \\end{equation}\\] Basically, the correlation coefficient, \\(r\\),is the Covariance divided by the multiplication of the standard deviation of the data in X and the standard deviation of the data in Y. If you think about this equation, the covariance is the product of the differences in X and Y. While the standard deviations are independently the differences in X and the differences in Y. So, in practical terms, the correlation coefficient is an standardized metric. It will never be smaller than -1 or larger than 1. That is why the correlation coefficient is such a nice term to access the tendency between two variables. If it is closer to -1 then you know the data probably follow a strong and negative trend. If it is close to 1, then the data follow a positive and strong trend. If it is closer to zero, then the data are all over the place (there is not correlation). Towards the end of the chapter, we will learn more about how to interpret the correlation coefficient. Lets calculate it, COVXY= cov(StudyingTimes$Score,StudyingTimes$Hours_Studying, use = &quot;everything&quot;, method = &quot;pearson&quot;) #Covariance SDX= sd(StudyingTimes$Hours_Studying) #Standard deviation for X SDY= sd(StudyingTimes$Score) #Standard deviation for Y r=COVXY/(SDX*SDY) r ## [1] 0.9817004 So the coefficient of correlation between the time that you study and the score in my class is 0.98. That means the more time you study the higher your gradenice, ah??? In R, the coefficient of correlation can be calculated directly with the function cor, cor(StudyingTimes$Score,StudyingTimes$Hours_Studying, method = &quot;pearson&quot;) ## [1] 0.9817004 Alternative formulation While looking into the correlation coefficient you will likely see alternative formulations of it that yield the same or very close approximations. For instance, you may find it formulated like this: \\[\\begin{equation} r = \\frac{1}{n-1} \\sum_{}\\frac{x-\\bar{x}}{Sx}\\frac{y-\\bar{y}}{Sy} \\end{equation}\\] This equation above, is pretty much the same we used earlier, but reorganizing the parts. At times, you can also defined as: \\[\\begin{equation} r = \\frac{n * \\sum_{} xy- (\\sum_{} x)*(\\sum_{} y)}{\\sqrt {n * \\sum_{} x^2- (\\sum_{} x)^2 } * \\sqrt {n * \\sum_{} y^2- (\\sum_{} y)^2 }} \\end{equation}\\] Which will yield a very close approximation to the equation we used earlier. Be careful! The notation \\(\\sum_{}x^2\\) means first square \\(x\\) and then calculate the sum, whereas \\((\\sum_{}x)^2\\) means first sum the x values and then square the result. For the equation above, all we have to compute is \\(\\sum_{}x\\), \\(\\sum_{}y\\), \\(\\sum_{}x^2\\), \\(\\sum_{}y^2\\), and \\(\\sum_{}x*y\\). Let try, for the sake of being sure and for you to use some tools from R. Y=StudyingTimes$Score X=StudyingTimes$Hours_Studying SumX= sum(X) #sum all values of x SumY= sum(Y) #sum all values of y SumX2=sum (X^2) #sum all values of x^2.. SumY2=sum (Y^2) # sprintf(&quot;%.0f&quot;,sum (Y **2)) SumXY=sum (X*Y) #sum all value of x * y n=length(Y) # the number of observations is basically the number of rows in the database The results are: \\(\\sum_{}x\\) = 13 \\(\\sum_{}y\\) = 371 \\(\\sum_{}x^2\\) = 43.94 \\(\\sum_{}y^2\\) = 28495 \\(\\sum_{}x*y\\) = 1061.8 n = 5 Now we plug those values into the coefficient of correlation, r, equation: \\[\\\\[.0005in]\\] \\[\\begin{equation} r = \\frac{n * \\sum_{} xy- (\\sum_{} x)*(\\sum_{} y)}{\\sqrt {n * \\sum_{} x^2- (\\sum_{} x)^2 } * \\sqrt {n * \\sum_{} y^2- (\\sum_{} y)^2 }} \\end{equation}\\] \\[\\\\[.0005in]\\] \\[\\begin{equation} r = \\frac{5 * 1061.8- (13)*(371)}{\\sqrt {5 * 43.94- (13)^2 } * \\sqrt {5 * 28495- (371)^2 }} \\end{equation}\\] \\[\\\\[.0005in]\\] In R, it is basically: r=(n*SumXY-(SumX*SumY)) / ( sqrt(n*SumX2 -SumX^2) * sqrt(n*SumY2 -SumY^2) ) r ## [1] 0.9817004 \\[\\begin{equation} r = 0.98 \\end{equation}\\] \\[\\\\[.0005in]\\] Hmm, what do you think is causing the difference to the original calculation? "],["interpreting-r.html", "Interpreting r", " Interpreting r The correlation coefficient, \\(r\\), is unitlessby dividing by the standard deviations the products of the differences in x and y, the result loose its units. This means the value can be compared among studies for any set of x and y variables but can also be interpreted between a set of standard values (i.e., -1 to 1). In short, the correlation coefficient, \\(r\\), can range from -1 (perfectly negative correlation) to 1 (perfectly positive correlation). Like in the images below. Figure 6.7: Examples of correlation coefficients "],["causation.html", "Causation", " Causation The correlation coefficient measures the strength of a linear relationship between two variables. Thus, it makes no implication about cause or effect. The fact that two variables tend to increase or decrease together does not mean that a change in one is causing a change in the other. At times, two variables may be strongly correlated because they are equally correlated to a third (either known or unknown) variable. Such variables are called lurking variables. Lets take the following example: Figure 6.8: Examples of lurking variables In the figure above, the dashed lines show an association. The solid red arrows show a cause-and-effect link. The variable x is explanatory, y is a response variable, and z is a lurking variable. Basically, in example B, you will find a strong correlation between x and y, not because they are causality related, but because they are both strongly affected by a variable you did not measure (i.e., Z in the case example above) or a so-call lurking variable. The effect is that you can make a spurious conclusion if you interpret the coefficient correlation as a cause and effect. For instance, you know that in tropical countries there is a strong correlation between the consumption of ice cream and shark attacks?. One could conclude that sharks like sweet people?. Hmmthink about this correlation.is there a lurking variable here?. what could it be? Figure 6.9: Example of a lurking variable "],["significance.html", "Significance", " Significance If you were to take any two random variables and correlate them together, you will still get a correlation value. You may think that if the two variables are random, then the correlation will be close to zerowell, you are wrong. It turns out that even by random chance alone, two variables may still be correlated. The chances of getting a higher correlation increase the lower the sample size. Just think about it, if you were to correlate any two data points, almost certainly your correlation will be 1 or -1. To address this potential caveat, we need to assess the Significance of the correlation. Basically, can the correlation you found emerge by chance alone? That word significance or significant is tricky because it implies a level or threshold of error you are willing to take. That threshold is called critical value, commonly identified with the letter alpha, \\(\\alpha\\). Again it is the margin of error you are willing to accept as error. In biology, we normally give ourselves a 5% chance of being wrong (p&lt;0.05). But at times, it is preferable to be more certain and we take only a 1% chance (p&lt;0.01). At times rather, you just provide the exact probability of the correlation being random, in which case we provide the exact p-value. Remember that the parameter \\(n\\) is call sample size. correlation tables For the case of the correlation, the significance of the correlation can be assessed quickly with a probability table, as the one shown below: Figure 6.10: Significance of correlations Lets use the example we have been working on about the relationship between the time students study and their grades. In that case, \\(r\\)=0.98 and the sample size \\(n\\) was 5 students. Is that coefficient of correlation significant?. Can I obtain a similarly high correlation, ifor any similar set of random variables?. To find out, you first select your critical value, \\(\\alpha\\), lets choose 0.05. In this case, the second column in the table above will be the one we are interested on. Next, you scroll-down column one until the sample size in the table matches ours (that is 5). At that row, if you look over the value in the second column, that is the coefficient of correlation of any two random variables with five points. In our case, that value is 0.88. Our correlation coefficients was larger, \\(r\\)=0.98, meaning that it is unlikely our correlation could occur by chance alone. There you have it: the more time you study the better grades you get, now demonstrated mathematically. Take a couple tokens, gh, so you do not forget that conclusion! Correlation p-values At times, you want to be more specific about the likely error that the correlation you found can be caused by chance alone. In this case, we need to report an exact p-value. Later on we will be studying p-values in more detail. In R, the exact p-value of a correlation is calculated with the function cor.test. Y=StudyingTimes$Score X=StudyingTimes$Hours_Studying cor.test (X,Y) ## ## Pearson&#39;s product-moment correlation ## ## data: X and Y ## t = 8.9289, df = 3, p-value = 0.002963 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7427169 0.9988455 ## sample estimates: ## cor ## 0.9817004 There are several things to the output of that R-function, cor.test, but for now, you just need to focus on the p-value number, which in our case, for the correlation between time studying and grades, is p-value=0.002963. Basically, given our sample size, there is a 0.3%, less than 1% chance, that our correlation can emerge by chance alone. In conclusion, if you want a good grade in my class you really need to take good time to study. Mathematics tells you that there is no way around it; we are nearly 99.9% that is a legit correlation!. "],["exercises-1.html", "Exercises", " Exercises Some of these questions may take a while to load depending on your internet connection. "],["homework-3.html", "Homework", " Homework When studying crime levels among universities around the country, a scientists found the following data x= 12.5, 30.0, 24.5, 14.3, 7.5, 27.7, 16.2, 20.1 y= 26, 73, 39, 23, 15, 30, 15, 25 Let x be students enroll (in thousands), and y the number of burglaries at the given university campus. Make a scatter diagram, adjust aesthetics so the figure follows publication guidelines. Display in the plot the correlation coefficient. Would you say the correlation is low, moderate, or high? positive or negative? Was it significant? Place the figure and your responses in a Word document and email it as a homework. Send a nice looking document. "],["7-regression.html", "7 Regression", " 7 Regression While the goal of correlation is to give you a metric of the strength of the relationship between two variables; the goal of regression is to give you a numerical representation of the tendency in your data. That numerical representation is called a model that describes a line that best represent the tendency in your two variables. At times, that line can also be called a regression line or a trend line. Here we will be working on linear regression modelsthis means to find an straight line to best represent the relationship between two variables. For instance, the blue line below appears to represent well the tendency that exist between variables Y and X. The goal of this chapter is to use a mathematical procedure (linear regression) that allows you to identify that line. As indicated in the prior chapter, all relationships do not have to be linear, there are also those so-call non-linear relationships. For now, we will cover only linear relationships, or straight lines that best summarize the relationship between two variables. There are a few cool benefits to having a single line describing the data: It allows you to mathematically define how two variables are related. Like if I increase X by 2 then Y increases by how much?. It will allow you to make predictions over areas in which you have no data. Like if X is 52.3 then Y is? You do not need to carry/display the data every time you want to work with the database. To define that best-fitting line, we will use what is called the least squares method; sounds scary, but it is actually very simple. \\[\\\\[.0005in]\\] Expectation for this chapter At the end of this chapter, it is expected that you: Can estimate a linear regression model that defines the linear relationship between two variables using the least-squares criterion. Be able to interpret the results from a linear model. Use the equation of the regression model to predict Y-values (interpolation and extrapolation). Calculate the coefficient of determination and understand what it tells you about the relationship between two variables. Alright, lets get to it. "],["parts-of-a-line.html", "Parts of a line", " Parts of a line Before we get into the needy-greedy of linear regression models, we should start by knowing how a line can be constructed. In practical terms, an specific straight line, in a XY space, can be drawn by knowing just two parameters: the intercept and the slope. Figure 7.1: The slope and the intercept The intercept is basically the position in the Y-axis where the regression line crosses, or the value of Y when X=0. The slope is the inclination of that line, or the change in Y divided by X. It also indicates the mount by which Y changes for unit of change in X; more on this later. With the slope and the intercept you can draw any line you like. At times, you will see the intercept being referred with the lowercase letter \\(a\\) or \\(b\\) or \\(b1\\), whereas the slope is at times named with the lowercase \\(m\\) or \\(b2\\) (those are not tokens). Lets check what the intercept and the slope do to a line. For plotting a line in R, we use the abline function, which we used before for drawing horizontal or vertical lines. You can also use that R-function to draw lines with specific slopes and intercepts. Lets try. Lets plot three lines with different slopes, but the same intercept. #play around with the lines of code below to see how they work #First lets create an empty plot and make the x and y axis nicer. plot(0, 0,xlab=&quot;X&quot;,ylab=&quot;Y&quot;, xlim=c(-4,4),ylim=c(-4,4.5),col=&quot;grey&quot;) abline(h=0,lwd=2, col=&quot;grey&quot;,lty=1) abline(v=0,lwd=2, col=&quot;grey&quot;,lty=1) # lets draw a first line with an intercept of 1, and a slope of 0.5. # In abline, the first number will be the intercept and the second will be the slope. Intercept=1 #lets create a variable for the intercept, which i choose to call intercept and make that variable = 1 Slope=0.5 #lets create a variable for the slope, which i choose to call slope and make that variable = 0.5 abline(Intercept, Slope, col=&quot;orange&quot;) #lets plot the line, with my slope and intercept and make it orange to differentiate it # now, draw another line keeping the intercept the same and increase the slope to 1.5 Slope=1.5 abline(Intercept, Slope, col=&quot;blue&quot;) #lets make it blue to differentiate it # try keeping the intercept the same and increase the slope to 2.5 Slope=2.5 abline(Intercept, Slope, col=&quot;green&quot;) #lets make it green to differentiate it #We can verify the intercept, by plotting the point in Y, where the line intercept. points(0,Intercept, pch=&quot;*&quot;,cex=3, col=&quot;red&quot;) #remember the intercept is when x=0. Lets now change the intercept, but keep the slope the same #play around with the parameters in the code below to see how they work #plot first plot(0, 0,xlab=&quot;X&quot;,ylab=&quot;Y&quot;, xlim=c(-4,4),ylim=c(-4,4.5),col=&quot;grey&quot;) abline(h=0,lwd=2, col=&quot;grey&quot;,lty=1) abline(v=0,lwd=2, col=&quot;grey&quot;,lty=1) # now try a combination of difference intercepts but the same slope. Intercept=1 Slope=0.5 abline(Intercept, Slope, col=&quot;orange&quot;) #lets plot the line, and make it orange to differentiate it # keep the slope the same and increase the Intercept to 2 Intercept=2 abline(Intercept, Slope, col=&quot;blue&quot;) #lets make it blue to differentiate it # keep the slope the same and increase the Intercept to 3 Intercept=3 abline(Intercept, Slope, col=&quot;green&quot;) #lets make it green to differentiate it How to interpret the slope The slope should be interpreted as the amount of change in Y for a single unit of change in X. Lets say, you are told that a given relationship between total rainfall (in litters) and the amount of time (in hours) is 500. From that slope alone, we can tell now that for each extra hour, there will be 500 litters of water falling. The units of the slope are the units of the variable in Y divided by the units of the varaibles in X. Say the units of Y are in litters and the units of X are in hours, so the slope will be in litters/hours. To contextualize the slope graphically, you can take any point along the trendline, and move horizontally one unit; the difference between the Y-point at the first X and the Y-point at X+1, is your slopeLets check. #plot first plot(0, 0,xlab=&quot;X&quot;,ylab=&quot;Y&quot;, xlim=c(0,10),ylim=c(0,5000),col=&quot;grey&quot;) abline(h=0,lwd=2, col=&quot;grey&quot;,lty=1) abline(v=0,lwd=2, col=&quot;grey&quot;,lty=1) #lets draw a trend line with a slope of 500, that intercepts the origin. Intercept=0 Slope=500 abline(Intercept, Slope, col=&quot;orange&quot;) #lets plot the trend line, and make it orange to differentiate it #lets move one unit along X, starting at zero segments (0,0,1,0, col=&quot;red&quot;,lwd=3) #lets make that segment red #from that point, X+1, lets move up 500 units, if the calculation is correct, then that segment of 500 units in Y should finish at the interception with the trend line...lets see segments (1,0,1,500, col=&quot;blue&quot;,lwd=3) #here I draw a segment starting a x=1, y=0...until x=1, y=500 The slope represents the inclination of the regression line and reflects the amount of change in Y for a single unit of change in X. How to interpret the intercept The intercept is the expected value of Y when X=0. Say that among children there is a relationship between age (in years) and size (in centimeters), and the Y-intercept is 35cm. What does that tell you?. So since the Y-intercept is the value at which X=0, then a Y-intercept of 35cm means the size of a child when he is zero years of age. Basically, children on average are born at 35cm of size. The intercept represents the value of Y, when X= 0. The units of the intercept are the same units of the varaible in Y. "],["purpuse-of-the-regression-line.html", "Purpuse of the regression line", " Purpuse of the regression line As indicated earlier, a common name for the intercept is the lowercase letter \\(b\\) and a common name for the slope is the lowercase letter \\(m\\), in the regression model they come together as: \\[\\begin{equation} Y = mX + b \\end{equation}\\] The beauty of the regression model above is that by knowing \\(m\\), and \\(b\\), you can predict any value of Y, if you know X. Say there is a relationship between years of higher education and salary, which is well defined with a regression equation with an intercept of $25,000 and a slope of $10,000 dollars/year. Given the units given, you should predict that salary is in the Y-axis and years of education in the X-axis. I can turn that into the equation, like: \\[\\begin{equation} Salary = 10,000 * \\text{(Years Of Education)} + 25,000 \\end{equation}\\] I can also display, that line in an XY plot, using the abline function: #plot first plot(0, 0,xlab=&quot;Years of education&quot;,ylab=&quot;Salary (in US dollars)&quot;, xlim=c(0,10),ylim=c(0,150000),col=&quot;grey&quot;) abline(h=0,lwd=2, col=&quot;grey&quot;,lty=1) abline(v=0,lwd=2, col=&quot;grey&quot;,lty=1) #lets draw a trend line with the given parameters Intercept=25000 Slope=10000 abline(Intercept, Slope, col=&quot;orange&quot;) #lets plot the trend line, and make it orange to differentiate it You can also ask questions, like what will be an average salary for a person that studies 4 years of higher education?..You simply replace the \\(x\\) variable in the equation with the number 4 and do the mathematical calculation to get the average expected salary. \\[\\begin{equation} Salary = 10,000 * 4 + 25,000 \\end{equation}\\] So the expected salary of a person that studies four years of higher education is $65.000. Ok, at this moment you know the basics of a linear regression model. In a nutshell, a linear regression model is a mathematical equation that includes a slope and an intercept which allows you to draw a line, from which you can also predict any value of Y given values of X. "],["the-least-squares-line.html", "The least-squares line", " The least-squares line By now, you know the formulation (i.e., \\(Y= mX + b\\)) and the general purpose (i.e., to predict Y, given values of X) of a linear regression model. The next task in this chapter is to figure out how to get the best line that can be drawn through two variables that we want to relate. Obviously, you can draw an infinite number of straight lines trhough a set of datapoints; then a key question is which line best describe the data?. That is the goal of the linear regression model: to draw a best-fit line through the datapoints. The best fitting line will be a line that minimizes the distance from each point to that line. In mathematical terms, that line is called the least-squares regression line. In a moment, you will see how that name (least-squares regression line) speaks by itself; I hope in a moment you will see how the term is self-explanatory. Understanding the least-squares line Lets use an analogy, and imagine the best fitting line as a knife that cut a cake. Take the figure below as an example. That blue line would be the knife and the red dotted lines the pieces of cake for each person. Will this be a good cut? Will that be the best-fitting line? Not quite s. I can imagine Laura not being happy about getting a smaller piece of cake. She may complain but there are four guys that will not support her, as they are likely very happy getting bigger shares of that cake. So in this case, this line is not the best describing the data. What about the line below?. Will that be a good-fitting line to the data? Well, may beyou may think the complains by Laura and Peter getting smaller pieces could be balanced out by the extra happiness of Chip and John getting larger shares of the cake. However, there are some justified complains that can be avoided if we better cut that cake; a cut in which we can reduce the complains by every body. That can only be done drawing a line that minimizes the distance from each point to the line, and that is the so-called least-square regression line. Like in the image below: Deciphering the least-squares line But of all lines that we can draw through a set of data points, how can we know what is the best-fit line?, the one in which everybody is happy using the cake analogy?. There are actually different ways to get to that linelets start by using brutal force, which may actually help us understand the idea behind the least-squares. We know for a fact that the best-fitting line has to pass by the XY coordinates defined by the mean of all values in X and the mean value of all values in Y. At that inflection point, all data in X and all data in Y are evenly separated, so the best fitting-line has to pass by that given point. Lets plot the mean value of X and the mean value of Y (Dashed grey-lines in the image below) and at the interception of those lines lets we plot the mean of X and the mean of Y (red dot in image below). Next, we draw a very inclined line passing trough that inflection point defined by the coordinates Mean-X and Mean-Y. Like in the image below. Next, we measure the distance from each point to that line (red-dashed lines), which I indicate with the red numbers in the image below. (Remember, the best line will be one with the smallest distance between each point and the given line) That difference between each point and the best-fitting line (red-dashed lines and red-numbers) are called residuals. It may also be named residual errors. Why call the residual also an error?. well, in the case of the linear regression, we want a model that best describes the data. Unfortunately, the best-line does not pass by every single one of the points, so that difference between the each point and the line is the error in the model. Next, we add up all residual errors. Remember, the best fitting line will be that one with the least residual error: \\((2.124)\\) + \\((-0.036)\\) + \\((0.614)\\) + \\((-0.276)\\) + \\((-2.426)\\) = \\(0\\) Hmm, that does no make sense, the sum is 0; yet we know that for Laura alone, the residual error is \\((2.124)\\). So how can you add up positive and negative errors, so they do not cancel each other out?. You should know. I hope you say by squaring pr elevating the given number to the power of 2. If you recall from the last chapter, the approach of squaring any value allows to convert all values positive or negative to non-negative valueslets try. \\((2.124)^2\\) + \\((-0.036)^2\\) + \\((0.614)^2\\) + \\((-0.276)^2\\) + \\((-2.426)^2\\) =\\(10.8513\\) Ok, that is more like it. That value that we just calculated is call the Sum of Square Errors or SSE. The best fitting line, will be that one in which SSE is the smallest. There why the approach to finding out the best finding line is called the least squares. Lets finish the exercise by brutal force, drawing lines with different inclinations and estimating their SSE. Like the figure below. Figure 7.2: Finiding the line with the least-squares error We can compare the sum of squares errors, SSE, of each line to find out the one with the least, like this: And it seems we have a winner, our line number 30 was the one with the lowest sum of squares, which I separate below: Ok, I hope is clear then what it is the least squares approach to find the best fitting line. Next, we will learn how to estimate the parameters for the intercept, \\(b\\), and the slope, \\(m\\), that define the line in the linear regression model, the smart and easy way. "],["estimating-the-least-squares-line.html", "Estimating the least-squares line", " Estimating the least-squares line The ultimate goal of a linear regression model is to identify the parameters for the intercept, \\(b\\), and the slope, \\(m\\), of the line that minimizes the sum of square errors also called least-square errors. The linear regression line is expresed as \\(Y = mX + b\\) Refreshing the Slope Before we get into the mathematical equation to describe the slope of the linear regression model, lets review your 4th grade geometry about how to calculate the slope between two points. If you recall Ms. Smith, your Match teacher in 4th grade, she told you that the slope between two points can be calculated as: \\[\\begin{equation} Slope = m = \\frac{\\Delta y}{\\Delta x}= \\frac{y_{2}-y_{1}}{x_{2}-x_{1}} \\end{equation}\\] Basically, the change in Y divided by the change in X. Put another way, if I change X by 1 unit, how much will Y change? Lets check the math, using a line for which we know the slope, using the R-function abline, as we did earlier. Slope= 2 #lets set a line with a slope of 2 Intercept=1 # We do not need the intercept but lets use a value of 1 as an example plot(0,0,xlab=&quot;X&quot;,ylab=&quot;Y&quot;, col=&quot;blue&quot;,pch=&quot;.&quot;, cex=2, xlim=c(-0,4),ylim=c(0,8), yaxs=&quot;i&quot;, xaxs=&quot;i&quot;) #lets create an empty plot #Next we draw the line with the known slope: abline(Intercept, Slope ,col=&quot;blue&quot;,lwd=1) #Now place two points along that line...say a point at the coordinates (1,3) and another point at the coordinates (3,8). #If you recall, the coordinates of a point are x and y given between parenthesis. # draw Point 1 in the plot and put a label to it. points(1,3,pch=21, col=&quot;black&quot;,bg=&quot;yellow&quot;,cex=2,lwd=.1) #First point text(1,3, labels=&quot;(1,3)&quot;,pos=2) #lets create a label # do the same for Point 2 points(3,7,pch=21, col=&quot;black&quot;,bg=&quot;yellow&quot;,cex=2,lwd=.1) #second point text(3,7, labels=&quot;(3,7)&quot;,pos=2) #lets create a label #lets draw a segment for the change X: is x0=1, and x1=3, then the difference is 2. segments(x0=1,y0=3,x=3,y=3, col=&quot;red&quot;, lty=2,cex=2) #lets draw the segment for the difference in x between the two points, and make it red text(2,3, labels=&quot;2&quot;,col=&quot;red&quot;,pos=1) #lets create a label for that segment #lets draw the difference in Y, which basically y0=3, and y=7, so the difference, delta, is 4. segments(x0=3,y0=3,x=3,y=7, col=&quot;blue&quot;, lty=2,cex=2) #lets draw the segment for the difference in y between the two points text(3,5, labels=&quot;4&quot;,col=&quot;blue&quot;,pos=4) #lets create a label Continuing with the example above, the change in Y, also called \\(\\Delta y\\), was 4. The change in X, also called \\(\\Delta x\\), was 2. So, the slope can be calculated as: \\[\\begin{equation} m = \\frac{\\Delta y}{\\Delta x}= \\frac{4}{2}=2 \\end{equation}\\] That is exactly, the slope we set in abline, which serve the purpose to illustrate that the slope of a line is simply the change in Y divided by the change in X. With that in mind lets now calculate the slope of the regression line "],["the-slope.html", "The slope", " The slope Why not to use Ms. Smith way to calculate the slope for the regression model?. Well, we use the same principle, difference in Y divided difference in X, but we cannot use the same formula because in the case of a regression model we have more than two points. There are numerous ways to calculate the slope, \\(m\\), of a linear regression model. However, the simplest is: \\[\\begin{equation} Slope = m = \\frac{\\sum(x-\\bar{x})*(y-\\bar{y})}{\\sum(x-\\bar{x})^2} \\end{equation}\\] We have seen those terms before. The numerator was included in the covariance (i.e., how two variables trend together) and the denominator was included in the variance (i.e., how disperse are the data in one variable). If you think about itthat equation speaks by itself. As we mentioned earlier, the slope of any line can be described as the change in Y divided by the change in X. From Chapter 5, the Section on Dispersion, you may recall that the best indicator of the variability in a variable was the variance, which has as term \\(\\sum(x-\\bar{x})^2\\). From chapter 6, you may recall that the best indicator of the tendency between two variables was the covariance \\(\\sum(x-\\bar{x})*(y-\\bar{y})\\). As such, we if want the slope among a set of points that follows their central tendency, then the change in X will be \\(\\sum(x-\\bar{x})^2\\), and the change in Y will be how Y varies with X, which mathematically is \\(\\sum(x-\\bar{x})*(y-\\bar{y})\\). You can phrase the equation for the slope in the regression line in a different way, if X changes by the variance of X, then Y will change by the covariance of Y and X. Ok, now that the equation for the slope is clear, lets calculate it. Lets use the data we have been using on the time studying and grades, Table 7.1: Grades and time studying Stats Names Hours Studying Grade \\[(x-\\bar{x})\\] \\[(y-\\bar{y})\\] Peter 0.5 55 -2.1 -19.2 Laura 1.8 64 -0.8 -10.2 John 2.4 75 -0.2 0.8 Chip 3.8 82 1.2 7.8 Tom 4.5 95 1.9 20.8 a a So, all we have to do is to replace the differences in X and the difference in Y in the slope formula: \\[\\begin{equation} slope = m = \\frac{(-2.1*-19.2) + (-0.8*-10.2) + (-0.2*0.8) + (1.2*7.8) +(1.9*20.8)} { (-2.1)^2 + (-0.8)^2 + (-0.2)^2 + (1.2)^2 +(1.9)^2} \\end{equation}\\] \\[\\begin{equation} slope = m = 9.6 \\end{equation}\\] So the slope, m, is equal to 9.59. The units will be the units in Y (i.e., grade), divided by the units of x (i.e., hours studying). So if the unit of change in X in one hour then the unit of change in Y will be 9.59 points higher in grade. Put another way, for each extra hour that you study a week, you can expect a 9.59 points higher in your grade, neat or what?. I should mention that there as several other ways to calculate the slope of the linear regression model, but I find them a bit more complicated and difficult understand. I prefer to use the simple formula above, but be aware there are a few other ways to get to the slope of the least-squares line. "],["the-intercept.html", "The intercept", " The intercept There are numerous combulated equations to calculate the intercept of the linear regression model. Here I want us to use a procedure that you probably learned in 6th grade. If you look at the linear regression model equation, \\[\\begin{equation} Y = mX + b \\end{equation}\\] You could isolate the intercept, \\(b\\), as: \\[\\begin{equation} b = Y - mX \\end{equation}\\] If you recall, the line of the least-squares regression line has to pass by the coordinates defined by the mean value of X and the mean value of X. So to estimate the intercept all you have to do is to replace the \\(X\\) and \\(Y\\) parameters of the linear regression equation by the mean of X, \\(\\bar{x}\\), and the mean of Y, \\(\\bar{y}\\). Like this: \\[\\begin{equation} b = \\bar{y} - m\\bar{x} \\end{equation}\\] Because you also know the slope from the prior section. Then you have all parameters of the equation above to stimate the intercept. Lets estimate the intercept for the relationship between time studying and grades. From that data, the mean of X, \\(\\bar{x}\\), was 2.6, and the mean of Y, \\(\\bar{y}\\), was 74.2. So, the intercept for that linear regression is: \\[\\begin{equation} Intercept = b =74.2 - 2.6*9.586 \\end{equation}\\] \\[\\begin{equation} Intercept = b =49.277 \\end{equation}\\] So the Y-Intercept of the least-square line for the relationship between grades and time studying is 49.28. You can interpret that value as the grade that you can expect in my class if you do not study a single hour of the week. Hmm, not good. "],["linear-regression-in-r.html", "Linear regression in R", " Linear regression in R By now you know the equation of the linear regression model, the idea behind the least-squares approach, and how to calculate and interpret the different elements of the equation. R can calculate all those equations automatically for you using the lm function, which stands for linear model. The cool think is that you can now interpret the results from that function. Lets try using the data on grades and time studying. #lets start by bringing back the data Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5) Grade=c(55, 64, 75, 82,95) # the function lm, requires you to simply set the model as y ~ x. In our case, grades are the dependent variable, so it goes on the Y-axis, and time studying is the independent variable, so it goes in the x-axis. lm (Grade~Hours_Studying) The results of the linear model, lm, in R are pretty straightforward, check the image below. Figure 7.3: Linear regression output in R "],["the-coeficient-of-determination.html", "The coeficient of determination", " The coeficient of determination There is one final cool thing about the linear regression model: it allows you to quantify a neat parameter called the coefficient of determination, \\(r^2\\). That parameter is useful for two main reasons: It tells you how good is the relationship between the two variables, although you sort of know this from the correlation coefficient, \\(r\\), that you estimated earlier. It tells you the percent of variance of Y that is explained by X. Say you found a relationship between plant size and nutrient input with an \\(r^2\\) of 85%. This tells you that 85% of the variability you have among your plants can be explained by the input of nutrients. In other words, if the relationship was positive, then plants that grew a lot had lots of nutrients, while plants that did not grow well was because they did not have nutrients. So given the high \\(r^2\\), it will probably be good idea to add nutrients to the plants. Say that in the opposite you found an \\(r^2\\) of 5%. Then only 5% of the variability in plant size can be explained by nutrient input. In this case, it may be wasteful to add nutrients to the plants, since they affect so little how plants will grow. This may be the case if you have some good soil, so, no need for nutrients. In this example about nutrients and plants, you can see how \\(r^2\\) let you make inferences about the strength of the relationships. One bad thing about \\(r^2\\), as oppose to the correlation coefficient, \\(r\\), is that you cannot know the direction of the relation (i.e., whether it is positive or negative). So you may still have to relay on the correlation coefficient, \\(r\\), to know in which direction are the two variables related. Before we get into the mathematics of the \\(r^2\\), check out this brief explanation: The coefficient of determination, \\(r^2\\), is mathematically calculated as: \\[\\begin{equation} \\text{Coefficient of Determination} = r^2 = \\frac{SSmean - SSfit}{SSmean} \\end{equation}\\] Lets break that equation into its pieces to see what is doing. \\(SSmean\\), stands for Sum of Squares of the Mean. That is the same term we have used before \\(\\sum(y - \\bar y)^2\\). Basically, how far from the mean is each point. If we were to divide that by the number of samples, you will get the variance that we studied earlier. Just to refresh, you take the mean of all values in Y (horizontal line, in image below), for each point measure the distance to that mean (dotted lines), then you square each value and add them together. If you did not square them, when summing them, the result will be zero. Figure 7.4: Sum of Squares from the Mean, SSmean You need to think of the Sum of Squares of the Mean, \\(SSmean\\), as the variability in Y. \\(SSfit\\), stands for Sum of Squares around the Fit. Lets see what this means. Take the data on grades and time studying, relate grades against times studying, and find the best line (Orange line in figure below). Then for each point measure the distance from the point to the line, or so-call residuals (red-dotted lines). Take each residual, square it, and then add them together. What you get is the Sum of Squares around the Fit, \\(SSfit\\). Figure 7.5: Sum of Squares around the Fit, SSfit You need to think of the Sum of Squares around the Fit, \\(SSfit\\), as the variability in Y that was not explained by X. Remember, the regression line is the mathematical formulation of how Y relates to X, whatever is not accounted for by that line are the residuals, or the variation in Y, that remains to be accounted for. So, it you look at the formulation for \\(r^2\\), basically, you are trying to quantify the fraction of variability of Y that was accounted for by the relationship of Y to X. Easy right? Lets calculate \\(r^2\\), #take the data on grades and time studying X=c(0.5, 1.8, 2.4, 3.8, 4.5) #hours studying Y=c(55, 64, 75, 82,95) #grades # lets estimate the regression line using lm, and lets put that model in a variable LM = lm (Y~X) #this is the linear model between Grades~Hours_Studying #you can find out the residuals of that model using the R-Function residuals. Residuals= residuals (LM) #here we create a vector with the residuals from our model #Estimate SSfit SSFit= sum (Residuals^2) # here you are squaring each residual, then adding them #Lets now estimate SSMean DeltaY=Y-mean(Y) # here you take each value in Y and subtract it to the mean of Y SSMean= sum(DeltaY^2) #here you square each score in the line above, sum them together #we have all we need for the calculation of the R2. R2=(SSMean-SSFit)/SSMean R2 ## [1] 0.9637357 So, the \\(r^2\\) of the relationship between grades and time studying is 0.96. That is the fraction of the variability in grades that is explained by the amount of time students study. You can also report the \\(r^2\\) as a percentage by multiplying the fraction by 100. In R, the \\(r^2\\) is outputted as part of the lm function in combination with the function summary, like this: summary (LM) #summary results of the lm we created in the code above. The image below indicates the different outputs, we have studied so far: Figure 7.6: lm outputs "],["predict-interpolation-extrapolation.html", "Predict: Interpolation, Extrapolation", " Predict: Interpolation, Extrapolation Ok, now we know how to build a linear regression model of the form: \\[\\begin{equation} Y = mX + b \\end{equation}\\] Isnt it beauty? That equation describes, to the best possible, how Y relates to X. With the extra support of the Coefficient of Determination, \\(r^2\\), you can also know how strong the relationship is. And it just keep giving. With that relationship, you can now predict any value of Y, at any given value of X. All you have is to replace the X parameter in the equation above, for the value you like, run the calculation to get the expected value of Y. Figure 7.7: Predictions with linear models Lets try. From the relationship between grades and time studying, we already know that the intercept, \\(b\\), was 49.28, and that the slope, \\(m\\), was 9.59. So, the linear model can be formulated as: \\[\\begin{equation} Y = 9.59X + 49.28 \\end{equation}\\] So, given that model, what would be the expected grade of a person studying 4 hours a week?. Simple, replace X for 4, run calculation and done, \\[\\begin{equation} Y = 9.59* 4 + 49.28 \\end{equation}\\] \\[\\begin{equation} Y = 87.64 \\end{equation}\\] For a person that studies four hours a week the expected grade will be 87.64. Interpolation and Extrapolation Predictions based on a linear model can be separated between those that are within the data you have, in which case the prediction is called an interpolation. If the prediction is beyond the values of X that you have, then the prediction is call and extrapolation. Figure 7.8: Interpolation and Extrapolation Needless to say, that when you extrapolate, there is always a risk that the forecast is wrong. The model you created only modeled the relationship between Y and X, for the data you have. Pass that set of data, the relationship may be different, so It is always important to be careful with extrapolations. Predicting Y values for X values that are between observed X values in the data set is called interpolation. Predicting Y values for X values that are beyond observed X values in the data set is called extrapolation or forecasts. Becuase extrapolation predict values beyond what is known, uncertainty is much higher and at times forecasts can be unrealistic. In R, you can predict any value from a linear model, using the predict function, lets try. #take the data on grades and time studying X=c(0.5, 1.8, 2.4, 3.8, 4.5) #hours studying Y=c(55, 64, 75, 82,95) #grades # lets estimate the regression line using lm, and lets put that model in a variable LM = lm (Y~X) #this is the linear model between Grades~Hours_Studying # lets now predict, the expected grades for three students that studied 1 hour, 3 hours and 10 hours PredictGradres=data.frame(X = c(1,3,100)) #here I create a data.frame with the times studied by the three students. You need to create a column, with the same name, as the model, so lm can know which one is the X-variable predict(LM, PredictGradres) ## 1 2 3 ## 58.86272 78.03432 1007.85680 From the predictions above, you can see how the model wrongly predicts that a person that studies 100 hours a week, will get a 1000.7 grade in the class. Obviously, the most you can get is 100%. This helps to illustrate the caution needed when extrapolating a linear model beyond the limits of the data. Two statisticians were traveling in an airplane from LA to New York. About an hour into the flight, the pilot announced that they had lost an engine. Dont worry, says the pilot, there are three left; but, instead of 5 hours it would take 7 hours to get to New York. A little later, the pilot announced that a second engine failed. Dont worry, says the pilot, there are two left; but, instead of 5 hours it would take 10 hours to get to New York. Somewhat later, the pilot came on the intercom again and announced that a third engine had died. Never fear, he announced, the plane can fly on a single engine. However, it would now take 18 hours to get to NewYork, the pilot added. At this point, one statistician turned to the other and said, Gee, I hope we dont lose that last engine, or well be up here forever! "],["outliers.html", "Outliers", " Outliers One of the critical issues with regression models, is that they can be influenced by extreme points. Those extreme points that clearly do not follow the main pattern are called outliers. Sometimes, those outliers could be measurement errors, but at times could also indicate the influence of variables that you did not measure. It is always good to visualize the data in an scatterplot to see if such cases examples of outliers exist on your data. Outliers can also be tested mathematically, by re-running the linear model without them, and check for the effect of removing them on the linear model. There are several approaches to test if such an effect of removing the outlier is significant or not in the linear model, which we will not cover here. But you need to know. Figure 7.9: The outlier "],["the-significance.html", "The significance", " The significance Several times in this book, we have mentioned how there is always a chance that any result can arise by chance alone. The linear regression model is not different. There may still be a chance that you find an \\(r^2\\) similar to the one you found if you did not use any independent variable at all. To rule out that possibility, we use what is called an F-test. We will review the F-statistics in more detail later in the Chapter about hypothesis testing. Basically, some people have taken the time to run millions of simulations of fake random datasets, created linear models with them with varying sample sizes and number of parameters, and estimated the fitness of those models. Then, they put the results of those random models in F-tables, which you can find at the end of most books on stats. The beauty of those tables is that you can compare your model, given the number of samples and the number of variables, to find out the fraction of models similar to yours that could have happened by chance. To compare your model to theirs, you need: the F-value from your model, which we will get into in a sec. the number of variables in your model (for the purpose of comparison to the their tables, we will call this value \\(v1\\)) A parameter, we will call \\(v2\\), which is the number of datapoints in the model minus the number of predictors in the model plus one. \\(v1\\) and \\(v2\\) are just parameters needed to compare to random models of similar parameters. If your model is similar to theirs, then, your results could have emerged by simple chance. If your results were different, then the relationship you found is legit. We will work more on this later on. Ok, but we need to estimate the F-Value of our model. There is an specific equation to calculate the F-value or F-statistics of a linear regression model. However, it can also be predicted using the \\(r^2\\), which we will use here for simplicity, using the following equation: \\[\\begin{equation} F_{statistics} = \\frac{r^2}{1-r^2} *\\frac{v2}{v1} \\end{equation}\\] Where \\(r^2\\), is the coefficient of determination; \\(v1\\) is number of predictors in the model; \\(v2\\) is the number of datapoints minus the number of predictors in the model plus one. If you think about that formula above, you are estimating an standard metric of the variance explained, \\(r^2\\), to the variance not explained, \\(1-r^2\\), for a model of certain characteristics of sample size and variables used. Because the most variance you can explain in any case is always just 100%, this metric could be assumed standard among any model random or not. And that is how then, we can compare our results on legit data to fake data, and see if any result of ours is different from random. With all parameters at hand for our model, we need to look into an F-table to estimate the critical F-value. A critical F-value is the expected F-value at which certain fraction of random models occur. Lets take a moment to understand this. Image a statistician runs one million regressions with random data using certain number of samples and independent variables and for each model he estimates the F-value above. He then creates a frequency distribution of the number of models at each F-value, like the figure below. Figure 7.10: F-distribution From that distribution, you can now find out the F-value at which say 90% of the random models occur; that value is the critical F-value, like in the image below. If say our model has an F-value larger than that critical F-value then we are 90% sure our model cannot have emerge by chance. Alternatively, you could say our model is significant at p&gt;0.1, which is the complement of 90% or 0.9 if you look at it as fractions. That p-value is also called the critical p-value or at times also named alpha, \\(\\alpha\\). Figure 7.11: Critical f-Value at 90% or p=0.1 Lets estimate the F-value for our regression model between grades and time studying, for which we know the \\(r^2\\) was \\(0.96\\). \\(v1\\), the number of predictors in the model would be 1, as we only have one independent variable (i.e., time studying) and, \\(v2\\) is 3; If you recall \\(v2\\) is the number of datapoints (five student) minus the number of predictors (one in our case) plus one. So \\[\\begin{equation} F_{statistics} = \\frac{0.96}{1-0.96} *\\frac{3}{1} \\end{equation}\\] \\[\\begin{equation} F_{statistics} = 79.7259164 \\end{equation}\\] So the F-value for our model was \\(79.73\\). Now we need to find out the critical F-value, and lets take a \\(\\alpha =0.05\\). For this we use an F-table, like the one below, which you can find on most books on stats. Because \\(v1\\) = 1 in our case, you have to select the first column, then scroll down until \\(v2\\) in the gray column is equal to 3, at the interception is the F-critical. Or the F-value at which 95% of the random models occurred. There are similar tables like this for each \\(\\alpha\\). So for a model like ours, 95% of the random models should have an critical F-value smaller than 10.13. However, our model had an F-value = 79.7259164. So our model is very different from the random expectation, or also-called significantly different at p&gt;0.05. Figure 7.12: lm outputs In R, the F-value is produced automatically with the \\(lm\\) function and the \\(summary\\) function. The outputs from the linear model, lm, also include the exact probability at which our model could be random, see image below. summary (LM) #Here LM is the linear model, lm, we create in the code just above. Figure 7.13: lm outputs In conclusion, there is a significantly strong relations (\\(r^2 = 0.96\\)) at p&lt;0.05 between studying for my class and getting a nice grade. There you go, keep studying. "],["multiple-regression.html", "Multiple regression", " Multiple regression Up to now, we have used the linear regression model to predict Y in terms of only one variable, X. However, as you may have noted in the examples above, it is rare for a model with a single independent variable to predict fully the dependent variablethose residuals that are left tell you there is something else that may influence the dependent variable. Linear regression further allows you to check for the effect of additional variables, using the same principle. Think about it this wayyou run a linear regression model with one independent variable and some residuals are left. Take those residuals as independent variable in a second linear model, relate those residuals to another variable, and see how much the unexplained variation by the first independent variable is explained by the second independent variable. You can keep going at it, until you could potentially find all the set of variables that explain your response variable. The example above is an oversimplification of how multiple regression works, as there are some collinearities among independent variables that you have to handle. But for now I just want you to know that linear regression allows you to test additional variables by trying to explain the unexplained variation in Y with additional variables. The so-call multiple regression analysis. Lets take the data we have been using, and say, we also got data on the amount of debt of each studentthis variable may indicate the level of stress of the student, a likelihood that they have a job that prevents them of working fully and well, etchow does this second variable influences the grades of students in my class?. Lets check.. #lets start by bringing back the data on time studying and grades Names=c(&quot;Peter&quot;,&quot;Laura&quot;, &quot;John&quot;, &quot;Chip&quot;, &quot;Tom&quot;) Hours_Studying=c(0.5, 1.8, 2.4, 3.8, 4.5) Grade=c(55, 64, 75, 82,95) #here is the data for the second independent variable for the same students Debt=c(80, 60, 55, 15, 5) #debt of students in thousands of dollars # we use the same lm function we used before and add debt as independent variable, like this: MultipleRegression=lm (Grade ~ Hours_Studying + Debt) #next we check the results summary(MultipleRegression) Figure 7.14: R multiple regression results The outputs are the same as we sow before. You have a unique intercept, but now you have a slope for each variable. You also need to pay attention to a second result, which is the p-values for the individual variables, which I indicate in the image above. Plus, the p-value for the full model. In this case, while the full model was significant, the individual variables were not. In this specific case, this is happening because our sample size is too small, which bring me to the caution that you need to have when running multiple regression analysis: adding more variables, while it will increase the predictive power of the model, also will increase your chances that the results may happen by chance. "],["non-linear-regression.html", "Non-linear regression", " Non-linear regression There will be numerous instances in which a variable Y does not relate linearly to a variable X. Think the growth of people, for instance. Early in life a person will grow fast, but over time this growth will reduce, eventually leveling off. These types of relationships are not well described with linear models; and there is where non-linear models come in. In R, non-linear regression models are handled with an specific function call \\(nlm\\), but you can also linearize the variable and use the linear model you used before. Take the example below, showing the relationship between age and height of a random person. Lets start by fitting a linear model to it. #let&#39;s create a variable for height (dependent variable) and other for age (independent variable) Age=c(2,4,6,8,10,12,14,16,18,20,22,24,26,28,30) Height=c(50, 65,85, 95, 110, 120,130,140,145, 152, 158, 169, 170, 171, 171.2) LM=lm(Height~Age) #here is the linear model plot(Height~Age) abline(coef = coef(LM)) summary(LM) ## ## Call: ## lm(formula = Height ~ Age) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.257 -5.947 3.235 7.426 11.253 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 59.3638 5.6038 10.59 9.18e-08 *** ## Age 4.3364 0.3082 14.07 3.03e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.31 on 13 degrees of freedom ## Multiple R-squared: 0.9384, Adjusted R-squared: 0.9337 ## F-statistic: 198 on 1 and 13 DF, p-value: 3.028e-09 The \\(R^2\\) is pretty good, \\(r^2\\)=0.9383925. You can try a non-linear model, by transforming the datathere are numerous types of transformations possible, and many will give you different types of fit using linear models. Here is where an approach called model selection comes in, which is to find which of all models to fit to the data is the best. That is quite an endeavor that we will not cover in this basic stats class, but you should know that selecting among different types of models is non-trivial. For for the sake of seeing how a non-linear model is done, lets try a logarithmic transformation of the data used above for height and age. Age=c(2,4,6,8,10,12,14,16,18,20,22,24,26,28,30) Height=c(50, 65,85, 95, 110, 120,130,140,145, 152, 158, 169, 170, 171, 171.2) #to log transform simply use the log function: LogHeight=log(Height) #log transforming the variable Y LogAge=log(Age) #log transforming the variable X LogLM=lm(LogHeight~LogAge) #now we re-run the model, but with the transformed variables plot(LogHeight~LogAge, ylab=(&quot;Log height (cm)&quot;), xlab=(&quot;Log age (years)&quot;)) abline(coef = coef(LogLM)) summary(LogLM) ## ## Call: ## lm(formula = LogHeight ~ LogAge) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.06615 -0.01057 0.01062 0.02135 0.03612 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.56706 0.02982 119.6 &lt; 2e-16 *** ## LogAge 0.48275 0.01120 43.1 2.04e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03277 on 13 degrees of freedom ## Multiple R-squared: 0.9931, Adjusted R-squared: 0.9925 ## F-statistic: 1858 on 1 and 13 DF, p-value: 2.043e-15 You can now see how this non-linear modeling of the variables gives you an even better \\(r^2\\)=0.9930516. There are numerous complexities to the use of non-linear models, that will not be addressed herethe main goal at this introductory level is to know that regression modeling can be done in non-linear relationships. "],["exercises-2.html", "Exercises", " Exercises Some of these questions may take a while to load depending on your internet connection. "],["homework-4.html", "Homework", " Homework A biologist has been monitoring the population of wolfs and caribou for several years in Denali National Park, Alaska. Let x represents the caribou population (in hundreds) and y the wolf population in the park. x = 30 34 27 25 17 23 20 y = 66 79 70 60 48 55 60 Make a scatter plot of the data, adjust aesthetics so the figure follows publication guidelines. What is the equation that defines the linear relationship between wolfs and caribou. What is the wolf population when there are not caribou. What is the increase wolf population for each hundred caribou. What will be the expected wolf population, at say 1000 caribou?. any caution on this prediction? Is that a strong relationship?. what evidence can you provide? Was it significant, what evidence can you provide?. also what does is mean significant? Place the figure and your responses in a Word document and email it as a homework. Send a nice looking document. "],["8-hypothesis-testing.html", "8 Hypothesis testing", " 8 Hypothesis testing In the prior chapters, we have been dealing with three of the core goals of statistics: visualization of data (i.e., scatterplots, histograms, etc.), description of the data (i.e., mean, mode, SD, etc), and relationships among variables (e.g., correlation, regression). In this chapter, we will deal with a fourth main goal of statistics: to tell if something (one thing) or somethings (many things) are different. There is an amazing level of complexity to that single question, of whether a thing or set of things are different. In this chapter, I want to break that complexity into its peaces, so you can better comprehend the results of any test aimed at testing for significant differences. Figure 8.1: Rejecting the extremes or special Expectation for this chapter At the end of this chapter, it is expected that you: Can state a null hypothesis and alternative hypothesis. Can interpret the level of significance and the critical values. Can interpret the p-value. Can interpret the different types of errors when making a conclusion on a hypothesis. Alright, lets get to it. "],["background-info.html", "Background info", " Background info If the question is asked if something is different, how do we know that a thing is different? It is rare the case when two things are identical, so is everything different?. In statistic, we commonly use the term significantly different to indicate more precisely the level at which something is different. We will get to this in a moment. When the question is formulated of whether something is significantly different; inherently we assume a couple things: There is a reference point to which we are comparing to. That reference is most commonly, almost always, a metric we obtained from the population. So it is compared to the data from the population that will know if something is different. A population is a whole, its every member of a group. A sample is a fraction of the population. Figure 8.2: Population, sample, individual The second thing implied is that there is a threshold beyond which we will assume something is different. Who determines the level at which something is different?. That will be the so-call level of significance, which at times will appear named as alpha, \\(\\alpha\\). As mentioned earlier, in biology and most sciences, we generally accept an \\(\\alpha\\) smaller than 0.05. Basically, we assume something is significantly different, if it is larger or smaller than the top or bottom 5% of all individuals in that population. Figure 8.3: How p&lt;0.05 feels like Of course, you can use different levels of significance, and it all cases it means the same: the threshold beyond which you assume something is different. The particular value in a distribution where the given \\(\\alpha\\) occurs is call the critical value. Say you want to test a hypothesis that a given rabbit is significantly larger than the common rabbit and you want to test this hypothesis with a level of significance of 0.1. In this case, you need to know the size at which 10% of the common rabbit occurs. In the example below, 10% of the rabbits are larger than 7in. In this case, the critical value for a level of significance of 0.1 is 7in. It is called critical value, because that is the cut off that will determine if your sample is significantly different or not. In the example below, the rabbit I want to compare is 6.8in tall, so at a significance level=0.01, this rabbit is not significantly large. Figure 8.4: Critical value By the way, 1 - \\(\\alpha\\) is what is call level of confidence. Think about it, if I conclude that something is significantly different at \\(\\alpha\\)=0.05, then my confidence has to be the complement of that, which will be 0.95 or 95%. The threshold beyond which we reject a null hyphothesis is called the level of significance or alpha, \\(\\alpha\\). The value in a population at which \\(alpha\\) falls is called the critical value. The complement to the level of significance, 1- \\(\\alpha\\), is call the level of confidence . Lets put together the data on the population and the level of significance for you to better know how then we make a call on whether something is different. p-value and alpha Early on, we studied how a population can be characterized based on the mean value of its individuals and the variability of those individuals, the so-call standard deviation. With those two parameters alone you can know how the entire population looks like. Lets look at an example. In the USA, the average men is 177.8cm in height with an standard deviation of 7.62cm. Lets plot this population. Mean_Height=177.8 # height of average male in the USA SD_Heigh=7.62 # Standard deviation of male population in the USA MenUSPop&lt;- rnorm(164000000,Mean_Height,SD_Heigh) #we select from a population with that mean and SD, the 164 million men in the use. This will be the entire population....this line may take a while, dependenting how good your computer is...you are taking over 160 million values...you can click scape if it take to long and use rather take a still large sample of 1.6 million men, for contnuing with the execersize. breaks = seq(100, 240, length.out = 80) #I create a set of bins for the x-axis of the distribution #next I plot the data for the size of the entire population of men in the USA hist(MenUSPop, main=NA,xlim=c(100, 240),ylim=c(0, 20000000),breaks = breaks, xlab = &quot;US men heigth (cm)&quot;,ylab = &quot;Number of people&quot;) #plot the mean height of men abline(v=Mean_Height, lwd=2, col=&quot;blue&quot;, lty=2) #lets add a label to mean label text(Mean_Height,20000000,labels=&quot;Mean&quot;,pos=4, col=&quot;blue&quot;) From the figure above, you can see that while most men are on average 177.8 cm tall, some men can get very tall and others are very short. And just so you know, on the distribution above, the tallest men was \\(225.48\\) cm while the shortest was \\(134.96\\) cm. One cool property of the population data, specially data that are normally distributed, is that you can tell relatively well the fraction of individuals away from the mean by knowing the standard deviation. If you recall Chapter 5, we mentioned that 95% of the individuals on any normally distributed population are within two-standard deviations from the mean. Lets check that out. In the figure below, I illustrate random samples of males taken from the US men height population. If I were to count the fraction of men two standard deviations below or above the mean height (percentage red numbers shown in the figure) on each sample, you would see that on any sample there are cumulatively almost always 5%, or about 2.5% on each tail of the distribution. Cool ah? Figure 8.5: Fraction of population beyond 2SD l And it gets even better, you can know very precisely, then the fraction of individuals in a population that are above or below a given standard deviation. Lets say I found a men that is 193.04cm tall. What fraction of men can be taller than him? In this case, this guy is 2SD (Standard deviations) above the mean. The average height was \\(177.8\\) and one standard deviation, SD, was \\(7.62\\). So, \\(177.8\\) + (2 *\\(7.62)\\) = \\(193.04\\). So you know that below the mean there are 50% of men, and two standard deviations above the mean are 47.5%. So 97.5% of the men are shorter than that guy. This guys is as tall as the tallest 2.5% of the men in th USA. If we assume a level of significance of 5%, then you can claim this one guy is significantly tall. Figure 8.6: How do you like me? This mathematic property of a normal population also works in reverse. Lets say I found a men that is \\(162.56\\)cm tall?. What fraction can be shorter than him? In this case, this guy is 2SD (Standard deviations) below the mean. The average height was \\(177.8\\) and one standard deviation, SD, was \\(7.62\\). So \\(177.8\\) - (2 *\\(7.62)\\) = \\(162.56\\). We know that above the mean there are 50% of men, and two standard deviations below the mean are 47.5%. So 97.5% of the men are taller than that guy. This guy is as short as the shortest 2.5% of the men in th USA. If we assume a level of significance of 5%, then you can claim this one guy is significantly short. In statistics, the fraction of individuals, members, data points, etc. that are above or below the given sample value in a population is called the p-value. So in the example above, our conclusion that this one guy was significantly shorter has a p-value = 0.025 or 2.5%. If we assume a level of significance, \\(\\alpha\\), of 0.05, then we are assuming that something is significantly smaller or taller, if it is as short or as tall to the shortest or tallest 5% of the population. In R, we can find out the critical value in a distribution of data at any given significance level using the \\(qnorm\\) function, which we used earlier. Worth nothing that this function assumes that the population is normally distributed. We will use this fucntion briefly. In the example just above, the short guy was \\(162.56\\)cm tall, which was as short as the shortest 2.5% of the population. So this person is significantly small at an \\(\\alpha\\) of 0.05. In fact, we can be more confident than that as our conclusion has a p-value of 0.025; We know that over 97.5% of the US population is taller than him. Multiple academic discussions have taken place around the need to report the level of significance when the p-value is more accurate. At least now you know what they both mean. From the example above, we can see that the overall approach to test if something is significantly different is relatively straightforward. All you need to know is how many standard deviations from the mean a given individual or sample is. From this you can know the fraction of individuals that are above or below that given number (the p-value). You can then compare that p-value to the level of significance, \\(\\alpha\\), you want to use. If the p-value is smaller than the level of significance then you conclude that the one thing is significantly different at the given \\(\\alpha\\). P-VALUE is the fraction of individuals in a population as extreme as or more extreme than an observed sample It is also refered as a probability. Lets finish this section by looking at the fraction of individuals in a population at increasing larger standard deviation. In short the fractions at each tail of a distributiona re very predictable if you know the standard deviation from a population. Figure 8.7: Significance level "],["approach-to-testing-a-hyphothesis.html", "Approach to testing a hyphothesis", " Approach to testing a hyphothesis In the prior section, you sow how to test if something is significantly different or not. While overtime you can get to it right away, you should know that there is a ritual/approach to how to test a hypothesis. It goes like this: State the null and alternative hypothesis. Chose the level of significance. Find the critical values. Compare the sample value to the critical value. Draw a conclusion. "],["stating-hyphotheses-and-tails.html", "Stating hyphotheses and tails", " Stating hyphotheses and tails The statistical approach to hypothesis testing is to reduce all likely options to a binary choice between the null hypothesis or the alternative hypothesis. The null hypothesis is denoted with the letters \\(Ho\\). The alternative hypothesis is denoted with the letters \\(H1\\). The null hypothesis is always making reference to how things should commonly be, it can also make references to the option of no-difference or that something is equal to a given value. The alternative hypothesis is all what the null hypothesis is not. Check the following video, about how to state hypothesis. In statistics, you can only reject the null hypothesis or fail to reject the null hypothesis (this is synonym for accept the null hypothesis, but this term is not accepted because we are never certain). Along the same lines, when you reject the null hypothesis, we you do not accept the alternative hypothesis. Figure 8.8: Painfull rejection narrative There are several ways to state hypotheses. Lets try a few examples to clarify: Say a burger company claims that their Combo #1 has 1,800 calories, with a variability (Standard deviation, SD) of 50 calories. To test if this is true you can set your hypotheses different ways, like: Null hypothesis, Ho: Combo1 = 1,800 calories. Alternative hypothesis, H1: Combo1 \\(\\neq\\) 1,800 calories. In this form, I am asking if the burger is equal to 1,800 calories. In this case, the alternative has to be that the burger is different than 1,800cal. To be different, a burger could be much smaller than 1,800cal but it can also be much larger than 1,800 calories. If I take several of their burgers, and measure their caloric content, I can reject the null hypothesis if the sample I took had much higher or lower number of calories that the critical value at which the significance level, \\(\\alpha\\), I chosen falls. This is what is call a two-tail test. Basically, you have to divide the level of significance in two as you will be testing if the sample is much smaller or much larger. If my sample falls in the red areas, in the image below, then I reject the null hypothesis, as it means that my one sample is much larger than the upper 97.5% or much lower 2.5%, or an \\(\\alpha\\)=0.05. Lets visualize this in R, mean=1800 # this is the mean number of calories in their burger. We can think of this as the population mean sd=50 #this is the variability in calories # create a normal distribution of burgers with the given mean and SD x &lt;- seq(-4,4,length=100)*sd + mean # This w y &lt;- dnorm(x,mean,sd) plot(x, y, type=&quot;l&quot;, xlab=&quot;Calories per burger&quot;, ylab= &quot;Fraction of burgers&quot;,ylim=c(0,0.009),main=&quot;Ho: Combo1 = 1,800 calories&quot;) # this is how the calories of the population of burgers should look like #because this is a two tail distribution, I mean something will be different if it is much larger or much smaller, then you have to look for the critical value at alpha divided by 2. LowerCriticalVal=qnorm(0.025,mean,sd) # we need to find the critical value at 2.5% and the upper 97.5%...that is a significance level of 5% UpperCriticalVal=qnorm(0.975,mean,sd) # upper critical value, or value above which 97.5% of the population occurs. #plot the critical thresholds set by alpha abline(v=UpperCriticalVal, lwd=2, col=&quot;red&quot;, lty=2) abline(v=LowerCriticalVal, lwd=2, col=&quot;red&quot;, lty=2) #Lets plot the critical areas polygon(c(x[x&gt;=UpperCriticalVal], UpperCriticalVal), c(y[x&gt;=UpperCriticalVal], y[x==max(x)]), col=&quot;red&quot;) #right hand tail polygon(c(x[x&lt;=LowerCriticalVal], LowerCriticalVal), c(y[x&lt;=LowerCriticalVal], y[x==max(x)]), col=&quot;red&quot;) #right hand tail #let&#39;s make this fancier and put arrows to indicate rejection areas. We will use the package shape for this #install.packages(&quot;shape&quot;) library(shape) Arrows(LowerCriticalVal,0.006,LowerCriticalVal-50,0.006,lwd=2, arr.type=&quot;triangle&quot;) text(LowerCriticalVal,0.0065,&quot;Reject&quot;,pos=2) #we can do add the same arrow for the upper tail of rejection zone Arrows(UpperCriticalVal,0.006,UpperCriticalVal+50,0.006,lwd=2, arr.type=&quot;triangle&quot;) text(UpperCriticalVal,0.0065,&quot;Reject&quot;,pos=4) #while we are at it, lets indicate the zone of the level of confidence Arrows(LowerCriticalVal,0.0085,UpperCriticalVal,0.0085,lwd=2, arr.type=&quot;curved&quot;,code=3) text(mean,0.0085,&quot;Level of confidence &quot;,pos=3) The hypotheses could also be stated as: Null hypothesis, Ho: Combo1 &lt;= 1,800 calories. Alternative hypothesis, H1: Combo1 &gt; 1,800 calories. In the example above, I am being more specific, testing the null hypothesis that the average burger has less than 1,800 caloriess; the alternative hypothesis has ti be that the burger has more than 1,800 calories. If I take several of their burgers, and measure their caloric content, I can reject the null hypothesis only if the sample I took had much higher number of calories than the critical value at which the significance level, \\(\\alpha\\), I chosen falls. This is what is call a right-tail test. Lets visualize this in R, mean=1800 # this is the mean number of calories in their burger. We can think of this as the population mean sd=50 #this is the variability in calories # create a normal distribution of burgers with the given mean and SD x &lt;- seq(-4,4,length=100)*sd + mean y &lt;- dnorm(x,mean,sd) plot(x, y, type=&quot;l&quot;, xlab=&quot;Calories per burger&quot;, ylab= &quot;Fraction of burgers&quot;,main=&quot;Ho: Combo1 &lt;= 1,800 calories&quot;) # this is how the calories of the population of burgers should look like UpperCriticalVal=qnorm(0.95,mean,sd) #in this case I am looking for the upper 5% of the individuals..or the threshold at which 95% of the population occurs #plot the critical thresholds set by alpha abline(v=UpperCriticalVal, lwd=2, col=&quot;red&quot;, lty=2) #Lets plot the one critical area polygon(c(x[x&gt;=UpperCriticalVal], UpperCriticalVal), c(y[x&gt;=UpperCriticalVal], y[x==max(x)]), col=&quot;red&quot;) #right hand tail The hypotheses could also be set like this: Null hypothesis, Ho: Combo1 &gt;= 1,800 calories. Alternative hypothesis, H1: Combo1 &lt; 1,800 calories. In the example above, I am testing if the burgers have more calories than what the company claims. The alternative hypothesis, then has to be that the burgers have less than 1,800 calories. If I take several of their burgers, and measure their caloric content, I can reject the null hypothesis only if the sample I took had much fewer number of calories than the critical value at which the significance level I chosen falls. This is what is call a left-tail test. Lets visualize this in R, mean=1800 # this is the mean number of calories in their burger. We can think of this as the population mean sd=50 #this is the variability in calories # create a normal distribution of burgers with the given mean and SD x &lt;- seq(-4,4,length=100)*sd + mean # This w y &lt;- dnorm(x,mean,sd) plot(x, y, type=&quot;l&quot;, xlab=&quot;Calories per burger&quot;, ylab= &quot;Fraction of burgers&quot;,main=&quot;Ho: Combo1 &gt;= 1,800 calories&quot;) # this is how the calories of the population of burgers should look like UpperCriticalVal=qnorm(0.05,mean,sd) #in this case I am looking for the lower 5% of the individuals..or the threshold at which 5% of the population occurs #Lets plot the one critical area polygon(c(x[x&lt;=UpperCriticalVal], UpperCriticalVal), c(y[x&lt;=UpperCriticalVal], y[x==max(x)]), col=&quot;red&quot;) #right hand tail #plot the critical thresholds set by alpha abline(v=UpperCriticalVal, lwd=2, col=&quot;red&quot;, lty=2) "],["accepting-or-rejecting-the-null-hypothesis.html", "Accepting or rejecting the null hypothesis", " Accepting or rejecting the null hypothesis In a nutshell, the idea of testing a hypothesis is to see if a given individual or sample is larger or smaller than the population cutoff set by the level of significance, \\(\\alpha\\). In a two tail test, the null hypothesis is rejected if the sample value is larger or smaller than the critical value. On the contrary, if your sample value is within the range outlined by your critical value, then you conclude that you failed to reject the null-hypothesis. Please remember that in statistics you only fail to reject or reject the null hypothesis but you never prove the alternative hypothesis when you reject the null hypothesis. In a right tail test, the null hypothesis is rejected if the sample value is larger than the critical value. Figure 8.9: Types of tests In a left tail test, the null hypothesis is rejected if the sample value is smaller than the critical value. Hypotheses can also be tested by simply comparing your chosen level of significance, \\(\\alpha\\), to your p-value. Every time your p-value is smaller than the \\(\\alpha\\) you reject the null hypothesis. Figure 8.10: If the P is low, the Ho must go Think about this for a moment. Lets say I am testing if the heart rate of a person is significantly larger that the average population using a level of significance at 0.05. In this case, I am assuming a heart rate will be significantly larger for any heart rate faster than the top 5% of all hearth rates in the human population. Lets say I measured the hearth rate of a person and estimated that it has a p-value of 0.02. This means that the heart rate of this one person is as fast as the top 2% fastest hearth rates in the population. In this case, the p-value is smaller than \\(\\alpha\\), so I reject the null hypothesis, and conclude that indeed the hearth rate of the given individual is significantly larger than expected from the average human population. Figure 8.11: The P-value "],["testing-a-hypothesis-by-brutal-force.html", "Testing a hypothesis by brutal force", " Testing a hypothesis by brutal force In the following chapters, we will introduce standardized ways to test hypotheses comparing sample values to critical values; I want to finish this chapter by illustrating the ultimate idea behind testing a hypothesis by running a brief simulation. This simulation will aim to show the general approach of hypothesis testing. Lets say that you work for the FDA, and you have been commissioned to provide evidence for a lawsuit case involving the calories in MacDonals burgers (this is a fictitious example). The pending lawsuit refers to a person who suffered a heart attack after his cholesterol levels increased considerably. Prior to the heart attack, the doctor of this person recommended him that he kept a daily diet of no more than 2000 calories a day, so he satisfied his weakness for burgers eating a new type of MacDonals burger that claimed to have less than 1,500 calories (Standard deviation, SD= 50). Let say that you collected a random sample of ten burgers and found that they have on average 1600 calories. So the question is, do the claim by this individual hold any right? Lets start by stating the hypotheses: Null hypothesis, Ho: burger &lt;= 1,500 calories. Alternative hypothesis, H1: burger &gt; 1,500 calories. In this case, I specifically want to know if these burgers have less than 1500 calories as claimed by the company. We want to test this hypothesis at an \\(\\alpha\\), level of significance, of 0.05 or 5%. Alright, lets start our simulation. MeanCal = 1600 #calories in the sample of burgers plot(1, type=&quot;n&quot;, xlab=&quot;Total calories in MacDonal&#39;s bugers&quot;, ylab=&quot;Percent of burgers&quot;, xlim=c(1000, 2000), ylim=c(0, 10)) # lets create an empty plot #plot the sample value abline(v=MeanCal, lwd=2, col=&quot;blue&quot;, lty=2) #lets name the mean sample value text(MeanCal,10,labels=&quot;Sample value&quot;,pos=4, col=&quot;blue&quot;) Next, you can plot the critical value for an alpha of 0.05 in a population with a mean of 1500 and an standard deviation of 50. In this case, the critical value at \\(\\alpha\\)=0.05, is simply the cutoff at which 95% of the calories in the MacDonals burgers occurs. If we are assuming the population data is normal and has a mean of 1500 and a Standard deviation, SD, of 50, we can find out that critical value using the \\(qnorm\\) function in R. CriticalVal=qnorm(0.95,1500,50) # here we want to know the cutoff at which 95% of the observations occur in a population with a mean of 1500 and an SD of 50 plot(1, type=&quot;n&quot;, xlab=&quot;Total calories in MacDonal&#39;s bugers&quot;, ylab=&quot;Percent of burgers&quot;, xlim=c(1000, 2000), ylim=c(0, 10)) # lets create an empty plot #plot the sample value abline(v=MeanCal, lwd=2, col=&quot;blue&quot;, lty=2) #lets name the mean sample value text(MeanCal,10,labels=&quot;Sample value&quot;,pos=4, col=&quot;blue&quot;) #plot the critical value abline(v=CriticalVal, lwd=2, col=&quot;red&quot;, lty=2) #lets name the statistical critical value text(CriticalVal,10,labels=&quot;Statistical critical value&quot;,pos=2, col=&quot;red&quot;) For the comparison between the critical value and the sample value, we can already observe that the sample burgers in reality have more calories than claimed by MacDonals at a significance level, \\(\\alpha\\), of 0.05. Lets try the same test but building an actual population of expected burgers. For this, we take one random sample from a normal population with a mean of 1500 and an standard deviation, SD, of 50. In R, you can take any number of individuals randomly from any normally distributed population using the function \\(rnorm\\). MeanPopulation=1500 SDPopulation=50 RandomSample1&lt;- rnorm(1,MeanPopulation,SDPopulation) RandomSample1 ## [1] 1508.21 So one sample burger from a population with a mean of 1500 and a standard deviation, SD, of 50 is \\(1508.21\\). Hmm, ok, but we only have one random sample. To be robust, we probably should take 100,000 random samples and from all of those see if the top 5% of the random samples, our critical threshold, is larger or smaller than our observed sample value. To run such a simulation, we can use the for loop function in R. Lets try. Let me explain that function with the example. population= c() #we create an empty vector where we will store the random averages of our 1000 random samples of ten burgers for (sampleI in 1:100000) { # here I set a variable called sampleI that will loop from 1 to 1000 meanRandomSampleI&lt;- rnorm(1,MeanPopulation,SDPopulation) #take a random value #lets place the value for that sample in the vector population=c(population, meanRandomSampleI ) #basically I am appending the value of every sample in every loop to the vector } # the code between {} will run for 100,000 times. to test lest count the number of entries in the vector length(population) ## [1] 100000 Ok, we just created a population of 100,000 random samples from a normal population with a mean of 1500 and an standard deviation, SD, of 50. Lets do a histogram of that database breaks = seq(1200, 1800, length.out = 100) #lets create a set of bins hist(population,main=NA,xlim=c(1200, 1800),ylim=c(0, 5000),breaks = breaks, xlab = &quot;Total calories in MacDonal&#39;s bugers&quot;,ylab = &quot;Number of ramdon samples&quot;) Looking good.lets draw a line for the 5% level of significance for the expected distribution. We will use the R function \\(quantile\\), which we used earlier, to find out the value located at the 95% percentile, which will outline the position to the right where 5% of the cases occur. Lets call this the modeled critical value, to differentiate it from the statistical critical value we calculated earlier. ModelledCriticalValue=quantile(population,0.95,type=1) #quantile finds the value in the population of calories located at the top 5% percentile ModelledCriticalValue ## 95% ## 1582.898 Now we can compare the average calorimetric content in our actual sample of 10 burgers (sample value) to the value located at the 5% mark (i.e., the level of significance). hist(population,main=NA,xlim=c(1200, 1800),ylim=c(0, 6000),breaks = breaks, xlab = &quot;Caloric content of burgers&quot;,ylab = &quot;Number of ramdon samples&quot;) #plot the sample value abline(v=MeanCal, lwd=2, col=&quot;blue&quot;, lty=2) #lets use a label for it text(MeanCal,6000,labels=&quot;Sample value&quot;,pos=4, col=&quot;blue&quot;) #plot the 5% modeled critical value abline(v=ModelledCriticalValue, lwd=4, col=&quot;orange&quot;, lty=2) #lets name the critical value text(ModelledCriticalValue,6000,labels=&quot;Modeled critical value&quot;,pos=2, col=&quot;orange&quot;) #plot the 5% statistical critical value abline(v=CriticalVal, lwd=2, col=&quot;red&quot;, lty=2) #lets name the mean sample value text(CriticalVal,5500,labels=&quot;Statistical critical value&quot;,pos=2, col=&quot;red&quot;) Pretty neat, ah?. the modeled and statistical critical value were identical. And they both were below the sample value. In short, the sample of ten burgers (Blue line in figure above) was much larger than the critical value at 5% (red lines), we then reject the null hypothesis that these burgers have less than 1,500 calories, and would have to embrace the alternative hypothesis that the burgers have more than 1500 calories. You can surmise in your report to the judge that the one guy lawsuiting MacDonals was deceived. A potential case of false advertisement. "],["error-types.html", "Error types", " Error types You probably noted the level of caution used while testing hypotheses. That is because we work on probabilities, so we are never certain. In hypothesis testing, we deal primarily with those things we could never know for sure; there is always a chance that we are wrong when making a conclusion. That chance of being wrong can be separated into two different types of errors called Type I and Type II errors. I found a couple neat examples online you should check for further clarification on the Type-I and Type-II errors here and here Lets try the one example about going to hell or to heaven depending on your threshold of what qualifies as a good or a bad thingthe example goes like this: Figure 8.12: Errors that send you to hell There is a God, we have never met in person, that holds the ultimate threshold of the actions that qualify to go to heaven or not. Because we do not know the guy, and we do not have fully clear guidelines, there may be an speculation on that threshold. The point here is that there is an ultimate threshold, the ultimate truth, which we do not know for certain. On the other side of this problem is our judgment of what God would mean such threshold is. The ten commitments were a nice start, but what about when you are mean to your teacher?. The Bible says do not do to others what you do not want others to do to you. Would being mean to another person take points out my score to go to heaven?. If I were you I would play it save and be nice to everyone, including your teacher, but we are uncertain if being mean reduces the score of the going-to-heaven ticket. The point above is that there is an ultimate threshold that defines the truth, a threshold that we do not know for certain, but that we can approximate. This approximation threshold is what we called \\(\\alpha\\) earlier: the chance we are giving ourselves to be wrong in our decisions. When our human \\(\\alpha\\) threshold matches the divine threshold, then we got lucky, and we were right every time we did something in life. No errors here. I did all right things knowingly knowing I will end up in heaven or I did all wrong things knowingly knowing I would end up in hell. However, if my threshold was too low and I did a lot of mean things, that turned out to be bad and reduced my going-to-heaven-score, then my threshold did have an error that let me to conclude that doing certain things were ok, when in truth they were not. If you think about this type of error, you accepted the hypothesis that certain things were ok, but turned out to be they were not fine. This is what is call a false negative, or a Type II error. Say that in the opposite my threshold to define what was right or not was too high. While it may help me minimize my Type II error, such a very high threshold means that I am probably not doing things that could be ok, things that may make life nicer. A very high threshold will lead to what is call a false positive or a Type I error. Rejecting things that turned out were ok. Figure 8.13: Type I and Type II errors A false positive, also call Type I error, is to reject the null hypothesis when it is, in fact, true. A false negative, also call Type II error, is to accept the null hypothesis when it is, in fact, false. "],["exercises-3.html", "Exercises", " Exercises Some of these questions may take a while to load depending on your internet connection. "],["homework-5.html", "Homework", " Homework The annual migration of a bird through a watershed in Florida has remained stable for several years at an average of 1,2 million birds and a standard deviation of 100,000 birds. In the last year, the number of birds was 700,000. There have been numerous concerns that a development project may be damaging the habitats of the bird. As a first order of inquiry, we need to know if this years number was significantly lower than in prior years. Please test this hypothesis at a 5% level of confidence? In this exercise, please: Write down the null and the alternative hypothesis. Please run an R-simulation to test this hypothesis. Create a frequency distribution plot indicating the expected distribution of the birds and mark with a line the observed value for the last year. Also mark on the figure the 5% boundary of the expected distribution. In a Word document write down your answers, the figure and the code. "],["9-standard-hyphostesis-tests.html", "9 Standard hyphostesis tests", " 9 Standard hyphostesis tests In the prior chapter, we learned that the basic principle of hypothesis testing was to find out if a given value (from a sample) was on the extremes of a population. The threshold for what we mean by extreme is determined by the \\(\\alpha\\) value. We can test a hypothesis by brutal force running a simulation, but we can also run a set of available tests that will tell you the position of your sample in relation to the population. There are a diversity of statistical tests available for testing hypotheses, but here we will deal only with those tests intended to test if one thing, two things or more than two things are different. By the way, the suitability of available tests depends on whether your data are normally distributed or not (please recall from chapter 3 that normally distributed data looks like a bell-shape curve). Figure 9.1: Data distribution Tests for normally distributed data are call parametric tests. In cases, where your data are not normal (e.g., it is biased, or homogeneous) requires a different set of tests called non-parametric tests. We will not cover non-parametric here but you need to be aware of this distinction. Expectation for this chapter At the end of this chapter, it is expected that you can identify the case you want to test (i.e., one, two or more sample tests), and select the proper statistical method to test for significant differences for each case. "],["alternative-types-of-tests.html", "Alternative types of tests", " Alternative types of tests When comparing samples, in broad terms, there are three likely options: One sample test This type of case relates to situations in which you have a sample parameter and you want to compare it a population. Say you are disappointed by how much smaller the candy bars in a store look like. Say you take a few samples of candy bars and found they are on average 180g. The label says they are on average 200g +/-10g. This would be a case of a one-sample test. Basically, you have one sample and you want to find out if it is different to the expected value from a population. Lets visualize this: SampleWeight=180 # this is the average weight of the samples you took PopulationWeight=200 # Average weight reported in the lable of the candy bar. This of this as the population mean. SD=10 # reported Standard deviation for the candy bars. Think if this as the population standard deviation SimulatedPopulation&lt;- rnorm(10000,PopulationWeight,SD) #lets simulate a population of 10000 candy bars. breaks = seq(150, 250, length.out = 80) #I create a set of bins for the x-axis of the distribution, this will determine how many bis you have #next I plot the data hist(SimulatedPopulation, main=NA,xlim=c(150, 250),ylim=c(0, 800),breaks = breaks, xlab = &quot;Candy bar weight (g)&quot;,ylab = &quot;Number of candies&quot;) #plot the sample mean abline(v=SampleWeight, lwd=2, col=&quot;red&quot;, lty=2) #lets add a label to mean label text(SampleWeight,700,labels=&quot;Sample&quot;,pos=4, col=&quot;red&quot;) Two sample test These are cases when you want to compare samples from two different populations. Say you have noted that when you fuel-up your car at a gas station A, you can take on average one more trip to work than when you fuel-up your car at gas station B. You are suspecting gas station B is giving less gas than it is suppose to. You can report this to a consumer group, and a likely test will be to take samples of one gallon from both gas stations and measure the actual volume they dispense. Say a sample of 50 trials yields volumes of 1.1 gal +/-.05gal in Gas station A and 1.01 gal +/-.02gal in Gas station B. This is a case of a two-sample test. In this case, you can treat both samples as independent, and assume they come from independent populations that should be similar. Lets visualize this: MeanGasA=1.1 # average gas volume for a suspected 1 gallon dispensing at gas station A SDGasA=0.05 # SD for sample from gas station A MeanGasB=1.01 # average gas volume for a suspected 1 gallon dispensing at gas station B SDGasB=0.02 # SD for sample from gas station B SimulatedGasA&lt;- rnorm(10000,MeanGasA,SDGasA) #lets simulate a population for gas station A. SimulatedGasB&lt;- rnorm(10000,MeanGasB,SDGasB) #lets simulate a population for gas station B. breaks = seq(0.7, 1.4, length.out = 100) #I create a set of bins for the x-axis of the distribution, this will determine how many bis you have #next I plot the data. because we want to plot two distributions on top of each other. we need to create each distribution independently and then add them to a single plot #histogram for gas station A. GasA= hist(SimulatedGasA, xlim=c(0.7, 1.4),ylim=c(0, 500),breaks = breaks, plot = FALSE) #histogram for gas station B. GasB= hist(SimulatedGasB, xlim=c(0.7, 1.4),ylim=c(0, 500),breaks = breaks, plot = FALSE) #because you are plotting two distributions, it will be nice to use different colors. and because they likely overlap, you should use semitransparent colors LightBlue &lt;- rgb(173,216,230,max = 255, alpha = 5) #the function rgb lets you select one color, and the alpha gives you how transparent DarkRed &lt;- rgb(255,192,203, max = 255, alpha = 95) #lets plot the first distribution plot(GasA, main=NA,xlim=c(0.7, 1.4),ylim=c(0, 1500),breaks = breaks, xlab = &quot;Dispensed gas volume for a reported 1 gallon (in gallons)&quot;,ylab = &quot;Number of trials&quot; ,col=LightBlue) plot(GasB, add=T ,col=DarkRed) #note how for the parameter col, you indicate the color you create as a variable above More than two sample test These are cases when you want to compare samples from more than two different populations. Say you have visited three countries and noticed that men were very different in height. You are use to the men in the USA, which we already established have an average height of 177.8cm +/-SD 7.62 cm. In France, they were 172.8cm +/-SD 5.62 cm and in North Korea they were 155.5 cm +/- 8.5cm. This is a case of a three samples, at times called multiple samples. Lets visualize this: USA_Height=177.8 # height of average male in the USA USA_SD=7.62 # Standard deviation of male population in the USA France_Height=172.8 # France data France_SD=5.62 # SD for france NKorea_Height=155.5 # North Korea data NKorea_SD=8.5 # SD for North Korea SimulatedUSA&lt;- rnorm(10000,USA_Height,USA_SD) #lets simulate a population for USA SimulatedFrance&lt;- rnorm(10000,France_Height,France_SD) #lets simulate a population for France SimulatedNKorea&lt;- rnorm(10000,NKorea_Height,NKorea_SD) #lets simulate a population for NKorea breaks = seq(120, 220, length.out = 100) #bins for the distribution #histogram for USA USA= hist(SimulatedUSA, xlim=c(120,220),ylim=c(0, 500),breaks = breaks, plot = FALSE) #histogram for FCrance France= hist(SimulatedFrance, xlim=c(120,220),ylim=c(0, 500),breaks = breaks, plot = FALSE) #histogram for USA NKorea= hist(SimulatedNKorea, xlim=c(120,220),ylim=c(0, 500),breaks = breaks, plot = FALSE) #because you are plotting three distributions, it will be nice to use different colors. and because they likely overlap, you should use semitransparent colors LightBlue &lt;- rgb(173,216,230,max = 255, alpha = 100) #the fucntion rgb lets your select one color, and the alpha gives you how tranparent DarkRed &lt;- rgb(255,192,203, max = 255, alpha = 95) MiddleOrange &lt;- rgb(255,69,0, max = 255, alpha = 95) #lets plot the first distribution plot(USA, main=NA,xlim=c(120, 220),ylim=c(0, 800),breaks = breaks, xlab = &quot;Men size (cm)&quot;,ylab = &quot;Number of people&quot; ,col=LightBlue) #now add the second distribution to the first plot, using the parameter add=TRUE, or add=T, theya re both the same plot(France, add=T ,col=DarkRed) #note how for the parameter col, you indicate the color you create as a variable above plot(NKorea, add=T ,col=MiddleOrange) #note how for the parameter col, you indicate the color you create as a variable above "],["selecting-the-right-test.html", "Selecting the right test", " Selecting the right test At this moment you should be able to identify cases involving comparisons between one, two or more samples. When you are interested for testing for significant differences in the cases above you have have to select among tests called Z-test, T-test or the ANOVA (also so-called analysis if variances). Which test you should use is based primarily on the number of samples you have, whether you know the variance of your population and whether the sample size is larger than 30 individuals or not. You need to use the following chart to select the proper test. Please remember that these are parametric tests, meaning they are only useful for cases in which data are normally distributed. Figure 9.2: Selecting statistical test Also take a moment to review how hypotheses are stated, as that will be important in a moment to select the right type of test. "],["homework-6.html", "Homework", " Homework Create a publication quality plot for each of these case examples. For each plot, indicate inside the plot if that is a case of one, two or more samples: Chevrolet and Honda are strongly competing to produce hybrid cars as an option to electrical cars. The Honda model drives 50mpg (miles per gallon) +/-5mpg and the Chevrolet model drives 55mpg (miles per gallon) +/-7mpg. Students at a school got on average GPA of 3.8, and are being considered for an award if they are among the best at the national level, in which the average GPA was 3.5 +/-0.12. China is currently looking to optimize food production within its borders and run an experiment on biomass production for wheat, rice and corn. The annual yields were 55500 tns +/-500tns for wheat, 45500 tns +/-200tns for rice, and 35500 tns +/-255tns for corn. Generate only the quality plots, no need to have the code in your report. "],["10-one-sample-tests.html", "10 One sample tests", " 10 One sample tests One sample tests are for cases in which you have one sample, and what to determine if it belongs to a certain population. Like: is it the gas in Hawaii significantly more expensive than the rest of the USA?. is a 10 year old girl taller than expected? These types of questions can be answered with either the Z-test or the T-Test, which we will introduce in this chapter. Both of these tests are suited for one-sample test, but the Z-test is used for cases in which you know the population standard deviation (SD) and your sample size is larger than 30 observations. If on the contrary, you do not know your population SD or your sample size is smaller than 30 observations, then you used the T-Test. Figure 10.1: Z-score fucntion At the end of this chapter, you are expected to: Run a one sample Z-test and interpret its results from R. Run a one sample T-test and interprets its results from R. "],["the-one-sample-z-test-by-hand.html", "The one-sample Z-test by hand", " The one-sample Z-test by hand As mentioned earlier, the Z-test is used in cases when you want to run a one-sample test, you know the population standard deviation and your sample size is larger than 30 individuals. (grabs this token L). The principle of the Z-score is very straightforward. Basically, this score allows you to know how many Standard deviations from the mean a given sample is. As it has been indicated several times in this book, it is relatively easy to know the proportion of individuals in a population at certain standard deviations from the mean (when the data are normally distributed, of course). As an example, plus or minus two standard deviations from the mean includes about 95% of the individuals in that population (assuming the population is normally distributed). Given this principle then you can easily know the fraction of individuals at certain standard deviations from the mean. The z-cores is basically an index that allows you to convert/standardize any value to a given number of standard deviations from the mean. Figure 10.2: Z-test fucntion Lets try an example. Say you want to know if the USA female team run particularly faster the 100m race than other teams. You studeid 31 girls and their times runing 100m in seconds were: 9.7, 9.7, 8.9, 9.2, 9.4, 9.1, 9.5, 9.6, 8.9, 9.1, 9.5, 9.2, 9.5, 9.4, 9.6, 9.2, 9.9, 10, 9.3, 9.6, 9.1, 9.5, 9.2, 9.5, 9.4, 9.6, 9.2, 9.9, 10, 9.3, 9.6. In turn, the historical time running that race is 9.5sec +/-0.2sec. From the example, it looks like we have one sample that we want to compare to a population. We have the standard deviation, SD, of the population and our sample size is larger than 30, so the best test here is a Z-test. As it is customary, we start by stating the hypothesis: H0: Time USA team = 9.5 sec The null hypothesis is how things are suppose to bethat means, the historical time of 9.5sec H1: Time USA team &lt; 9.5 sec The alternative hypothesis is what we are interested on, or claimingthat the USA team is faster than 9.5sec Lets calculate the z-score: Sample=c(9.7, 9.7, 8.9, 9.2, 9.4, 9.1, 9.5, 9.6, 8.9, 9.1, 9.5, 9.2, 9.5, 9.4, 9.6, 9.2, 9.9, 10, 9.3, 9.6, 9.1, 9.5, 9.2, 9.5, 9.4, 9.6, 9.2, 9.9, 10, 9.3, 9.6) #lets put the values in a vector SampleMean=mean(Sample) #mean time racing for the girls sampled SampleSize= length(Sample) # this is the sample size or number of girls measured PopulationMean= 9.5 #this is the population mean...or the historical time it has taken people to run 100m PopulationSD=0.2 #this is the population standard deviation #let&#39;s now estimate the z-score using the equation above. ZTest=(SampleMean-PopulationMean)/(PopulationSD/sqrt(SampleSize)) ZTest ## [1] -1.70625 Now you need to recall chapter 8 to find out if this is a left-, right- or two tail test. If you have forgotten, check the video as a refreshers: Because, our alternative hypothesis is stating that the sample is smaller than, then we have to use a left-sided test. Figure 10.3: Types of tests And with that you should look for the significance level (\\(\\alpha\\)) in a Z-table of a left-sided test, like this one below (These tables are in the back of each stats book and online). The Z-table would give you the p-value for a given Z-valuein other words, what fraction of the population is beyond the given z-score. Figure 10.4: Left-sided Z-table To find out the p-value for a given Z-score in the left-sided Z-table, scroll down the first column looking for the first decimal point in your calculated z-score. In our case that will be -1.9. Once on that row, go back to the first row, and move horizontally until the column with the second decimal point in your calculated Z-score, which in our case is 0. Basically, you are looking for the value at the interception between the first decimal point in your calculated z-score indicated in the first column, and the second decimal of your z-score indicated in the first row of the Z-table. In our case, the p-value for a Z-score of -1.90 is 0.0287. What that tells you is that the girls in the USA team are as fast as the top 2.87% of all teams. If we compare our significance level, \\(\\alpha\\), of 0.05, to our calculated p-value of 0.0287, you can observed that the p-value is smaller than the \\(\\alpha\\), so the Ho (null hypothesis) must go. Basically, we reject the null hypothesis that the 100m run time of this team is 9.5sec or higher and conclude that these girls indeed run faster than the historical average. "],["the-one-sample-z-test-in-r.html", "The one-sample Z-test in R", " The one-sample Z-test in R We can run such a test in R, using the package BSDA, and its function z.test. Lets try it. # install.packages(&quot;BSDA&quot;) #first install the library, if you have not this library installed....simply remove the # sign and run this line library (&quot;BSDA&quot;) #load the library Sample=c(9.7, 9.7, 8.9, 9.2, 9.4, 9.1, 9.5, 9.6, 8.9, 9.1, 9.5, 9.2, 9.5, 9.4, 9.6, 9.2, 9.9, 10, 9.3, 9.6) #lets put the values in a vector PopulationMean= 9.5 #this is the population mean...or the historical time it has taken people to run 100m PopulationSD=0.2 #this is the population standard deviation z.test(x=Sample, alternative = &quot;less&quot;, mu = PopulationMean, sigma.x = PopulationSD, conf.level = 0.95) #the parameters needed to run a Z-test are self-explanatory. x is the array with your sample data. alternative is the type of test to run, in this case we want to check is the sample is less than the population mean. mu is the population mean, and sigma.x is the standard deviation of the population. This function ask for the confidence level, which as we indicated before is the complement to the level of significance we use, in our case our significance level is 0.05, so the confidence level will be 0.95 The results will look like the image below, which are nearly identical to our hand calculation. a Figure 10.5: R-results for a one-sample z-test "],["the-one-sample-t-test-by-hand.html", "The one-sample T-test by hand", " The one-sample T-test by hand We perform a one-Sample t-test when we want to compare a sample mean with the population mean and we do not know the population standard deviation or our sample size is small, n &lt; 30. The difference from the Z Test is that we do not have the information on Population Variance here. We use the sample standard deviation instead of population standard deviation in this case. Figure 10.6: R-results for a one-sample z-test Lets try an example. Lets say we want to determine if on average girls score more than 600 points in a given exam. We do not have the information related to variance (or standard deviation) for girls scores, so we take randomly the scores of 10 girls. We choose our \\(\\alpha\\) value (significance level) to be 0.05. The scores for the ten girl were: 587, 602, 627, 610, 619, 622, 605, 608, 596, 592. Lets set the hypothesis H0: \\(\\mu\\) =&lt; 600 the true or expected value H1: \\(\\mu\\) &gt; 600 the question of interest, which in this case is to know if the girsl scored higher In this case, we have a one-sample comparison, we do not know the population variance, and our sample size is only 10 individuals, so a T-test is best suited here. Lets do one by hand, Sample=c(587, 602, 627, 610, 619, 622, 605, 608, 596, 592) #lets put the values in a vector SampleMean=mean(Sample) #mean score of the girls sampled SampleSD=sd(Sample) #this is the sample standard deviation SampleSize= length(Sample) # this is the sample size PopulationMean= 600 #this is the true, expected value, think of it as the population mean... #let&#39;s now estimate the T-score using the equation above. TTest=(SampleMean-PopulationMean)/(SampleSD/sqrt(SampleSize)) TTest So our estimated T-value is 1.64. Now, we need to find out the critical t-value at the 0.05 significance level. For this, we need to estimate something call Degrees of freedom, which in this case is simply our sample size minus one. In our case, the degrees of freedom, then, are 9. With the information of the \\(\\alpha\\) value (i.e., 0.05 in our case) and the degrees of freedom, DF (i.e, 9 in our case), we can look for the critical T-value in a table, like the one below. Basically, scroll down the first column looking for 9 DF, then move horizontally to the column displaying the 0.05 level of significance, where they intercept that is our critical t-value. In our case that number is 1.8331. Figure 10.7: R-results for a one-sample z-test Our critical t-value for 9DF and \\(\\alpha\\)=0.05 is 1.8331. This means 5% of the given population are above a T-value of 1.8331. In our case, the calculated t-value was 1.64, which is smaller than the critical value, so we fail to reject the null hypothesis and dont have enough evidence to support the hypothesis that on average, girls score more than 600 in the exam. There is only one T-table for cases of left-, right-, or two-tail test. When in need of a left-side score, simply remove the sign and look for the critical t-value in the table above. This is because the T-distribution is symmetric from the mean: it is the same right or left. When in need of a two-tail test, divide the significance level by 2, and look for that value in the T-table. Say, you are using a significance level of 0.05 testing an alternative hypothesis of something being different, then you will be running a two-tail test, meaning you will be looking to see if your sample value is on either tail of the population distribution. In this case, you need to divide the significance level (\\(\\alpha\\)) by 2 to account for the fact that you are checking the two-tails of the distribution. "],["the-one-sample-t-test-in-r.html", "The one-sample T-test in R", " The one-sample T-test in R We can run a T-test in R, using the package BSDA, and its function tsum.test. Lets try it. library (&quot;BSDA&quot;) #load the library Sample=c(587, 602, 627, 610, 619, 622, 605, 608, 596, 592) #lets put the values in a vector SampleMean=mean(Sample) #this is the sample mean SampleSD= sd(Sample) #this is the sample standard deviation SampleSize=length(Sample) #sample size PopulationMean= 600 #this is the population mean...or the expected score, which is assumed to be 600 points tsum.test(mean.x=SampleMean, s.x = SampleSD, n.x = SampleSize, alternative = &quot;greater&quot;, mu = PopulationMean, conf.level = 0.95) #the parameters needed to run a T-test are self-explanatory. mean.x is the mean of your sample data. alternative is the type of test to run, in this case we want to check is the sample is greater than the population mean. mu is the population mean, and s.x is the standard deviation of the sample. This function ask for the confidence level, which as we indicated before is the complement to the level of significance to use, in our case our significance level is 0.05, so the confidence level will be 0.95 The results will look like the image below, which are nearly identical to our hand calculation. Figure 10.8: R-results for a one-sample z-test "],["homework-7.html", "Homework", " Homework For each of the problems below do: State the null and alternative hypothesis. will this be a one tail or two-tail test?. Indicate the type of test to use (Z-test or T-test). Run the given test in R. Report your results on top of a publication quality plot. Describe your conclusion. Problem 1: Public safety standards indicate that CO2 emissions of gasoline cars should be 8.89kilogram +/-.2 kg for each gallon of gasoline used.A car company is being sued by costumers who claim these cars produce much more carbon emissions. A sample of 500 cars of this given company were used to measure carbon emissions and the results indicated that on average the cars produced 9.2 CO2kg/gallon of gas. Do the data support the claim of these costumers? Problem 2: An energy drink company recently purchased a bottling machine for their 250ml trade-mark product. Before releasing any product, the factory manager wants to confirm the right volume of liquid is being dispensed by this machine. He measured the volume of liquid in several samples, which were 245.5, 249.2, 251.2, 251.1, 252ml. Given these data, what will be your recommendation about starting to pack the product with his machine? "],["11-two-sample-tests.html", "11 Two sample tests", " 11 Two sample tests Two sample tests are for cases in which you want to compare two independent samples. Like: Are the students in school A yielding better math scores than students in School B?. Are apples heavier than oranges?. The premise is that the more similar the two samples, the less likely they are to be significantly different. Figure 11.1: Two populations with different means, n=5 These types of questions can be answered with either the Z-test or the T-Test, which we will introduce in this chapter. Both of these tests are suited for two-sample tests, but the Z-test is preferred to the t-test for large samples (n &gt; 30) or when the population variance is known. Figure 11.2: Z-test or T-test? At the end of this chapter, you are expected to: Run a two sample Z-test and interpret its results from R. Run a two sample T-test and interprets its results from R. "],["the-two-sample-z-test-by-hand.html", "The two-sample Z-test by hand", " The two-sample Z-test by hand The two-sample Z-test is used to compare the means of two different samples, when you know the population standard deviation and your sample size is larger than 30 individuals. Figure 11.3: Z-test fucntion for two samples Lets try an example. A researcher wants to determine if a given drug is any different than a placebo at affecting physical endurance (i.e, the time it takes to do 20 push-ups). Nine hundred subjects are given the drug prior to testing, whereas 1000 other subjects receive a placebo. For the drug group, it took 9.78sec on average (S.D. = 4.05) to do 20 push-ups, whereas the placebo group took 15.10sec on average (S.D. = 4.28). Does this drug has any effect on physical performance? Lets test this hypothesis at an \\(\\alpha\\) of 0.05,or 5%. Lets start by visualizing these data: Drug_Mean=9.78 Drug_SDSD=4.05 Placebo_Mean=15.10 Placebo_SD=4.28 SimulatedDrug&lt;- rnorm(10000,Drug_Mean,Drug_SDSD) SimulatedPlacebo&lt;- rnorm(10000,Placebo_Mean,Placebo_SD) breaks = seq(-20,40, length.out = 50) #bins for the distribution #histogram for drug Drug= hist(SimulatedDrug, xlim=c(-20,40),ylim=c(0, 500),breaks = breaks, plot = FALSE) #histogram for Placebo Placebo= hist(SimulatedPlacebo, xlim=c(-20,40),ylim=c(0, 500),breaks = breaks, plot = FALSE) #because you are plotting two distributions, it will be nice to use different colors. and because they likely overlap, you should use semitransparent colors LightBlue &lt;- rgb(173,216,230,max = 255, alpha = 100) #the function rgb lets your select one color, and the alpha gives you how transparent DarkRed &lt;- rgb(255,192,203, max = 255, alpha = 95) #lets plot the first distribution plot(Drug, main=NA,xlim=c(-20,40),ylim=c(0, 1500),breaks = breaks, xlab = &quot;Time to do 20 push-up (Seconds)&quot;,ylab = &quot;Number of people&quot; ,col=LightBlue) #now add the second distribution to the first plot, using the parameter add=TRUE, or add=T, theya re both the same plot(Placebo, add=T ,col=DarkRed) #note how for the parameter col, you indicate the color you create as a variable above Lets state the hypotheses: H0: Drug performance time = placebo time The null hypothesis is that the two groups take the same time. H1: Drug performance time \\(\\neq\\) placebo time The alternative hypothesis is that they are different. If you read carefully the question, you will see that the motivation of the researcher was to see if the drug was any different. I did that on purpose, so you can state the alternative hypothesis accordingly. Perhaps a better statement could be that the drug is better, in which case the null hypothesis would be greater than, in which case we would need a right-hand test, as oppose to a two-tail test when we asked is they are different. When it comes to statistics, and most things in life, you always do what is written, as that is your supporting evidence. So what test do we use?. Well, the sample size is larger than 30, so a Z-test seems appropriated. Lets calculate the Z-score for comparing two samples Drug_Mean=9.78 Drug_SDSD=4.05 Drug_n =900 #sample size for the drug treatment Placebo_Mean=15.10 Placebo_SD=4.28 Placebo_n=1000 #sample size for the placebo treatment Z_Score_Numerator= (Drug_Mean-Placebo_Mean) -0 #because we assume that the two treatments have the same effect, we then have to assume that the difference between the population means is zero. Z_Score_denominator= sqrt(((Drug_SDSD^2)/Drug_n)+((Placebo_SD^2)/Placebo_n)) Z_score=Z_Score_Numerator/Z_Score_denominator Z_score ## [1] -27.82961 By now, we know well that two standard deviations from the mean include 95% of the observations in a normally distributed population. A value that is -27.82 standard deviations away from the mean does not even appear in traditional z-tables, so we can certainly reject the null hypothesis and conclude that this drug is indeed different than a placebo. "],["the-two-sample-z-test-in-r.html", "The two-sample Z-test in R", " The two-sample Z-test in R We can run such a test in R, using the package BSDA, and its function z.test. Lets try it. # install.packages(&quot;BSDA&quot;) #first install the library, if you have not this library installed....simply remove the # sign and run this line library (&quot;BSDA&quot;) #load the library Drug_Mean=9.78 Drug_SDSD=4.05 Drug_n =900 #sample size for the drug treatment Placebo_Mean=15.10 Placebo_SD=4.28 Placebo_n=1000 #sample size for the placebo treatment zsum.test(mean.x=Drug_Mean, sigma.x = Drug_SDSD, n.x = Drug_n, mean.y = Placebo_Mean, sigma.y = Placebo_SD, n.y = Placebo_n, alternative = &quot;two.sided&quot;, mu = 0,conf.level = 0.95) #the parameters needed to run a Z-test are self-explanatory, similar to the one-sample test. x would indicate the data for the drug and y the data for the placebo. In this case, we are testing a two-tail hypothesis. Figure 11.4: Z-test results for two samples from R "],["the-two-sample-t-test-by-hand.html", "The two-sample T-test by hand", " The two-sample T-test by hand We perform a Two-Sample t-test when we want to compare the mean of two samples, and our sample sizes are smaller than 30. Figure 11.5: T-test fucntion for two samples Lets try an example. Historically, males have scored 15 points more than girls in a given exam. No standard deviations exists. To test if this is true today, a researcher performed such a test in 10 males and 10 females and found that males scored 630.1 on average (+/- 13.42SD) and females 606.8 (+/- 13.14 SD). He wants to test the significance of this conclusion with a \\(\\alpha\\) value (significance level) of 0.05. Lets start by stating the hypotheses: H0: Men scores -(minus) Female scores &lt;= 15, The null hypothesis is the complement of the alternative hypothesis, so set that one first H1: Men scores-Female scores &gt; 15 The question of interest is that males get 15 points more than females. So the difference between male and female scores is larger than 15 points. What test to use?. Well the sample size is smaller than 30, so we need to use a T-Test. And before we do any calculations, lets start by visualizing these data: Males_Mean=630.1 Males_SDSD=13.42 Females_Mean=606.8 Females_SD=13.14 SimulatedMales&lt;- rnorm(10000,Males_Mean,Males_SDSD) SimulatedFemales&lt;- rnorm(10000,Females_Mean,Females_SD) breaks = seq(500,700, length.out = 50) #bins for the distribution #histogram for drug Males= hist(SimulatedMales,breaks = breaks, plot = FALSE) #histogram for Placebo Females= hist(SimulatedFemales, breaks = breaks, plot = FALSE) #because you are plotting two distributions, it will be nice to use different colors. and because they likely overlap, you should use semitransparent colors LightBlue &lt;- rgb(173,216,230,max = 255, alpha = 100) #the function rgb lets your select one color, and the alpha gives you how tranparent DarkRed &lt;- rgb(255,192,203, max = 255, alpha = 95) #lets plot the first distribution plot(Males, main=NA,xlim=c(500,700),ylim=c(0, 1500),breaks = breaks, xlab = &quot;Score test&quot;,ylab = &quot;Number of people&quot; ,col=LightBlue) #now add the second distribution to the first plot, using the parameter add=TRUE, or add=T, they are both the same plot(Females, add=T ,col=DarkRed) Lets calculate the T-score for comparing two samples: Males_Mean=630.1 Males_SDSD=13.42 Females_Mean=606.8 Females_SD=13.14 SampleSize= 10 #we can use this variable for both males and females, since in both cases 10 people were analyzed T_Score_Numerator= (Males_Mean-Females_Mean) -15 #because we assume that males score 15 point more, we need to assume that the difference between the population means is 15 points T_Score_denominator= sqrt(((Males_SDSD^2)/SampleSize)+((Females_SD^2)/SampleSize)) T_score=T_Score_Numerator/T_Score_denominator T_score ## [1] 1.397465 Next, look for the t-critical value in the t-table for 18 degrees of freedom. In the case of a two sample t-test, degrees of freedoms are equal to the sample size one plus sample size two minus two) and a level of significance \\(\\alpha\\) of 0.05 (Here we are looking at one tail test, as the alternative hypothesis states that one mean is larger than the other). Figure 11.6: R-results for a one-sample z-test So the critical t-Score is 1.734. Since our calculated t-value (i.e., 1.397465) was smaller than the critical t-value (1.734), we fail to reject the null hypothesis. Males doe not get 15 points higher than females in this exam as it has been historically. "],["the-two-sample-t-test-in-r.html", "The two-sample T-test in R", " The two-sample T-test in R We can run a T-test in R, using the package BSDA, and its function tsum.test. Lets try it. library (&quot;BSDA&quot;) #load the library Males_Mean=630.1 Males_SDSD=13.42 Females_Mean=606.8 Females_SD=13.14 SampleSize= 10 tsum.test(mean.x=Males_Mean, s.x = Males_SDSD, n.x = SampleSize, mean.y=Females_Mean, s.y = Females_SD, n.y = SampleSize, alternative = &quot;greater&quot;, mu = 15, conf.level = 0.95) #the parameters needed to run a T-test are self-explanatory. x parameters would be for males and y parameters for females, mu is the expected difference between the population means, which we know from the example is 15 score points Figure 11.7: R-results for a two-sample T-test "],["homework-8.html", "Homework", " Homework For each of the problems below do: State the null and alternative hypothesis. will this be a one tail or two-tail test?. Indicate the type of test to use (Z-test or T-test). Run the given test in R. Report your results on top of a publication quality plot. Describe your conclusion. Problem 1: Two competing headache remedies claim to give fast-acting relief. An experiment was performed to compare the mean lengths of time required for bodily absorption of brand A and brand B headache remedies. The lengths of time in minutes for the drugs to reach a specified level in the blood were recorded. Results were are follow: Brand A: Mean time= 21.8 min; sd= 8.7 min; n=500. Brand B: Mean time= 18.9 min; sd= 7.5 min; n=350. Are the two drugs different?. Test this at a significance level of 0.05 Problem 2: A consumer group is testing two camp stoves. To test the heating capacity of a stove, it measures the time required to bring 2 quarts of water from 50°F to boiling (at sea level). The following results are obtained: Model 1: Mean time = 11.4 min; sd= 2.5 min; n= 10 stoves. Model 2: Mean time = 9.9 min; s2=3.0 min; n=12 stoves. Assume that the time required to bring water to a boil is normally distributed for each stove. Is there any difference between the performances of these two models? Use a 5% level of significance. "],["12-multiple-sample-test.html", "12 Multiple sample test", " 12 Multiple sample test This chapter describes the mathematical approach to check for differences among independent groups, or so-call ANOVA test (or Analysis of Variance). Although the name of the technique refers to variances, the main goal of ANOVA is to investigate differences among means. There are numerous ways in which you can look for differences among groups, which will determine the type of ANOVA to use. Lets check some examples, to clarify the distinctions. Say the International Olympic Association wants to endorse an energy drink for its athletes and two different companies send samples of their products.A first, order question could be if the drinks make any difference in the endurance of their athletes. And you can compare, say the times running 100m for groups of athletes given the two types of drinks and a control given water. In this case, you want tot test, if at least one of the three groups is different, and in this case, you will use what is call a one-way ANOVA. You only have one factor, which is the drink type. Figure 12.1: Z-test or T-test? Say, however, that you are also interested in knowing if such an effect varies between men and females. In this case, you have two factors and you will need to know the effect of the drink and the effect of gender. For this type of cases, you will use what is called a two-way ANOVA.You have two factors (drink type and gender). Figure 12.2: Z-test or T-test? In this course, we will study only one-way ANOVAS. You should also be aware that to run an ANOVA you need to test for different key assumptions. The data are normally distributed. As mentioned earlier, this assumption is key to all types of parametric tests. Basically we need to know the data follows a bell shape, such that we can approximates the distribution of data. The variances among the different groups have to be homogeneous, or so-call homogeneity of variance.As we will learn soon, ANOVA is bassed on the ratio the variance between groups and the variance within groups. So if variances are non homogeneous among groups this could bias the ANOVA test. The observations are independent. No outliers are present. At the end of this chapter, you are expected to: Run an ANOVA in R, and interpret its results Run a post-hoc test, identify differences among specific pairs of samples. "],["anova-theory.html", "ANOVA theory", " ANOVA theory Figure 12.3: Z-test or T-test? "],["anova-by-hand.html", "ANOVA by hand", " ANOVA by hand "],["anova-in-r.html", "ANOVA in R", " ANOVA in R "],["post-hoc-test-in-r.html", "Post-hoc test in R", " Post-hoc test in R "],["homework-9.html", "Homework", " Homework For each of the problems below do: State the null and alternative hypothesis. will this be a one tail or two-tail test? Indicate the type of test to use (Z-test or T-test). Run the given test in R. Report your results on top of a publication quality plot. Describe your conclusion. Problem 1: Two competing headache remedies claim to give fast-acting relief. An experiment was performed to compare the mean lengths of time required for bodily absorption of brand A and brand B headache remedies. The lengths of time in minutes for the drugs to reach a specified level in the blood were recorded. Results were are follow: Brand A: Mean time= 21.8 min; sd= 8.7 min; n=500 Brand B: Mean time= 18.9 min; sd= 7.5 min; n=350 Are the two drugs different?. test this at a significance level of 0.05 Problem 2: A consumer group is testing two camp stoves. To test the heating capacity of a stove, it measures the time required to bring 2 quarts of water from 50°F to boiling (at sea level). The following results are obtained: Model 1: Mean time = 11.4 min; sd= 2.5 min; n= 10 stoves Model 2: Mean time = 9.9 min; s2=3.0 min; n=12 stoves Assume that the time required to bring water to a boil is normally distributed for each stove. Is there any difference between the performances of these two models? Use a 5% level of significance. "]]
